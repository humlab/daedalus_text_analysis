{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text corpus processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr {\n",
       "    text-align: left;\n",
       "    vertical-align: top;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr {\n",
    "    text-align: left;\n",
    "    vertical-align: top;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statens offentliga utredningar (SOU)\n",
    "\n",
    "- KB har digitaliserat alla SOU:er publicerade mellan **1922 och 1999** (1129 stycken).\n",
    "- Utredningarna kan laddas ned från **http://regina.kb.se/sou/** i PDF-format.\n",
    "- Filerna kan vara rätt stora, flera hundra MB, då de innehåller dels alla sidor scannade.\n",
    "- De innehåller även inbäddad **OCR:ad text** vilket möjliggör textsökning.\n",
    "- OCR:a text är genomgående av mycket **hög kvalitet**.\n",
    "\n",
    "[<img src=\"./images/kb_regina_sou.jpg\" alt=\"http://regina.kb.se/sou/\" width=\"700px\"/>](http://regina.kb.se/sou/)\n",
    "\n",
    "\n",
    "### The scanned documents\n",
    "\n",
    "| SOU 1933:15        | Page 23 | \n",
    "| ------------- |:-------------|\n",
    "| [<img src=\"./images/kb-digark-2106487-page-0.jpg\" alt=\"1933:15 Utredning och förslag angående importmonopol på kaffe\" width=\"400px\"/>](https://weburn.kb.se/sou/211/urn-nbn-se-kb-digark-2106487.pdf)      | <img src=\"./images/kb-digark-2106487-page-23.jpg\" alt=\"1933:15 Page 23\" width=\"400px\"/> |\n",
    "\n",
    "### The OCR:ed text is of very high quality\n",
    "\n",
    "> 23 Enligt dessa siffror skulle sålunda **uuder** åren 1911—1913 43 % och år\n",
    "1931 65 % av Sveriges import av orostat kaffe ha varit av brasilianskt\n",
    "ursprung. Ingendera av dessa relationer är emellertid riktig, den första\n",
    "dock i vida mindre mån än den sista. Före kriget var Tyskland ännu en\n",
    "stor mellanhand för vår kaffehandel, liksom i viss utsträckning även\n",
    "Danmark och Holland, Före 1905 voro dessa länder, framför allt Tysk-\n",
    "land, såvitt kan slutas av den svenska handelsstatistiken, ännu mera do-\n",
    "minerande — vi köpte vid denna tid så gott som allt vårt kaffe genom\n",
    "affärshus i Hamburg, Rotterdam, Köpenhamn o. s. v. Efter tillkomsten\n",
    "år 1904 av ett nytt svenskt rederiföretag med trafik på Syd- och Central-\n",
    "amerika har bladet vänt sig. Redan år 1905 återfinnas Brasilien och\n",
    "Centralamerika i vår importstatistik med införselsiffror av resp. 4-5 och\n",
    "0\\*3 milj. kg. från att förut ha varit obefintliga, och sedan dess har den\n",
    "direkta införseln oavlåtligt stigit på den indirektas bekostnad. År 1931\n",
    "redovisas enligt tab. 15 den direkta importen till 85 % av den totala och\n",
    "importen från Brasilien till 65 %. I verkligheten torde omkr. 70—75 %\n",
    "av vårt kaffe vara av brasilianskt ursprung. Danmark, Holland, Tysk-\n",
    "land och England förmedla väsentligen vår import av dyrare kaffesor-\n",
    "ter från Guatemala (vars export till stor del behärskas av den tyska\n",
    "handeln), Mexiko, Costa Rica, Haiti, Java o. s. v.\n",
    "\n",
    "### But tables, indexes tec. are problematic\n",
    "\n",
    "> T a b . 16. E x p o r t å r 1930 av brasilianskt kaffe.\n",
    "U. S. A\n",
    "Frankrike\n",
    "Tyskland\n",
    "Holland\n",
    "Italien\n",
    "Argentina • • • -\n",
    "Sverige\n",
    "Belgien\n",
    "Övriga\n",
    "Summa\n",
    "1 000 säckar\n",
    "å 60 kg.\n",
    "8 006\n",
    "1995\n",
    "912\n",
    "862\n",
    "781\n",
    "482\n",
    "444\n",
    "410\n",
    "1399\n",
    "15 291\n",
    "52-4\n",
    "13-o\n",
    "6-0\n",
    "5-6\n",
    "5 i\n",
    "3-2\n",
    "2-9\n",
    "27\n",
    "9 1\n",
    "lOOo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Clean up the text in SOU 1933:15\n",
    "- Typical text preprocessing includes removal of frequent words, short words, etc.\n",
    "- Also common to filter based part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a71f8b6b32343a5bb1a829f8adf38d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Show', index=2, options={'Sanitized text': 'sanitized_text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# folded code\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "hyphen_regexp = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        document = f.read()\n",
    "    return document\n",
    "\n",
    "def tokenize_and_sanitize(document, de_hyphen=True, min_length=3, only_isalpha=True, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "        \n",
    "    # hantera avstavningar\n",
    "    if de_hyphen:\n",
    "        document = re.sub(hyphen_regexp, r\"\\1\\2\\n\", document)\n",
    "\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    \n",
    "    # Ta bort ord kortare än tre tecken\n",
    "    if min_length > 0:\n",
    "        tokens = [ x for x in tokens if len([w for w in x if w.isalpha()]) > min_length ]\n",
    "        \n",
    "    # Ta bort ord som inte innehåller någon siffra eller bokstav\n",
    "    if only_isalpha:\n",
    "        tokens = [ x for x in tokens if x.isalpha() ]\n",
    "\n",
    "    if remove_puncts:\n",
    "        tokens = [ x.translate(punct_table) for x in tokens ]\n",
    "\n",
    "    # Transformera till små bokstäver\n",
    "    if to_lower:\n",
    "        tokens = [ x.lower() for x in tokens ]\n",
    "        \n",
    "    # Ta bort de vanligaste stoporden\n",
    "    if remove_stop:\n",
    "        stopwords = nltk.corpus.stopwords.words('swedish')\n",
    "        tokens = [ x for x in tokens if x not in stopwords ]\n",
    "        \n",
    "    return [ x for x in tokens if len(x) > 0 ]\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    \n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(8, 8 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "container=dict(\n",
    "    display_type=widgets.Dropdown(\n",
    "        description='Show',\n",
    "        value='statistics',\n",
    "        options={\n",
    "            'Source text': 'source_text',\n",
    "            'Sanitized text': 'sanitized_text',\n",
    "            'Statistics': 'statistics'           \n",
    "    }),\n",
    "    min_length=widgets.IntSlider(value=0, min=0, max=5, step=1, description='Min alpha', tooltip='Min number of alphabetic characters'),\n",
    "    de_hyphen=widgets.ToggleButton(value=False, description='Dehyphen', disabled=False, tooltip='Fix hyphens', icon=''),\n",
    "    to_lower=widgets.ToggleButton(value=False, description='Lowercase', disabled=False, tooltip='Transform text to lowercase', icon=''),\n",
    "    remove_stop=widgets.ToggleButton(value=False, description='No stopwords', disabled=False, tooltip='Remove stopwords', icon=''),\n",
    "    only_isalpha=widgets.ToggleButton(value=False, description='Only alpha', disabled=False, tooltip='Keep only alphabetic words', icon=''),\n",
    "    remove_puncts=widgets.ToggleButton(value=False, description='Remove puncts.', disabled=False, tooltip='Remove punctioations characters', icon=''),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='' )\n",
    ")\n",
    "\n",
    "output1 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "output2 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "default_output = None\n",
    "\n",
    "tokens = []\n",
    "\n",
    "def display_document(display_type, to_lower, remove_stop, only_isalpha, remove_puncts, min_length, de_hyphen, word_count=500):\n",
    "\n",
    "    global tokens\n",
    "    \n",
    "    p =  container['progress']\n",
    "    p.value = 0\n",
    "    try:\n",
    "        output1.clear_output()\n",
    "        output2.clear_output()\n",
    "        default_output.clear_output()\n",
    "        document = read_text_file('./data/urn-nbn-se-kb-digark-2106487.txt')\n",
    "        p.value = p.value + 1\n",
    "\n",
    "        if display_type == 'source_text':\n",
    "            # Utskrift av de första och sista 250 tecknen:\n",
    "            with output1:\n",
    "                print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            p.value = p.value + 1\n",
    "            return\n",
    "        \n",
    "        p.value = p.value + 1\n",
    "\n",
    "        tokens = tokenize_and_sanitize(\n",
    "            document,\n",
    "            de_hyphen=de_hyphen,\n",
    "            min_length=min_length,\n",
    "            only_isalpha=only_isalpha,\n",
    "            remove_puncts=remove_puncts,\n",
    "            to_lower=to_lower,\n",
    "            remove_stop=remove_stop\n",
    "        )\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics']:\n",
    "            \n",
    "            p.value = p.value + 1\n",
    "            \n",
    "            if display_type == 'sanitized_text':\n",
    "                with output1:\n",
    "                    display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                        ' '.join(tokens[:word_count]),\n",
    "                        ' '.join(tokens[-word_count:])\n",
    "                    ))\n",
    "                p.value = p.value + 1\n",
    "                return\n",
    "            \n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(tokens)\n",
    "                p.value = p.value + 1\n",
    "                \n",
    "                with output1:\n",
    "                    \n",
    "                    df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                    display(df)\n",
    "                \n",
    "                with output2:\n",
    "                    \n",
    "                    print('Antal ord (termer): {}'.format(wf.N()))\n",
    "                    print('Antal unika termer (vokabulär): {}'.format(wf.B()))\n",
    "                    print(' ')\n",
    "                    \n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "                    \n",
    "                    wf = nltk.FreqDist([len(x) for x in tokens])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "    \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        p.value = 0\n",
    "\n",
    "i_widgets = widgets.interactive(display_document, **container)\n",
    "default_output = i_widgets.children[-1]\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        container['display_type'],\n",
    "        container['to_lower'],\n",
    "        container['remove_stop'],\n",
    "        container['de_hyphen'],\n",
    "        container['only_isalpha'],\n",
    "        container['remove_puncts']\n",
    "    ]),\n",
    "    widgets.HBox([container['min_length'], container['progress']]),\n",
    "    widgets.HBox([output1, output2]),\n",
    "    default_output\n",
    "]))\n",
    "\n",
    "i_widgets.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Part-Of-Speech tagging (POS) using Språkbanken Sparv\n",
    "\n",
    "> - Part-of-speech tagging (POS) is the process of **assigning word classes** to each word.\n",
    "> - The **Språkbanken Sparv** API is an online tool\n",
    "> - Sparv does a lot more: tokenization, dependency analysis, lemmatization, named entity recognition, sentiment analysis, ...\n",
    "> - Sparv uses **HunPOS** (insert reference) for POS tagging\n",
    "> - POS tagging is a** common NLP task** and can be used to improve quality of various NLP methods.\n",
    "> - POS tag set: https://spraakbanken.gu.se/korp/markup/msdtags.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb0eefcc58a454bbd981e9fe1a90f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Textarea(value='Detta är ett bra exempel på en text som kan annoteras via Sparv.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# folded code\n",
    "import requests\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "\n",
    "settings={\n",
    "    \"corpus\": \"exempelkorpus\",\n",
    "    \"lang\": \"sv\",\n",
    "    \"textmode\": \"plain\",\n",
    "    \"word_segmenter\": \"default_tokenizer\",\n",
    "    \"sentence_segmentation\": {\n",
    "        \"sentence_chunk\": \"paragraph\",\n",
    "        \"sentence_segmenter\": \"default_tokenizer\"\n",
    "    },\n",
    "    \"paragraph_segmentation\": {\n",
    "        \"paragraph_segmenter\": \"blanklines\"\n",
    "    },\n",
    "    \"positional_attributes\": {\n",
    "        \"lexical_attributes\": [ \"pos\", \"msd\", \"lemma\", ],\n",
    "        \"compound_attributes\": [ ],\n",
    "        \"dependency_attributes\": [ ],\n",
    "        \"sentiment\": [ ]\n",
    "    },\n",
    "    \"named_entity_recognition\": [ ],\n",
    "    \"text_attributes\": {\n",
    "        \"readability_metrics\": [ ]\n",
    "    }\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/xml\",\n",
    "}\n",
    "\n",
    "cc=dict(\n",
    "    text_widget = widgets.Textarea(\n",
    "        value='Detta är ett bra exempel på en text som kan annoteras via Sparv.',\n",
    "        placeholder='Type some',\n",
    "        description='Text:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width=\"400px\")\n",
    "    ),\n",
    "    button=widgets.Button(\n",
    "        description='Annotate it!',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "        tooltip='Use Språkbanken Sparv to annotate text!',\n",
    "        icon=''\n",
    "    ),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=3, step=1, description='' )\n",
    ")\n",
    "\n",
    "cc_output = widgets.Output() #layout={'border': '1px solid black'})\n",
    "\n",
    "def call_spraakbanken(text):\n",
    "    p = cc['progress']\n",
    "    try:\n",
    "        cc_output.clear_output()\n",
    "        data = {\n",
    "            'text': text,\n",
    "        } \n",
    "        url = \"https://ws.spraakbanken.gu.se/ws/sparv/v2/?settings={}\".format(json.dumps(settings))\n",
    "\n",
    "        p.value = 1\n",
    "        response = requests.post(url=url, headers=headers, data=data) \n",
    "        p.value = 2\n",
    "        with cc_output:\n",
    "            print(response.text)\n",
    "        p.value = 3\n",
    "    except Exception as ex:\n",
    "        raise\n",
    "    finally:\n",
    "        p.value = 0\n",
    "        \n",
    "from IPython.display import display\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    text = cc['text_widget'].value\n",
    "    call_spraakbanken(text)\n",
    "\n",
    "cc['button'].on_click(on_button_clicked)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([cc['text_widget'], cc['button'], cc['progress'] ]),\n",
    "    cc_output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Språkbanken NER tagging av SOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971d529b0d1c404aa5a75a5ee9888228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='document_id', options={'Stadsregioner i Europa': 34, 'Den elintens…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_csv('./data/SOU_1990_total_ner_extracted.csv', sep='\\t',\n",
    "                       names=['filename', 'year', 'location', 'categories', 'entity'])\n",
    "\n",
    "entities['document_id'] = entities.filename.apply(lambda x: int(x.split('_')[1]))\n",
    "entities['categories'] = entities.categories.str.replace('/', ' ')\n",
    "entities['category'] = entities.categories.str.split(' ').str.get(0)\n",
    "entities['sub_category'] = entities.categories.str.split(' ').str.get(1)\n",
    "\n",
    "entities.drop(['location', 'categories'], inplace=True, axis=1)\n",
    "\n",
    "document_names = pd.read_csv('./data/SOU_1990_index.csv',\n",
    "                             sep='\\t',\n",
    "                             names=['year', 'sequence_id', 'report_name']).set_index('sequence_id')\n",
    "\n",
    "def plot_freqdist(wf, n=25, **kwargs):\n",
    "    data = list(zip(*wf.most_common(n)))\n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(13, 13/1.618))\n",
    "    plt.plot(x, y, '--ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.show()\n",
    "\n",
    "doc_names = { v: k for k, v in document_names.report_name.to_dict().items()}\n",
    "doc_names['** ALL DOCUMENTS **'] = 0\n",
    "@widgets.interact(category=entities.category.unique())\n",
    "def display_most_frequent_pos_tags(document_id=doc_names, category='LOC', top=10):\n",
    "    global entities\n",
    "    locations = entities\n",
    "    if document_id > 0:\n",
    "        locations = locations.loc[locations.document_id==document_id]\n",
    "    locations = locations.loc[locations.category==category]['entity']\n",
    "    location_freqs = nltk.FreqDist(locations)\n",
    "    #location_freqs.tabulate()\n",
    "    plot_freqdist(location_freqs, n=top)\n",
    "\n",
    "# display_most_frequent_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
