{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic modelling\n",
    "\n",
    "> Blei, 2003: Latent dirichlet allocation [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "\n",
    "**LDA is based on a number of assumptions**\n",
    "- that documents have a (are generated by) mix of *N* **latent themes** (topics)\n",
    "- that documents are generated with a **probibalistic repetetive process**\n",
    "- that words' order in document is irrelevant (BOW model)\n",
    "\n",
    "```\n",
    "generate document D of size n:\n",
    "    draw a topic probability distribution for D (i.e. the \"mix\" of topics)\n",
    "        repeat n times:\n",
    "            draw a topic T from the topic distribution\n",
    "            draw a word W from topic T's distribution and add it to D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation**\n",
    "- LDA defines a **statistical model** for the (assumed) generative process\n",
    "- Numerical mmethods are used to compute unknown parameters in this model\n",
    "- The computation is not deterministic (different results on same data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representations**\n",
    "- Word are represented as an integer (an integer index into a Vocabulary)\n",
    "- A document is represented as a BOW (bag-of-words)...\n",
    "- ...a simple vector with word counts (no order preserved)\n",
    "- A topic is a word probability distribution vector\n",
    "- ... item i represents the probability that word i is generated by the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this Notebook\n",
    "- The topic model must be evaluated by a human.\n",
    " - Is the topics relevant? Is the mixes of topics OK?\n",
    " - Is the distributions OK? Is the topic count OK? \n",
    "- It is challanging to evaluate a topic model - visualizations can help.\n",
    "- Notebooks (in R or Python) can be used to browse and explore models\n",
    "- Used by researcher to evaluate and analyse a topic model.\n",
    "- Some computed metrics can also help the researcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some evaluation methods/metrics\n",
    "* Browsing through topic-word distributions and document-topic distributions\n",
    "* Finding and assigning conceptual interpretations of topics\n",
    "* Finding key topics in the corpus, and topic trends\n",
    "* Use of metrics to highlight suspect data\n",
    " * Display similarity of topics to known distributions (uniform distribution, mean corpus distribution etc)\n",
    " * Display similar or overlapping topics, topic clusters (for some metric)\n",
    " * Display the ubiquitousness of topics\n",
    " * Display topic-topic co-occurrence (same document)\n",
    " * Use of added reference documents have expected topic\n",
    "\n",
    "**Suggested readings av evaluating topic models**\n",
    "- Reading tea leaves: how humans interpret topic models, Chang et al. (2009)\n",
    "- Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality, Lau et al.\n",
    "- http://dirichlet.net/pdf/wallach09evaluation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for investigating TMs\n",
    "\n",
    "> - [Termite](http://vis.stanford.edu/papers/termite) (Stanford)\n",
    "> - [HiÃ©rarchie](https://nlp.stanford.edu/events/illvi2014/papers/smith-illvi2014b.pdf)\n",
    "> - [Word Embedding Visual Explorer](http://residue3.sci.utah.edu/?) [source](https://ronxin.github.io/wevi/), [paper](https://arxiv.org/abs/1411.2738)\n",
    "> - LAMVI, Sentiview, Lexos and many more.\n",
    "> - [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) (Google, PCA, VSM)\n",
    "\n",
    "Many interesting blogs by Benjamin Schmidt, Ted Underwood etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a topic model\n",
    "\n",
    "- [MALLET](http://mallet.cs.umass.edu/)\n",
    "> McCallum, Andrew Kachites.  \"MALLET: A Machine Learning for Language Toolkit.\" http://mallet.cs.umass.edu. 2002.\n",
    "- [gensim](https://radimrehurek.com/gensim/index.html)\n",
    "> Radim Rehurek and Petr Sojka, \"Software Framework for Topic Modelling with Large Corpora\", 2010\n",
    "- [Stanford Topic Modeling Toolbox](https://nlp.stanford.edu/software/tmt/tmt-0.4/)\n",
    "\n",
    "(+many other frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>mandatory step</span> Setup and Initialize the Notebook\n",
    "Press SHIFT-ENTER or \"Run\" button to execute cell. The code imports Python libraries and frameworks, and initializes the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"f81d3fc5-b862-45a0-b9ef-9b3c68cadeb6\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Folded Code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import common.utility\n",
    "from common.model_utility import ModelUtility\n",
    "from common.plot_utility import WordcloudUtility\n",
    "import common.widgets_utility as wf\n",
    "#import common.network_utility\n",
    "#import common.vectorspace_utility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import logging\n",
    "import bokeh.models as bm\n",
    "import bokeh.palettes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pivottablejs import pivot_ui\n",
    "from IPython.display import display, HTML, clear_output, IFrame\n",
    "from itertools import product\n",
    "from bokeh.io import output_file, push_notebook\n",
    "from bokeh.core.properties import value, expr\n",
    "from bokeh.transform import transform, jitter\n",
    "from bokeh.layouts import row, column, widgetbox\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.models import ColumnDataSource, CustomJS\n",
    "\n",
    "logger = logging.getLogger('explore-topic-models')\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "AGGREGATES = { 'mean': np.mean, 'sum': np.sum, 'max': np.max, 'std': np.std }\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "pd.set_option('precision', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>MANDATORY STEP</span> Select LDA Topic Model\n",
    "Select one of the previously computed and prepared topic models that you wan't to use in subsequent steps. New models are computed in batch in accordance to the following flow:\n",
    "<img src=\"./tm-data/images/workflow-prepare.svg\" style=\"width: 800px;\">\n",
    "The resulting model files, marked by the red box in the diagram, are made avaliable for selecton simply by uploading them into separate folders in ./data. The upload can be done with Jupyter Lab's upload feature. \n",
    " \n",
    "Note that it can take some time (20-30 seconds) to load a model for the first time if the large file sizes. Subsequent load is much faster since the system extracts data to CSV-files which gives faster loads. Also note that subsequent cells are NOT updated automatically when a new model is selected. Instead you must use the **play** button, or press **Shift-Enter** to execute the current cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41543449a97d47769c1bb8ebe1412d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Topic model', layout=Layout(width='75%'), options=('20180910_SOU_1990_T50â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hidden code: Select current model state\n",
    "class ModelState:\n",
    "    \n",
    "    def __init__(self, data_folder):\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.basenames = ModelUtility.get_model_names(data_folder)\n",
    "        self.basename = self.basenames[0]\n",
    "        self.on_set_model_callback = None\n",
    "        \n",
    "    def set_model(self, basename=None):\n",
    "\n",
    "        basename = basename or self.basename\n",
    "        \n",
    "        self.basename = basename\n",
    "        self.topic_keys = ModelUtility.get_topic_keys(self.data_folder, basename)\n",
    "        state.max_alpha = self.topic_keys.alpha.max()\n",
    "        self.topic_overview = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'topic_tokens')\n",
    "        self.document_topic_weights = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'doc_topic_weights')\\\n",
    "            .drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "        self.topic_token_weights = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'topic_token_weights')\\\n",
    "            .drop('Unnamed: 0', axis=1, errors='ignore')\\\n",
    "            .dropna(subset=['token'])\n",
    "        self._years = list(range(\n",
    "            self.document_topic_weights.year.min(), self.document_topic_weights.year.max() + 1))\n",
    "        self.min_year = min(self._years)\n",
    "        self.max_year = max(self._years)\n",
    "        self.years = [None] + self._years\n",
    "        self.n_topics = self.topic_overview.topic_id.max() + 1\n",
    "        # https://stackoverflow.com/questions/44561609/how-does-mallet-set-its-default-hyperparameters-for-lda-i-e-alpha-and-beta\n",
    "        self.initial_alpha = 0.0  # 5.0 / self.n_topics if 'mallet' in state.basename else 1.0 / self.n_topics\n",
    "        self.initial_beta = 0.0  # 0.01 if 'mallet' in basename else 1.0 / self.n_topics\n",
    "        self._lda = None\n",
    "        self._topic_titles = None\n",
    "        self.corpus_documents = ModelUtility.get_corpus_documents(self.data_folder, self.basename).set_index('document_id')\n",
    "        print(\"Current model: \" + self.basename.upper())\n",
    "        \n",
    "        if self.on_set_model_callback is not None:\n",
    "            self.on_set_model_callback(self)\n",
    "            \n",
    "        # _fix_topictokens()\n",
    "        return self\n",
    "    \n",
    "    def get_document_topic_weights(self, year=None, topic_id=None):\n",
    "        df = self.document_topic_weights\n",
    "        if year is None and topic_id is None:\n",
    "            return df\n",
    "        if topic_id is None:\n",
    "            return df[(df.year == year)]\n",
    "        if year is None:\n",
    "            return df[(df.topic_id == topic_id)]\n",
    "        return df[(df.year == year)&(df.topic_id == topic_id)]\n",
    "    \n",
    "    def get_unique_topic_ids(self):\n",
    "        return self.document_topic_weights['topic_id'].unique()\n",
    "    \n",
    "    def get_topic_weight_by_year_or_document(self, key='mean', year=None):\n",
    "        pivot_column = 'year' if year is None else 'document_id'    \n",
    "        df = self.get_document_topic_weights(year) \\\n",
    "            .groupby([pivot_column,'topic_id']) \\\n",
    "            .agg(AGGREGATES[key])[['weight']].reset_index()\n",
    "        return df, pivot_column\n",
    "    \n",
    "    def get_topic_tokens_dict(self, topic_id, n_top=200):\n",
    "        return self.get_topic_tokens(topic_id)\\\n",
    "            .sort_values(['weight'], ascending=False)\\\n",
    "            .head(n_top)[['token', 'weight']]\\\n",
    "            .set_index('token').to_dict()['weight']\n",
    "\n",
    "    def compute_topic_terms_vector_space(self, n_words=100):\n",
    "        '''\n",
    "        Create an align topic-term vector space of top n_words from each topic\n",
    "        '''\n",
    "        unaligned_vector_dicts = ( self.get_topic_tokens_dict(topic_id, n_words) for topic_id in range(0, self.n_topics) )\n",
    "        X, feature_names = ModelUtility.compute_and_align_vector_space(unaligned_vector_dicts)\n",
    "        return X, feature_names\n",
    "\n",
    "    def get_lda(self):\n",
    "        raise Exception(\"Use of LDA model disabled in this Notebook\")\n",
    "        '''\n",
    "        Get gensim model. Only used for pyLDAvis display\n",
    "        '''\n",
    "        if self._lda is None:\n",
    "            filename = os.path.join(self.data_folder, self.basename, 'gensim_model_{}.gensim.gz'.format(self.basename))\n",
    "            if os.path.isfile(filename):\n",
    "                self._lda = LdaModel.load(filename)\n",
    "                print('LDA model loaded...')\n",
    "            else:\n",
    "                print('LDA not found on disk...')\n",
    "        return self._lda \n",
    "    \n",
    "    def get_topic_titles(self, n_words=100, cache=True):\n",
    "        if cache and self._topic_titles is not None:\n",
    "            return self._topic_titles\n",
    "        _topic_titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=n_words)\n",
    "        self._topic_titles = _topic_titles if cache else None\n",
    "        return _topic_titles\n",
    "    \n",
    "    def get_topic_tokens(self, topic_id, max_n_words=500):\n",
    "        tokens = state.topic_token_weights\\\n",
    "            .loc[lambda x: x.topic_id == topic_id]\\\n",
    "            .sort_values('weight',ascending=False)[:max_n_words]\n",
    "        return tokens\n",
    "    \n",
    "    def get_topic_alphas(self):\n",
    "        tokens = state.topic_token_weights\\\n",
    "            .loc[lambda x: x.topic_id == topic_id]\\\n",
    "            .sort_values('weight',ascending=False)[:max_n_words]\n",
    "        alpas = ModelUtility.get_topic_alphas\n",
    "        return tokens\n",
    "    \n",
    "    def get_topic_year_aggregate_weights(self, fn, threshold):\n",
    "        df = self.document_topic_weights\n",
    "        #df = df[(df.weight>=threshold)]\n",
    "        df = df.groupby(['year', 'topic_id']).agg(fn)['weight'].reset_index()\n",
    "        df = df[(df.weight>=threshold)]\n",
    "        return df\n",
    "    \n",
    "    def get_topic_proportions(self):\n",
    "        corpus_documents = self.get_corpus_documents()\n",
    "        document_topic_weights = self.get_document_topic_weights()\n",
    "        topic_proportion = ModelUtility.compute_topic_proportions(document_topic_weights, corpus_documents)\n",
    "        return topic_proportion\n",
    "    \n",
    "    def get_corpus_documents(self):\n",
    "        #if self.corpus_documents is None:\n",
    "        #    self.corpus_documents = ModelUtility.get_corpus_documents(self.data_folder, self.basename)\n",
    "        return self.corpus_documents\n",
    "\n",
    "    def on_set_model(self, callback):\n",
    "        self.on_set_model_callback = callback\n",
    "        return self\n",
    "        \n",
    "def on_set_model_handler(state):\n",
    "\n",
    "    if 'report_name' in state.corpus_documents:\n",
    "        return\n",
    "    \n",
    "    state.source_documents = pd.read_csv('data/SOU_1990_index.csv', sep='\\t', header=None, names=['year', 'report_id', 'report_name'])\n",
    "    state.corpus_documents['report_id'] = state.corpus_documents.document.str.split('_').apply(lambda x: x[1]).astype(np.int64)\n",
    "    state.corpus_documents['report_name'] = pd.merge(state.corpus_documents, state.source_documents, how='inner', on=['year', 'report_id']).report_name\n",
    "    state.corpus_documents['report_name'] = state.corpus_documents.apply(lambda x: '{}-{} {}'.format(x['year'], x['report_id'], x['report_name'])[:50], axis=1)\n",
    "    state.document_topic_weights['report_name'] = pd.merge(state.document_topic_weights, state.corpus_documents, left_on='document_id', right_index=True).report_name\n",
    "\n",
    "def select_model_main(state):\n",
    "    \n",
    "    basename_widget = widgets.Dropdown(\n",
    "        options=state.basenames,\n",
    "        value=state.basename,\n",
    "        description='Topic model',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='75%')\n",
    "    )\n",
    "    \n",
    "    w = widgets.interactive(select_model_handler, basename=basename_widget, state=widgets.fixed(state))\n",
    "    display(widgets.VBox((basename_widget,) + (w.children[-1],)))\n",
    "    w.update()\n",
    "\n",
    "state = ModelState('./data').on_set_model(on_set_model_handler)\n",
    "\n",
    "select_model_main(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Alpha Hyperparameter\n",
    "\n",
    "- See [Probabalistic Topic Models](http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf) for a description of LDA hyperparameters.\n",
    "- The **alpha** hyperparameter affects the document-topic distribution.\n",
    "- The LDA model is said to be *symmetric* if the same alpha value is used for all topics, and *assymetric* if it can vary per topic.\n",
    "- If a assymetric model, then high alphas can indicate a \"stopwords\" topic (frequent words), and low alphas can indicate bogus topics.\n",
    "- This chart is of no value for symmetric models. \n",
    "- See also: [stackexchange what-exactly-is-the-alpha-in-the-dirichlet-distribution](https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d907124ab67847e8835682797a1f158d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='alpha_plot' style='line-height: 20px;'>Hover topics to display words!<â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Alpha / Lambda Plot\n",
    "\n",
    "_topic_keys = ModelUtility.get_topic_keys(state.data_folder, state.basename)\n",
    "\n",
    "def plot_alpha(df):\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "    p = figure(x_range=df.topic.values,plot_width=900, plot_height=400, title='',\n",
    "               tools=TOOLS, toolbar_location=\"above\")\n",
    "    p.xaxis[0].axis_label = 'Topic'\n",
    "    p.yaxis[0].axis_label = 'Alpha'\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.y_range.start = 0.0\n",
    "    x_axis_type = 'enum'\n",
    "    p.xgrid.visible = False\n",
    "\n",
    "    glyph = bm.glyphs.VBar(x='topic', top='alpha', bottom=0, width=0.5, fill_color='color')\n",
    "    cr = p.add_glyph(source, glyph)\n",
    "\n",
    "    titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=100)\n",
    "    p.add_tools(bm.HoverTool(tooltips=None, callback=wf.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, 'alpha_plot'), renderers=[cr]))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def display_alpha(output_format, sort_by, window):\n",
    "    global state\n",
    "    palette = bokeh.palettes.PiYG[4]\n",
    "    topic_keys = ModelUtility.get_topic_keys(state.data_folder, state.basename).reset_index()\n",
    "    topic_keys = topic_keys[((topic_keys.alpha >= window[0]) & (topic_keys.alpha <= window[1]))]\n",
    "    topic_keys['topic'] = topic_keys.topic_id.apply(lambda x: str(x))\n",
    "    topic_keys['color'] = palette[1]  # topic_keys.alpha.apply(lambda x: palette[1] if x >= state.initial_alpha else palette[2])\n",
    "    if sort_by.lower() == 'alpha':\n",
    "        topic_keys = topic_keys.sort_values('alpha', axis=0)\n",
    "    if output_format == 'Chart':\n",
    "        p = plot_alpha(topic_keys)\n",
    "        show(p)\n",
    "    else:\n",
    "        source = bm.ColumnDataSource(topic_keys)\n",
    "        columns = [\n",
    "            TableColumn(field=\"topic_id\", title=\"ID\"),\n",
    "            TableColumn(field=\"alpha\", title=\"Alpha\"),\n",
    "            TableColumn(field=\"tokens\", title=\"Tokens\"),\n",
    "        ]\n",
    "        data_table = DataTable(source=source, columns=columns, width=950, height=600)\n",
    "        show(widgetbox(data_table))\n",
    "\n",
    "def plot_alpha_main():\n",
    "    \n",
    "    za = wf.BaseWidgetUtility(\n",
    "        text_id='alpha_plot',\n",
    "        text=wf.create_text_widget('alpha_plot',default_value='Hover topics to display words!'),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        sort_by=wf.create_select_widget('Sort by', ['Topic', 'Alpha'], default='Alpha'),\n",
    "        window=widgets.FloatRangeSlider(\n",
    "            description='Window',\n",
    "            min=0, max=state.max_alpha + 0.1,\n",
    "            step=0.01,\n",
    "            value=(0, state.max_alpha + 0.1),  # (state.initial_alpha, state.max_alpha + 0.1),\n",
    "            continuous_update=False\n",
    "        )\n",
    "    )\n",
    "    za.next_topic_id = za.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    wa = widgets.interactive(\n",
    "        display_alpha,\n",
    "        output_format=za.output_format,\n",
    "        sort_by=za.sort_by,\n",
    "        window=za.window\n",
    "    )\n",
    "    za.text.layout = widgets.Layout(width='95%') #  , height='120px')\n",
    "    wa.children[-1].layout = widgets.Layout(width='98%')\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        za.text,\n",
    "        widgets.HBox([za.output_format, za.window, za.sort_by]),\n",
    "        widgets.HBox([wa.children[-1]])\n",
    "    ]))\n",
    "    wa.update()\n",
    "    \n",
    "plot_alpha_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2226f05bee894eb9a6da6a0eea7736bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='dirichlet_alpha_plot' style='line-height: 20px;'>Hover topics to displâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dir(alpha) test sample\n",
    "def plot_dirichlet_alpha_sample(df):\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "    p = figure(x_range=df.topic.values,plot_width=900, plot_height=400, title='',\n",
    "               tools=TOOLS, toolbar_location=\"above\")\n",
    "    p.xaxis[0].axis_label = 'Topic'\n",
    "    p.yaxis[0].axis_label = 'Value'\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.y_range.start = 0.0\n",
    "    x_axis_type = 'enum'\n",
    "    p.xgrid.visible = False\n",
    "\n",
    "    glyph = bm.glyphs.VBar(x='topic', top='value', bottom=0, width=0.5, fill_color='color')\n",
    "    cr = p.add_glyph(source, glyph)\n",
    "\n",
    "    titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=100)\n",
    "    p.add_tools(bm.HoverTool(tooltips=None, callback=wf.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, 'dirichlet_alpha_plot'), renderers=[cr]))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def display_dirichlet_alpha_draw():\n",
    "    global state\n",
    "    palette = bokeh.palettes.PiYG[4]\n",
    "    topic_keys = ModelUtility.get_topic_keys(state.data_folder, state.basename).reset_index()\n",
    "    topic_keys['topic'] = topic_keys.topic_id.apply(lambda x: str(x))\n",
    "    topic_keys['color'] = palette[1] # topic_keys.alpha.apply(lambda x: palette[1] if x >= state.initial_alpha else palette[2])\n",
    "    topic_keys['value'] = np.random.dirichlet(topic_keys.alpha)\n",
    "\n",
    "    p = plot_dirichlet_alpha_sample(topic_keys)\n",
    "    show(p)\n",
    "\n",
    "def draw_dirichlet_alpha_main():\n",
    "    \n",
    "    zd = wf.BaseWidgetUtility(\n",
    "        text_id='dirichlet_alpha_plot',\n",
    "        text=wf.create_text_widget('dirichlet_alpha_plot',default_value='Hover topics to display words!'),\n",
    "    )\n",
    "    zd.refresh_button = widgets.Button(\n",
    "        description='Draw',\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Click me',\n",
    "        icon='check'\n",
    "    )\n",
    "\n",
    "    def on_refresh_button_clicked(b):\n",
    "        wd.update()\n",
    "\n",
    "    zd.refresh_button.on_click(on_refresh_button_clicked)\n",
    "\n",
    "    wd = widgets.interactive(display_dirichlet_alpha_draw)\n",
    "    zd.text.layout = widgets.Layout(width='95%')\n",
    "    wd.children[-1].layout = widgets.Layout(width='98%')\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        zd.text,\n",
    "        widgets.HBox([zd.refresh_button]),\n",
    "        widgets.HBox([wd.children[-1]])\n",
    "    ]))\n",
    "    wd.update()\n",
    "    \n",
    "draw_dirichlet_alpha_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents' Topic-Weight Distribution Frequency\n",
    "This graph displays **the distribution of document topic-weights** for the selected model. The X-axis percentage value between 0 and 100 and the Y-axis is the number of document topic-weights for each (integer) percentage. Not surprisingly is the vast majority (97-98)% of the weights zero, or close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c94be8042d47d49f713a542ea73c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectionRangeSlider(description='Interval', index=(0, 99), options=(0, 1, 2, 3, 4, 5, 6, 7, 8,â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Topic Weight Distribution\n",
    "\n",
    "def topic_weights_distribution_main():\n",
    "    \n",
    "    def display_topic_weights_distribution(p_range, distribution):\n",
    "        selection = distribution[p_range[0]:p_range[1]+1]\n",
    "        title = '{0:.2f}% of all document-topic weights are within selected interval'\\\n",
    "              .format(100 * (selection.sum() / distribution.sum()))\n",
    "        selection.plot(figsize=(12,6), title=title, kind='line', xlim=(0,100), ylim=(0,None))\n",
    "\n",
    "    distribution = state.get_document_topic_weights()\n",
    "    distribution['weight%'] = (distribution.weight * 100).astype('int')\n",
    "    distribution = distribution.groupby('weight%').size()\n",
    "\n",
    "    p_range = widgets.SelectionRangeSlider(\n",
    "        options=range(0,100), index=(0,99), description='Interval', continues_update=False\n",
    "    )\n",
    "\n",
    "    w = widgets.interactive(\n",
    "        display_topic_weights_distribution,\n",
    "        p_range=p_range,\n",
    "        distribution=widgets.fixed(distribution))\n",
    "    display(widgets.VBox(\n",
    "        (p_range,) +\n",
    "        (w.children[-1],)))\n",
    "    w.update()\n",
    "    \n",
    "topic_weights_distribution_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Word Distribution - Wordcloud and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c752f74a2d4340b44333678309fc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='tx02' style='line-height: 20px;'></span>\", placeholder=''), HBox(childâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_topic_distribution_widgets(callback, state, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=state.n_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', state.n_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(topic_id=0, n_words=100, output_format='Wordcloud', widget_container=None):\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = state.topic_token_weights.loc[(state.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = state.get_topic_titles(n_words=n_words, cache=True).iloc[topic_id]\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = state.get_topic_tokens(topic_id, n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(state.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "display_topic_distribution_widgets(display_wordcloud, state, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Word Distribution - Chart\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93fadff51ec4235b9d7bffa21f64804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='wc01' style='line-height: 20px;'></span>\", placeholder=''), HBox(childâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display topic's word distribution\n",
    "\n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = ColumnDataSource(tokens)\n",
    "\n",
    "    p = figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bm.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def plot_topic_tokens_charts(tokens, flag=True):\n",
    "\n",
    "    if flag:\n",
    "        left = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        show(left)\n",
    "        return\n",
    "\n",
    "    left = plot_topic_word_distribution(tokens, plot_width=450, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "    right = plot_topic_word_distribution(tokens, plot_width=450, plot_height=500, title='', tools='pan')\n",
    "\n",
    "    source = ColumnDataSource({'x':[], 'y':[], 'width':[], 'height':[]})\n",
    "    left.x_range.callback = create_js_callback('x', 'width', source)\n",
    "    left.y_range.callback = create_js_callback('y', 'height', source)\n",
    "\n",
    "    rect = bm.Rect(x='x', y='y', width='width', height='height', fill_alpha=0.0, line_color='blue', line_alpha=0.4)\n",
    "    right.add_glyph(source, rect)\n",
    "\n",
    "    show(row(left, right))\n",
    "\n",
    "def display_topic_tokens(topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    tokens = state.get_topic_tokens(topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        plot_topic_tokens_charts(tokens)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "    widget_container.reset()\n",
    "        \n",
    "display_topic_distribution_widgets(display_topic_tokens, state, 'wc01', ['Chart', 'Table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic's Trend Over Time or Documents\n",
    "- Displays topic's share over documents or time.\n",
    "- Note that source documents (i.e. SOU reports) are splitted into 1000 word chunks (LDA document) by the topic modelling process\n",
    "- If \"SOU Report\" or \"Year\" is selected then the **max** or **mean** weight is selected from corresponding LDA documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2328e9d672cc45d88fcc765d44910ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='topic_share_plot' style='line-height: 20px;'></span>\", placeholder='')â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "import numpy as np\n",
    "import math\n",
    "import bokeh.plotting\n",
    "from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "from bokeh.models.glyphs import VBar\n",
    "from bokeh.io import curdoc, show\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=800, title='', tools=TOOLS, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, pivot_config, value_column, widgets_container, output_format='Chart', state=None, threshold=0.01):\n",
    "    \n",
    "    pivot_column = pivot_config['pivot_column']\n",
    "    tokens = state.get_topic_titles(n_words=200, cache=True).iloc[topic_id]\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = value_column if pivot_column is not None else 'weight'\n",
    "    \n",
    "    df = state.document_topic_weights[(state.document_topic_weights.topic_id==topic_id)]\n",
    "    \n",
    "    if pivot_column is not None:\n",
    "        df = df.groupby([pivot_column]).agg([np.mean, np.max])['weight'].reset_index()\n",
    "        df.columns = [pivot_column, 'mean', 'max' ]\n",
    "        df = df[(df[value_column] > threshold)]\n",
    "        \n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = pivot_column.title()\n",
    "        y_label = value_column.title() + ('weight' if value_column != 'weight' else '')\n",
    "        p = plot_topic_trend(df, pivot_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        show(p)\n",
    "\n",
    "def create_topic_trend_widgets(state):\n",
    "    pivot_options = {\n",
    "        '': { 'pivot_column': None, 'filter': None },\n",
    "        'SOU Report': { 'pivot_column': 'report_name', 'filter': None },\n",
    "        'Year': { 'pivot_column': 'year', 'filter': None },\n",
    "        'LDA Document': { 'pivot_column': 'document_id', 'filter': None }\n",
    "    } \n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        #year=wf.create_select_widget('Year', options=state.years, value=state.years[-1]),\n",
    "        pivot_config=widgets.Dropdown(\n",
    "            options=pivot_options,\n",
    "            value=pivot_options['SOU Report'],\n",
    "            description='Group by'\n",
    "        ),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.05, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=state.n_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "        aggregate=widgets.Dropdown(options=['max', 'mean'], value='max', description='Aggregate')\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', state.n_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        pivot_config=wc.pivot_config,\n",
    "        value_column=wc.aggregate,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        state=widgets.fixed(state),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.pivot_config, wc.aggregate, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "    \n",
    "create_topic_trend_widgets(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic to Document Network\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseWidgetUtility' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-e45edf9282f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mmain_topic_year_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-e45edf9282f9>\u001b[0m in \u001b[0;36mmain_topic_year_network\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain_topic_year_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     zn = BaseWidgetUtility(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtext_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nx_id1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseWidgetUtility' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "     \n",
    "def plot_topic_year_network(network, layout, scale=1.0, titles=None):\n",
    "\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=TOOLS)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bm.HoverTool(renderers=[r_topics], tooltips=None, callback=WidgetUtility.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(\n",
    "        x='x', y='y', text='name', level='overlay',\n",
    "        x_offset=0, y_offset=0, text_font_size='8pt'\n",
    "    )\n",
    "    \n",
    "    p.add_layout(\n",
    "        bm.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bm.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_year_network(state):\n",
    "    \n",
    "    zn = BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='nx_id1',\n",
    "        text=wf.create_text_widget('nx_id1'),\n",
    "        year=wf.create_int_slider(\n",
    "            description='Year', min=state.min_year, max=state.max_year, step=1, value=state.min_year\n",
    "        ),\n",
    "        scale=wf.create_float_slider('Scale', min=0.0, max=1.0, step=0.01, value=0.1),\n",
    "        threshold=wf.create_float_slider('Threshold', min=0.0, max=1.0, step=0.01, value=0.10),\n",
    "        output_format=wf.create_select_widget('Format', ['Network', 'List', 'Pivot'], default='Network'),\n",
    "        layout=wf.create_select_widget('Layout', list(layout_algorithms.keys()), default='Fruchterman-Reingold'),\n",
    "        progress=wf.create_int_progress_widget(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    ) \n",
    "    \n",
    "    zn.previous = zn.create_prev_id_button('year', 10000)\n",
    "    zn.next = zn.create_next_id_button('year', 10000)    \n",
    "    \n",
    "    def display_topic_year_network(\n",
    "        layout_algorithm, threshold=0.10, scale=1.0, year=None, output_format='Network'\n",
    "    ):\n",
    "        zn.progress.value = 1\n",
    "        titles = state.get_topics_tokens_as_text()\n",
    "        df = state.get_document_topic_weights(year=year, topic_id=None)\n",
    "        df = df[(df.weight >= threshold)]\n",
    "        zn.progress.value = 2\n",
    "\n",
    "        network = NetworkUtility.create_bipartite_network(df, 'document', 'topic_id')\n",
    "        zn.progress.value = 3\n",
    "\n",
    "        if output_format == 'Network':\n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            zn.progress.value = 4\n",
    "            p = plot_topic_year_network(network, layout, scale=scale, titles=titles)\n",
    "            show(p)\n",
    "\n",
    "        elif output_format == 'List':\n",
    "            display(HTML(df.to_html()))\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        zn.progress.value = 0\n",
    "\n",
    "    wn = widgets.interactive(\n",
    "        display_topic_year_network, layout_algorithm=zn.layout,\n",
    "        threshold=zn.threshold, scale=zn.scale,\n",
    "        year=zn.year, output_format=zn.output_format\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox(\n",
    "        (zn.text, ) +\n",
    "        (widgets.HBox((zn.layout, ) + (zn.year,) + (zn.previous,) + (zn.next,)),) +\n",
    "        (widgets.HBox((zn.threshold,) + (zn.scale,) + (zn.output_format,)),) +\n",
    "        (zn.progress, ) +\n",
    "        (wn.children[-1],)))\n",
    "\n",
    "    wn.update()\n",
    "    \n",
    "main_topic_year_network(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Co-Occurence\n",
    "Computes weighted graph of topics co-occurring in the same document. Topics are defined as co-occurring if they both exists  in the same document both having weights above threshold. Weight are number of co-occurrences (binary yes or no). Node size reflects topic proportions over the entire corpus (normalized document) length, and are computed in accordance to how node sizes are computed in LDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca9f37acf6a4a2393217012bb904031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='cooc_id' style='line-height: 20px;'></span>\", placeholder=''), HBox(châ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize topic co-occurrence\n",
    "%run ./common/plot_utility\n",
    "G = None\n",
    "def display_topic_co_occurrence_network(layout, threshold, scale, output_format):\n",
    "\n",
    "    global state, zn\n",
    "    try:\n",
    "        metric = 'Threshold'\n",
    "        titles = state.get_topics_tokens_as_text()\n",
    "\n",
    "        if metric == 'Threshold':\n",
    "            df = state.get_document_topic_weights()\n",
    "            df = df.loc[(df.weight >= threshold)]\n",
    "            df = pd.merge(df, df, how='inner', left_on='document_id', right_on='document_id')\n",
    "            df = df.loc[(df.topic_id_x < df.topic_id_y)]\n",
    "            df = df.groupby([df.topic_id_x, df.topic_id_y]).size().reset_index()\n",
    "            df.columns = ['source', 'target', 'weight']\n",
    "\n",
    "        if output_format == 'Network':\n",
    "            network = NetworkUtility.create_network(df, source_field='source', target_field='target', weight='weight')\n",
    "            p = PlotNetworkUtility.plot_network(\n",
    "                network=network,\n",
    "                layout_algorithm=layout,\n",
    "                scale=scale,\n",
    "                threshold=0.0,\n",
    "                node_description=state.get_topics_tokens_as_text(),\n",
    "                node_proportions=state.get_topic_proportions(),\n",
    "                weight_scale=10.0,\n",
    "                normalize_weights=True,\n",
    "                element_id='cooc_id',\n",
    "                figsize=(900,500)\n",
    "            )\n",
    "            show(p)\n",
    "        elif output_format == 'List':\n",
    "            display(HTML(df.to_html()))\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "    except Exception as x:\n",
    "        print(\"No data: please adjust filters\")\n",
    "        \n",
    "zn = BaseWidgetUtility(\n",
    "    n_topics=state.n_topics,\n",
    "    text_id='cooc_id',\n",
    "    text=wf.create_text_widget('cooc_id'),\n",
    "    scale=wf.create_float_slider('Scale', min=0.0, max=1.0, step=0.01, value=0.1),\n",
    "    threshold=wf.create_float_slider('Threshold', min=0.0, max=1.0, step=0.01, value=0.35),\n",
    "    output_format=wf.create_select_widget('Format', ['Network', 'List', 'Pivot'], default='Network'),\n",
    "    layout=wf.create_select_widget('Layout', list(layout_algorithms.keys()), default='Fruchterman-Reingold'),\n",
    "    progress=wf.create_int_progress_widget(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    ") \n",
    "\n",
    "wn = widgets.interactive(\n",
    "    display_topic_co_occurrence_network,\n",
    "    layout=zn.layout,\n",
    "    threshold=zn.threshold,\n",
    "    scale=zn.scale,\n",
    "    output_format=zn.output_format\n",
    ")\n",
    "\n",
    "display(widgets.VBox(\n",
    "    (zn.text, ) +\n",
    "    (widgets.HBox((zn.layout, )),) +\n",
    "    (widgets.HBox((zn.threshold,) + (zn.scale,) + (zn.output_format,)),) +\n",
    "    (zn.progress, ) +\n",
    "    (wn.children[-1],)))\n",
    "\n",
    "wn.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Similarity Network\n",
    "This plot displays topic similarity based on **euclidean or cosine distances** between the **topic-to-word vectors**. Please note that the computations can take some time to exceute, especially for larger LDA models.\n",
    "\n",
    "1. Compute a multi dimensional topic vector space based on the top n words for each topic. Since the subset of words differs, and their positions differs between topics they need to be aligned in common space so that 1) each vector has the same dimension (i.e. number of unique top n tokens over all topics) and 2) each token has the same position within that space. (using sklearn DictVectorizer). The vector space will have as many dimensions as the number of unique top n words over all topics.\n",
    "2. Reduce the topic vector space into a 2D space (using sklearn PCA)\n",
    "3. Normalize the 2D space (sklearn Normalizer)\n",
    "\n",
    "Note: Steps 1 to 3 above (the most time consuming) are executed whenever an option marked with an asterix is changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309b46f55f264e6a919683419af7483f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='nx_id3' style='line-height: 20px;'></span>\", placeholder=''), HBox(chiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "\n",
    "# if 'zy_data' not in globals():\n",
    "correlation_network_state_data = types.SimpleNamespace(\n",
    "    basename=None,\n",
    "    network=None,\n",
    "    X_n_space=None,\n",
    "    X_n_space_feature_names=None,\n",
    "    distance_matrix=None,\n",
    "    metric=None,\n",
    "    topic_proportions=None,\n",
    "    n_words = 0\n",
    ")\n",
    "\n",
    "    \n",
    "def plot_clustering_dendogram(clustering):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html\n",
    "    R = dendrogram(clustering)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def VectorSpaceHelper_compute_distance_matrix(X_n_space, metric='euclidean'):\n",
    "    # https://se.mathworks.com/help/stats/pdist.html\n",
    "    metric = metric.lower()\n",
    "    if metric == 'kullbackâleibler': metric = VectorSpaceHelper.kullback_leibler_divergence\n",
    "    if metric == 'scipy.stats.entropy': metric = scipy.stats.entropy\n",
    "    #print(metric)\n",
    "    X = X_n_space.toarray() if hasattr(X_n_space, 'toarray') else X_n_space\n",
    "    #X_n_space += 0.00001\n",
    "    distances = distance.pdist(X, metric=metric)\n",
    "    #print(distances)\n",
    "    distance_matrix = distance.squareform(distances)\n",
    "    #print(distance_matrix)    \n",
    "    return distance_matrix\n",
    "\n",
    "def main_correlation_network(state, zy_data):\n",
    "\n",
    "    zy = BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='nx_id3',\n",
    "        text=wf.create_text_widget('nx_id3'),\n",
    "        scale=wf.create_float_slider('Scale', min=0.0, max=1.0, step=0.01, value=0.1),\n",
    "        year=wf.create_int_slider(\n",
    "            description='Year', min=state.min_year, max=state.max_year, step=1, value=state.min_year\n",
    "        ),\n",
    "        n_words=wf.create_int_slider(description='#words*', min=10, max=500, step=1, value=20),\n",
    "        metric=wf.create_select_widget(label='Metric*', values=list(DISTANCE_METRICS.keys()), default='Euclidean'),\n",
    "        threshold=wf.create_float_slider('Threshold', min=0.0, max=1.0, step=0.01, value=0.01),\n",
    "        output_format=wf.create_select_widget('Format', ['Network', 'List'], default='Network'),\n",
    "        layout=wf.create_select_widget('Layout', list(layout_algorithms.keys()), default='Fruchterman-Reingold'),\n",
    "        progress=wf.create_int_progress_widget(min=0, max=7, step=1, value=0, layout=widgets.Layout(width=\"90%\"))\n",
    "    ) \n",
    "    \n",
    "    def display_correlation_network(\n",
    "        layout_algorithm,\n",
    "        threshold=0.10,\n",
    "        scale=1.0,\n",
    "        metric='Euclidean',\n",
    "        n_words=200,\n",
    "        output_format='Network'\n",
    "    ):\n",
    "\n",
    "        try:\n",
    "\n",
    "            zy.progress.value = 1\n",
    "            metric = DISTANCE_METRICS[metric]\n",
    "\n",
    "            node_description = state.get_topics_tokens_as_text()\n",
    "            node_proportions = state.get_topic_proportions()\n",
    "\n",
    "            zy.progress.value = 2\n",
    "            if zy_data.network is None or state.basename != zy_data.basename or zy_data.metric != metric or zy_data.n_words != n_words:\n",
    "\n",
    "                zy_data.basename = state.basename\n",
    "                zy_data.n_words = n_words\n",
    "                zy_data.X_n_space, zy_data.X_n_space_feature_names = state.compute_topic_terms_vector_space(n_words=n_words)\n",
    "\n",
    "                #print(zy_data.X_n_space.shape)\n",
    "                #print(zy_data.X_n_space_feature_names)\n",
    "                zy.progress.value = 3\n",
    "                zy_data.distance_matrix = VectorSpaceHelper_compute_distance_matrix(zy_data.X_n_space, metric=metric)\n",
    "                zy_data.network = None\n",
    "\n",
    "            edges_data = VectorSpaceHelper.lower_triangle_iterator(zy_data.distance_matrix, threshold)\n",
    "\n",
    "            zy.progress.value = 4\n",
    "            if output_format == 'List':\n",
    "                df = pd.DataFrame(edges_data, columns=['x', 'y', 'weight'])\n",
    "                zy.progress.value = 5\n",
    "                display(HTML(df.to_html()))\n",
    "            else:\n",
    "                zy.progress.value = 5\n",
    "                if zy_data.network is None:\n",
    "                    zy_data.network = NetworkUtility.create_network_from_xyw_list(edges_data) # zy_data.distance_matrix)\n",
    "                zy.progress.value = 6\n",
    "                p = PlotNetworkUtility.plot_network(\n",
    "                    network=zy_data.network,\n",
    "                    layout_algorithm=layout_algorithm,\n",
    "                    scale=scale,\n",
    "                    threshold=threshold,\n",
    "                    node_description=node_description,\n",
    "                    node_proportions=node_proportions,\n",
    "                    element_id='nx_id3',\n",
    "                    figsize=(1000,600)\n",
    "                )\n",
    "                zy.progress.value = 6\n",
    "                show(p)\n",
    "\n",
    "            zy.progress.value = 7\n",
    "            zy.progress.value = 0\n",
    "        except Exception as ex:\n",
    "            # logger.exception(ex)\n",
    "            # print('Error: {}'.format(ex))\n",
    "            print('Empty set: please change filters')\n",
    "            zy.progress.value = 0\n",
    "\n",
    "\n",
    "    wy = widgets.interactive(\n",
    "        display_correlation_network,\n",
    "        layout_algorithm=zy.layout,\n",
    "        threshold=zy.threshold,\n",
    "        scale=zy.scale,\n",
    "        metric=zy.metric,\n",
    "        n_words=zy.n_words,\n",
    "        output_format=zy.output_format\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox(\n",
    "        (zy.text, ) +\n",
    "        (widgets.HBox((zy.threshold,) + (zy.metric,) + (zy.output_format,)),) +\n",
    "        (widgets.HBox((zy.n_words,) + (zy.layout,) + (zy.scale,)),) +\n",
    "        (zy.progress,) +\n",
    "        (wy.children[-1],)))\n",
    "\n",
    "    wy.update()\n",
    "\n",
    "    \n",
    "main_correlation_network(state, correlation_network_state_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test: Analyse Document Similarity\n",
    "The document similarity is computed using dimensionality reduction of document-topic distributions.\n",
    "\n",
    "- Is there an established method of identifying the most (topically) interesting documents?\n",
    "- Use a goodness of fit to test against uniform discrete density distribution?\n",
    "  Wasserstein distance? Chi-square? KS-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_similarity_distribution():\n",
    "    df = state.get_document_topic_weights()\n",
    "    X_m_n_sparse = compute_document_topic_vector_space(df)\n",
    "    matrix = VectorSpaceHelper.compute_distance_matrix(X_m_n_sparse, metric='cosine')\n",
    "    x_dim, y_dim = matrix.shape\n",
    "    items = ((i, j, matrix[i,j]) for i, j in product(range(0,x_dim), range(0,y_dim)) if i < j)\n",
    "    ns, nm, ws = list(zip(*items))\n",
    "    df = pd.DataFrame(dict(n=ns,m=nm,w=ws))\n",
    "    df['similarity'] = (df.w*1000).astype('int')\n",
    "    p = df.groupby('similarity').size().iloc[0:970].plot()\n",
    "    \n",
    "def compute_document_topic_vector_space(df):\n",
    "    #https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model\n",
    "    #''' Filter out topics below given threshold '''\n",
    "    #df = df[df.weight][['document_id', 'topic_id', 'weight']]\n",
    "\n",
    "    ''' Create a dict (pair) for each topic-weight row '''\n",
    "    df['weight_dict'] = df.apply(lambda x: { int(x.topic_id): x.weight}, axis=1)\n",
    "\n",
    "    ''' Create a list of all dicts for each documents'''\n",
    "    df = df.groupby('document_id')['weight_dict'].apply(list)\n",
    "\n",
    "    ''' Merge the list of pair dicts into a single dict '''\n",
    "    df = df.apply(lambda L: { k: v for d in L for k, v in d.items() } )\n",
    "\n",
    "    ''' Fit the topic weighs into a sparse matrix (dimensions m_documents X n_topics)'''\n",
    "    v = DictVectorizer()\n",
    "    X_m_n_sparse = v.fit_transform(df)\n",
    "\n",
    "    return X_m_n_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264d9eccac60416a8866423e025f6f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='nx_id4' style='line-height: 20px;'></span>\", placeholder=''), HBox(chiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# T-SNE 2D Visualization\n",
    "if 'ds_data' not in globals():\n",
    "    ds_data = types.SimpleNamespace(\n",
    "        X_m_n_sparse=None,\n",
    "        X_2_space=None,\n",
    "        threshold=None,\n",
    "        reducer=None,\n",
    "        perplexity=None,\n",
    "        G=None,\n",
    "        description=state.get_corpus_documents().rename(columns={'document': 'text'})['text']\n",
    "    )\n",
    "\n",
    "def plot_document_similarity_by_topics_tsne(threshold=0.001, reducer='tsne', perplexity=30):\n",
    "    global u, ds_data\n",
    "    \n",
    "    df = state.get_document_topic_weights()\n",
    "    \n",
    "    u.progress.value = 1\n",
    "    if ds_data.X_m_n_sparse is None:\n",
    "        ds_data.X_m_n_sparse = compute_document_topic_vector_space(df)\n",
    "        ds_data.threshold = threshold\n",
    "        ds_data.X_2_space = None\n",
    "    \n",
    "    u.progress.value = 2\n",
    "    if ds_data.X_2_space is None or ds_data.reducer != reducer\\\n",
    "            or ds_data.perplexity != perplexity or ds_data.threshold != threshold:\n",
    "        ds_data.X_2_space = VectorSpaceHelper.reduce_dimensions(\n",
    "            ds_data.X_m_n_sparse, method=reducer,\n",
    "            n_components=2, perplexity=perplexity)\n",
    "        ds_data.reduce = reducer\n",
    "        ds_data.perplexity = perplexity\n",
    "        \n",
    "    u.progress.value = 3\n",
    "\n",
    "    description = state.get_corpus_documents().rename(columns={'document': 'text'})['text']\n",
    "        \n",
    "    u.progress.value = 4\n",
    "    p = plot_2d_vector_space(ds_data.X_2_space, proportions=None,\n",
    "            size=(20,60), description=ds_data.description, dom_id='nx_id4', glyph_style=dict(alpha=0.05))\n",
    "    \n",
    "    u.progress.value = 5\n",
    "    show(p)\n",
    "    u.progress.value = 0\n",
    "\n",
    "u = BaseWidgetUtility()\n",
    "u.threshold = u.create_float_slider('Threshold', min=0.0, max=0.10, step=0.01, value=0.01)\n",
    "u.reducer = u.create_select_widget(label='Reducer*', values=['pca', 'pca_norm', 'tsne'], default='tsne')\n",
    "u.progress = u.create_int_progress_widget(min=0, max=5, step=1)\n",
    "u.perplexity = u.create_int_slider(description='Perplexity', min=1, max=100, step=1, value=30)\n",
    "u.text = u.create_text_widget(element_id='nx_id4')\n",
    "\n",
    "w = widgets.interactive(plot_document_similarity_by_topics_tsne,\n",
    "                threshold=u.threshold,\n",
    "                reducer=u.reducer,\n",
    "                perplexity=u.perplexity)\n",
    "\n",
    "display(widgets.VBox(\n",
    "    (u.text, ) +\n",
    "    (widgets.HBox((u.threshold,) + (u.reducer,) + (u.perplexity,) + (u.progress,)),) +\n",
    "    (w.children[-1],)))\n",
    "\n",
    "w.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some (assorted) references\n",
    "\n",
    "> Blei: https://scholar.google.com/citations?user=8OYE6iEAAAAJ\n",
    "\n",
    "- Blei, 2003: Latent dirichlet allocation [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "- Blei, 2012: Probabilistic topic models [PDF](https://pdfs.semanticscholar.org/01f3/290d6f3dee5978a53d9d2362f44daebc4008.pdf)\n",
    "- Blei, 2006: Dynamic topic models [PDF](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2036&context=compsci)\n",
    "- Introduction to Probabilistic Topic Models: [PDF](http://menome.com/wp/wp-content/uploads/2014/12/Blei2011.pdf)\n",
    "- Mcauliffe, Blei, 2008: Supervised topic models [PDF](http://papers.nips.cc/paper/3328-supervised-topic-models.pdf)\n",
    "- Grimmer, 2013: Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Politica\n",
    "[PDF](http://www.jstor.org/stable/pdf/24572662.pdf?casa_token=PnEPVj2gkkwAAAAA:_Vg_oSs-p6gtYvjJ3eEDUQB7UsakHQtBOdFIdeJxRpuGH5-7tq09fkUGxQ0-Bek5X2uOSya35-MoEo-cPo-K5DM1W-z1R0UppL6OqP53y6SNS7alAl8)\n",
    "- Chuang, 2013: Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment\n",
    "[PDF](http://vis.stanford.edu/files/2013-TopicModelDiagnostics-ICML.pdf)\n",
    "[Sup](http://vis.stanford.edu/files/2013-TopicalModelDiagnostics-SuppMaterial.pdf)\n",
    "- Lecture, Blei, 2009: [Video](http://videolectures.net/mlss09uk_blei_tm/) \n",
    "- Prof. David Blei - Probabilistic Topic Models and User Behavior [YoutTube](https://www.youtube.com/watch?v=FkckgwMHP2s)\n",
    "- PyData Berlin 2017 (Matti Lyra) [YouTube](https://www.youtube.com/watch?v=FkckgwMHP2s) [YouTube](https://www.youtube.com/watch?v=Dv86zdWjJKQ)\n",
    "[NB](https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb)\n",
    "- Probabilistic Topic Models: [PDF](https://pdfs.semanticscholar.org/01f3/290d6f3dee5978a53d9d2362f44daebc4008.pdf) [PDF](https://mimno.infosci.cornell.edu/info6150/readings/Blei2012.pdf)\n",
    "- Visualizing Topic Models: [PDF](http://ajbc.io/projects/papers/ChaneyBlei2012.pdf)\n",
    "- Topic models: [PDF](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.1205&rep=rep1&type=pdf#page=96)\n",
    "- Sievert, LDAvis: A method for visualizing and interpreting topics [PDF](http://www.aclweb.org/anthology/W14-3110)\n",
    "- Ted Underwood: [Blog](https://tedunderwood.com/category/methodology/topic-modeling/bayesian-topic-modeling/)\n",
    "- Stanford Topic Modeling Toolbox: [Link](https://nlp.stanford.edu/software/tmt/tmt-0.4/)\n",
    "- Blog, Naomi Saphra: Understanding Latent Dirichlet Allocation [Link](https://nsaphra.github.io/2012/07/09/LDA/)\n",
    "- blog.bogatron.net: Visualizing Dirichlet Distributions with Matplotlib [Link](http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)\n",
    "- Wikipedia: [Topic Model](https://en.wikipedia.org/wiki/Topic_model) [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "[Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)\n",
    "- Visualization using dimensionality reduction (e.g. T-SNE, PCA) [Shusen Liu, 2016], (pitfalls)\n",
    "http://qpleple.com/bib/#Chuang12\n",
    "http://qpleple.com/word-relevance/\n",
    "- Finding scientific topics: [PDF](http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf)\n",
    "\n",
    "### Powered by\n",
    "<img src=\"./images/powered_by.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
