{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text corpus processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr {\n",
    "    text-align: left;\n",
    "    vertical-align: top;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statens offentliga utredningar (SOU)\n",
    "\n",
    "Kungliga biblioteket har digitaliserat alla SOU:er publicerade mellan 1922 och 1999 (1129 stycken). Utredningarna kan laddas ned från http://regina.kb.se/sou/ i PDF-format. Filerna kan vara rätt stora, flera hundra MB, då de innehåller dels alla sidor scannade. De innehåller även inbäddad OCR:ad text vilket möjliggör textsökning. Kvaliteten av OCR är genomgående mycket hög.\n",
    "\n",
    "[<img src=\"./images/kb_regina_sou.jpg\" alt=\"http://regina.kb.se/sou/\" width=\"80%\"/>](http://regina.kb.se/sou/)\n",
    "         \n",
    "### Scannade dokument\n",
    "\n",
    "| SOU 1933:15        | Page 23 | \n",
    "| ------------- |:-------------|\n",
    "| [<img src=\"./images/kb-digark-2106487-page-0.jpg\" alt=\"1933:15 Utredning och förslag angående importmonopol på kaffe\" width=\"400\"/>](https://weburn.kb.se/sou/211/urn-nbn-se-kb-digark-2106487.pdf)      | <img src=\"./images/kb-digark-2106487-page-23.jpg\" alt=\"1933:15 Page 23\" width=\"400\"/> |\n",
    "\n",
    "### De i scannade bilderna har OCR-tolkats (KB)\n",
    "\n",
    "OCR (Optical Character Recognition) innebär maskinell tolkning bilder till text. Processen kan dra nytta av typiska återkommande fel, ordlistor etc. för att få ett bättre resultat. Kvalitet av text är mycket beroende av kvalitet av källmaterial och OCR-processen i sig.\n",
    "\n",
    "> 23 Enligt dessa siffror skulle sålunda **uuder** åren 1911—1913 43 % och år\n",
    "1931 65 % av Sveriges import av orostat kaffe ha varit av brasilianskt\n",
    "ursprung. Ingendera av dessa relationer är emellertid riktig, den första\n",
    "dock i vida mindre mån än den sista. Före kriget var Tyskland ännu en\n",
    "stor mellanhand för vår kaffehandel, liksom i viss utsträckning även\n",
    "Danmark och Holland, Före 1905 voro dessa länder, framför allt Tysk-\n",
    "land, såvitt kan slutas av den svenska handelsstatistiken, ännu mera do-\n",
    "minerande — vi köpte vid denna tid så gott som allt vårt kaffe genom\n",
    "affärshus i Hamburg, Rotterdam, Köpenhamn o. s. v. Efter tillkomsten\n",
    "år 1904 av ett nytt svenskt rederiföretag med trafik på Syd- och Central-\n",
    "amerika har bladet vänt sig. Redan år 1905 återfinnas Brasilien och\n",
    "Centralamerika i vår importstatistik med införselsiffror av resp. 4-5 och\n",
    "0\\*3 milj. kg. från att förut ha varit obefintliga, och sedan dess har den\n",
    "direkta införseln oavlåtligt stigit på den indirektas bekostnad. År 1931\n",
    "redovisas enligt tab. 15 den direkta importen till 85 % av den totala och\n",
    "importen från Brasilien till 65 %. I verkligheten torde omkr. 70—75 %\n",
    "av vårt kaffe vara av brasilianskt ursprung. Danmark, Holland, Tysk-\n",
    "land och England förmedla väsentligen vår import av dyrare kaffesor-\n",
    "ter från Guatemala (vars export till stor del behärskas av den tyska\n",
    "handeln), Mexiko, Costa Rica, Haiti, Java o. s. v.\n",
    "\n",
    "Mer strukturerad information på sidan såsom tabeller, innehållsförteckning, index etc kan vara mycket problematiska, med ofta många fel i scanningen (beroende av material):\n",
    "\n",
    "> T a b . 16. E x p o r t å r 1930 av brasilianskt kaffe.\n",
    "U. S. A\n",
    "Frankrike\n",
    "Tyskland\n",
    "Holland\n",
    "Italien\n",
    "Argentina • • • -\n",
    "Sverige\n",
    "Belgien\n",
    "Övriga\n",
    "Summa\n",
    "1 000 säckar\n",
    "å 60 kg.\n",
    "8 006\n",
    "1995\n",
    "912\n",
    "862\n",
    "781\n",
    "482\n",
    "444\n",
    "410\n",
    "1399\n",
    "15 291\n",
    "52-4\n",
    "13-o\n",
    "6-0\n",
    "5-6\n",
    "5 i\n",
    "3-2\n",
    "2-9\n",
    "27\n",
    "9 1\n",
    "lOOo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vilken information finns i texten?\n",
    "\n",
    "- Geografiska platser på olika nivåer (länder, städer, världsdelar)\n",
    "- Temporal information (år)\n",
    "- Olika typer av **metriska värden**\n",
    "- Ordklasser såsom substantiv, verb\n",
    "- Prepositioner, konjunktioner kanske inte säger så mycket?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### Geografiska platser\n",
    "\n",
    "> Enligt dessa siffror skulle sålunda uuder åren 1911—1913 43 % och år\n",
    "1931 65 % av **Sveriges** import av orostat kaffe ha varit av **brasilianskt**\n",
    "ursprung. Ingendera av dessa relationer är emellertid riktig, den första\n",
    "dock i vida mindre mån än den sista. Före kriget var **Tyskland** ännu en\n",
    "stor mellanhand för vår kaffehandel, liksom i viss utsträckning även\n",
    "**Danmark** och **Holland**, Före 1905 voro dessa länder, framför allt **Tyskland**,\n",
    "såvitt kan slutas av den svenska handelsstatistiken, ännu mera dominerande —\n",
    "vi köpte vid denna tid så gott som allt vårt kaffe genom\n",
    "affärshus i **Hamburg**, **Rotterdam**, **Köpenhamn** o. s. v. Efter tillkomsten\n",
    "år 1904 av ett nytt **svenskt** rederiföretag med trafik på **Syd- och Centralamerika**\n",
    "har bladet vänt sig. Redan år 1905 återfinnas **Brasilien** och\n",
    "**Centralamerika** i vår importstatistik med införselsiffror av resp. 4-5 och\n",
    "0\\*3 milj. kg. från att förut ha varit obefintliga, och sedan dess har den\n",
    "direkta införseln oavlåtligt stigit på den indirektas bekostnad. År 1931\n",
    "redovisas enligt tab. 15 den direkta importen till 85 % av den totala och\n",
    "importen från **Brasilien** till 65 %. I verkligheten torde omkr. 70—75 %\n",
    "av vårt kaffe vara av **brasilianskt** ursprung. **Danmark**, **Holland**, **Tyskland** och **England**\n",
    "förmedla väsentligen vår import av dyrare kaffesorter\n",
    "från **Guatemala** (vars export till stor del behärskas av den **tyska**\n",
    "handeln), **Mexiko**, **Costa Rica**, **Haiti**, **Java** o. s. v.*\n",
    "    \n",
    "### Tidsmässiga angivelser\n",
    "\n",
    "> Enligt dessa siffror skulle sålunda uuder **åren 1911—1913** 43 % och **år\n",
    "1931** 65 % av Sveriges import av orostat kaffe ha varit av brasilianskt\n",
    "ursprung. Ingendera av dessa relationer är emellertid riktig, den första\n",
    "dock i vida mindre mån än den sista. **Före kriget** var Tyskland ännu en\n",
    "stor mellanhand för vår kaffehandel, liksom i viss utsträckning även\n",
    "Danmark och Holland, **Före 1905** voro dessa länder, framför allt Tyskland,\n",
    "såvitt kan slutas av den svenska handelsstatistiken, ännu mera dominerande —\n",
    "vi köpte **vid denna tid** så gott som allt vårt kaffe genom\n",
    "affärshus i Hamburg, Rotterdam, Köpenhamn o. s. v. Efter tillkomsten\n",
    "**år 1904** av ett nytt svenskt rederiföretag med trafik på Syd- och Centralamerika\n",
    "har bladet vänt sig. Redan **år 1905** återfinnas Brasilien och\n",
    "Centralamerika i vår importstatistik med införselsiffror av resp. 4-5 och\n",
    "0\\*3 milj. kg. från att förut ha varit obefintliga, och sedan dess har den direkta införseln oavlåtligt stigit på den indirektas bekostnad. År **1931**\n",
    "redovisas enligt tab. 15 den direkta importen till 85 % av den totala och\n",
    "av vårt kaffe vara av brasilianskt ursprung. Danmark, Holland, Tyskland och England\n",
    "förmedla väsentligen vår import av dyrare kaffesorter\n",
    "från Guatemala (vars export till stor del behärskas av den tyska\n",
    "handeln), Mexiko, Costa Rica, Haiti, Java o. s. v.\n",
    "\n",
    "### Vilka är nyckelorden?\n",
    "\n",
    "> kanske **kaffe, handel, statistik, (länder), (år)**?\n",
    "\n",
    "### Bara substantiv och egennamn (enligt Språkbanken Sparv)\n",
    "\n",
    "> siffror uuder åren % år % Sveriges import kaffe ursprung relationer mån kriget Tyskland mellanhand kaffehandel utsträckning Danmark Holland länder Tyskland handelsstatistiken tid kaffe affärshus Hamburg Rotterdam Köpenhamn s. v. tillkomsten år rederiföretag trafik Syd- Centralamerika bladet år Brasilien Centralamerika importstatistik införselsiffror \\ milj. kg införseln bekostnad År importen % kaffe ursprung Danmark Holland Tyskland England import kaffesorter Guatemala export del handeln Mexiko Costa Rica Haiti Java s. v.\n",
    "\n",
    "### Bara substantiv och egennamn i deras grundform (enligt Språkbanken Sparv)\n",
    "\n",
    "> siffra uuder år % år % Sverige import kaffe ursprung relation mån krig Tyskland mellanhand kaffehandel utsträckning Danmark Holland land Tyskland handelsstatistik tid kaffe affärshus Hamburg Rotterdam Köpenhamn s. o. s. v.:075 tillkomst år rederiföretag trafik Syd- Centralamerika blad år Brasilien Centralamerika importstatistik införselsiffror \\ milj. kg införsel bekostnad år import % kaffe ursprung Danmark Holland Tyskland England import kaffesort Guatemala export del handel Mexiko Costa Rica Costa Rica:32 Haiti Java s. o. s. v.\n",
    "\n",
    "### Efter enklare städningar\n",
    "Vi ser mycket skräp och termer som inte är ord. Det är vanligt att städa texten genom att ex.\n",
    "- ändra texten till genomgående små bokstäver\n",
    "- ta bort termer som inte är är kort (ex. ett tecken)\n",
    "- ta bort termer som innehåller siffror\n",
    "- ta bort mest frekventa termerna (ofta kallade för stoppord)\n",
    "- rad avslutas med \"-\" så ta bort \"-\" och radbrytning dvs slå samman avstavat ord\n",
    "\n",
    "Korta ord borttagna, samt ord som saknar alfanumerisk bokstav, samt ord ändrade till genomgående små bokstäver\n",
    "\n",
    "> siffra uuder sverige import kaffe ursprung relation mån krig tyskland mellanhand kaffehandel utsträckning danmark holland land tyskland handelsstatistik tid kaffe affärshus hamburg rotterdam köpenhamn v.:075 tillkomst rederiföretag trafik syd- centralamerika blad brasilien centralamerika importstatistik införselsiffror milj införsel bekostnad import kaffe ursprung danmark holland tyskland england import kaffesort guatemala export del handel mexiko costa rica costa rica:32 haiti java\n",
    "\n",
    "**Top-10 lista av ord efter ordfrekvens:**\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "    <thead>\n",
    "        <tr style=\"text-align: left;\">\n",
    "          <th>kaffe</th>\n",
    "          <th>import</th>\n",
    "          <th>tyskland</th>\n",
    "          <th>danmark</th>\n",
    "          <th>costa</th>\n",
    "          <th>ursprung</th>\n",
    "          <th>holland</th>\n",
    "          <th>centralamerika</th>\n",
    "          <th>sverige</th>\n",
    "          <th>kaffehandel</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stoppordslista i ramverket NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(' '.join(nltk.corpus.stopwords.words('swedish')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can this be automised with Python?\n",
    "This example uses the entire text in SOU 1933-15.\n",
    "\n",
    "**Tokenization**\n",
    "- The texts is, from a systems perspective, just a linear sequence of characters.\n",
    "- We want to process the text into syntactical units, in this case words (could also be sentences etc).\n",
    "- This process, called **tokenization**, uses \"white spaces\" (space, tab, new line etc) and punctuations to split the text.\n",
    "- As a result we get a sequence (ordered list) of tokens/words.\n",
    "- Most NLP frameworks have built in functions for various kinds of tokenizations (text, tweets, regexp).\n",
    "- In this case we filter out punctuations.\n",
    "\n",
    "**Basic cleaning (filter and transformation) **\n",
    "- We (might) want to remove characters (e.g. punctuations) or words deemed less important.\n",
    "- Other candidates for removal are short words, common words, stopwords, numerals, certain part-of-speech,...?\n",
    "- We might also want reduce the size of the vocabulary (condense the text)\n",
    "-  Transform all words to lower case\n",
    "-  Use the root form of words (stemming or lemmatizing)\n",
    "\n",
    "The flow in this example is:\n",
    "1. Read the entire text from a file on disk\n",
    "2. Make any necessary transformations to the text as a whole (hyphenation)\n",
    "3. Split the text into a sequence of lexical/syntactical units (words, possibly also sentences and paragraphs)\n",
    "4. Transform and/or filter the words\n",
    "5. Display the result in different ways\n",
    "\n",
    "N.B. The charts seem to follow [Zip's law](https://simple.wikipedia.org/wiki/Zipf%27s_law):\n",
    "> *Zipf's law states that given a large sample of words used, the frequency of any word is inversely proportional to its rank in the frequency table...\n",
    "...Given a set of Zipfian distributed frequencies, sorted from most common to least common, the second most common frequency will occur ½ as often as the first. The third most common frequency will occur ⅓ as often as the first. The fourth most common frequency will occur ¼ as often as the first. The nth most common frequency will occur 1⁄n as often as the first...* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# folded code\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "hyphen_regexp = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        document = f.read()\n",
    "    return document\n",
    "\n",
    "def tokenize_and_sanitize(document, de_hyphen=True, min_length=3, only_isalpha=True, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "        \n",
    "    # hantera avstavningar\n",
    "    if de_hyphen:\n",
    "        document = re.sub(hyphen_regexp, r\"\\1\\2\\n\", document)\n",
    "\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    \n",
    "    # Ta bort ord kortare än tre tecken\n",
    "    if min_length > 0:\n",
    "        tokens = [ x for x in tokens if len([w for w in x if w.isalpha()]) > min_length ]\n",
    "        \n",
    "    # Ta bort ord som inte innehåller någon siffra eller bokstav\n",
    "    if only_isalpha:\n",
    "        tokens = [ x for x in tokens if x.isalpha() ]\n",
    "\n",
    "    if remove_puncts:\n",
    "        tokens = [ x.translate(punct_table) for x in tokens ]\n",
    "\n",
    "    # Transformera till små bokstäver\n",
    "    if to_lower:\n",
    "        tokens = [ x.lower() for x in tokens ]\n",
    "        \n",
    "    # Ta bort de vanligaste stoporden\n",
    "    if remove_stop:\n",
    "        stopwords = nltk.corpus.stopwords.words('swedish')\n",
    "        tokens = [ x for x in tokens if x not in stopwords ]\n",
    "        \n",
    "    return [ x for x in tokens if len(x) > 0 ]\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    \n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(8, 8 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "container=dict(\n",
    "    display_type=widgets.Dropdown(\n",
    "        description='Show',\n",
    "        options={\n",
    "            'Source text': 'source_text',\n",
    "            'Sanitized text': 'sanitized_text',\n",
    "            'Statistics': 'statistics'           \n",
    "    }),\n",
    "    min_length=widgets.IntSlider(value=0, min=0, max=5, step=1, description='Min alpha', tooltip='Min number of alphabetic characters'),\n",
    "    de_hyphen=widgets.ToggleButton(value=False, description='Dehyphen', disabled=False, tooltip='Fix hyphens', icon=''),\n",
    "    to_lower=widgets.ToggleButton(value=False, description='Lowercase', disabled=False, tooltip='Transform text to lowercase', icon=''),\n",
    "    remove_stop=widgets.ToggleButton(value=False, description='No stopwords', disabled=False, tooltip='Remove stopwords', icon=''),\n",
    "    only_isalpha=widgets.ToggleButton(value=False, description='Only alpha', disabled=False, tooltip='Keep only alphabetic words', icon=''),\n",
    "    remove_puncts=widgets.ToggleButton(value=False, description='Remove puncts.', disabled=False, tooltip='Remove punctioations characters', icon=''),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='' )\n",
    ")\n",
    "\n",
    "output1 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "output2 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "default_output = None\n",
    "\n",
    "tokens = []\n",
    "\n",
    "def display_document(display_type, to_lower, remove_stop, only_isalpha, remove_puncts, min_length, de_hyphen, word_count=500):\n",
    "\n",
    "    global tokens\n",
    "    \n",
    "    p =  container['progress']\n",
    "    p.value = 0\n",
    "    try:\n",
    "        output1.clear_output()\n",
    "        output2.clear_output()\n",
    "        default_output.clear_output()\n",
    "        document = read_text_file('./data/urn-nbn-se-kb-digark-2106487.txt')\n",
    "        p.value = p.value + 1\n",
    "\n",
    "        if display_type == 'source_text':\n",
    "            # Utskrift av de första och sista 250 tecknen:\n",
    "            with output1:\n",
    "                print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            p.value = p.value + 1\n",
    "            return\n",
    "        \n",
    "        p.value = p.value + 1\n",
    "\n",
    "        tokens = tokenize_and_sanitize(\n",
    "            document,\n",
    "            de_hyphen=de_hyphen,\n",
    "            min_length=min_length,\n",
    "            only_isalpha=only_isalpha,\n",
    "            remove_puncts=remove_puncts,\n",
    "            to_lower=to_lower,\n",
    "            remove_stop=remove_stop\n",
    "        )\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics']:\n",
    "            \n",
    "            p.value = p.value + 1\n",
    "            \n",
    "            if display_type == 'sanitized_text':\n",
    "                with output1:\n",
    "                    display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                        ' '.join(tokens[:word_count]),\n",
    "                        ' '.join(tokens[-word_count:])\n",
    "                    ))\n",
    "                p.value = p.value + 1\n",
    "                return\n",
    "            \n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(tokens)\n",
    "                p.value = p.value + 1\n",
    "                \n",
    "                with output1:\n",
    "                    \n",
    "                    df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                    display(df)\n",
    "                \n",
    "                with output2:\n",
    "                    \n",
    "                    print('Antal ord (termer): {}'.format(wf.N()))\n",
    "                    print('Antal unika termer (vokabulär): {}'.format(wf.B()))\n",
    "                    print(' ')\n",
    "                    \n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "                    \n",
    "                    wf = nltk.FreqDist([len(x) for x in tokens])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "    \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        p.value = 0\n",
    "\n",
    "i_widgets = widgets.interactive(display_document, **container)\n",
    "default_output = i_widgets.children[-1]\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        container['display_type'],\n",
    "        container['to_lower'],\n",
    "        container['remove_stop'],\n",
    "        container['de_hyphen'],\n",
    "        container['only_isalpha'],\n",
    "        container['remove_puncts']\n",
    "    ]),\n",
    "    widgets.HBox([container['min_length'], container['progress']]),\n",
    "    widgets.HBox([output1, output2]),\n",
    "    default_output\n",
    "]))\n",
    "\n",
    "i_widgets.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion plots (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "\n",
    "def create_sample_corpus(de_hyphen=True, min_length=2, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "    document = read_text_file('./data/urn-nbn-se-kb-digark-2106487.txt')\n",
    "    tokens = tokenize_and_sanitize(\n",
    "        document,\n",
    "        de_hyphen=de_hyphen,\n",
    "        min_length=min_length,\n",
    "        only_isalpha=False,\n",
    "        remove_puncts=remove_puncts,\n",
    "        to_lower=to_lower,\n",
    "        remove_stop=remove_stop\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "def plot_dispersion(de_hyphen=True, min_length=2, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "    \n",
    "    tokens = create_sample_corpus(de_hyphen=de_hyphen,\n",
    "        min_length=min_length,\n",
    "        remove_puncts=remove_puncts,\n",
    "        to_lower=to_lower,\n",
    "        remove_stop=remove_stop\n",
    "    )\n",
    "\n",
    "    if 'tokens' not in globals() or len(tokens or []) == 0:\n",
    "        print('Please specify token set using previous cell')\n",
    "        return\n",
    "\n",
    "    text = nltk.text.Text(tokens)\n",
    "\n",
    "    wf = nltk.FreqDist(tokens)\n",
    "\n",
    "    top_words = wf.most_common(10)\n",
    "    \n",
    "    text.dispersion_plot([ x for (x,y) in top_words])\n",
    "\n",
    "plot_dispersion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech tagging (POS)\n",
    "\n",
    "This is an example on how to POS tag text using an online web API.\n",
    "\n",
    "> - Part-of-speech tagging (POS) is the process of assigning word classes to each word.\n",
    "> - The Språkbanken Sparv API is an online tool that can be used for POS tagging\n",
    "> - Sparv does a lot more: tokenization, dependency analysis, lemmatization, named entity recognition, sentiment analysis, ...\n",
    "> - Sparv uses HunPOS (insert reference) for POS tagging\n",
    "\n",
    "POS tagging is a common NLP task and can be used to improve quality of various NLP methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech taggning (and Dependency Analysis)\n",
    "\n",
    "https://spraakbanken.gu.se/korp/markup/msdtags.html\n",
    "CoreNLP (Stanford Tagger)\n",
    "https://spraakbanken.gu.se/sparv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456e83cbf37342cea02dcb1f1d999856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Textarea(value='Detta är ett bra exempel på en text som kan annoteras via Sparv.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# folded code\n",
    "import requests\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "\n",
    "settings={\n",
    "    \"corpus\": \"exempelkorpus\",\n",
    "    \"lang\": \"sv\",\n",
    "    \"textmode\": \"plain\",\n",
    "    \"word_segmenter\": \"default_tokenizer\",\n",
    "    \"sentence_segmentation\": {\n",
    "        \"sentence_chunk\": \"paragraph\",\n",
    "        \"sentence_segmenter\": \"default_tokenizer\"\n",
    "    },\n",
    "    \"paragraph_segmentation\": {\n",
    "        \"paragraph_segmenter\": \"blanklines\"\n",
    "    },\n",
    "    \"positional_attributes\": {\n",
    "        \"lexical_attributes\": [ \"pos\", \"msd\", \"lemma\", ],\n",
    "        \"compound_attributes\": [ ],\n",
    "        \"dependency_attributes\": [ ],\n",
    "        \"sentiment\": [ ]\n",
    "    },\n",
    "    \"named_entity_recognition\": [ ],\n",
    "    \"text_attributes\": {\n",
    "        \"readability_metrics\": [ ]\n",
    "    }\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/xml\",\n",
    "}\n",
    "\n",
    "cc=dict(\n",
    "    text_widget = widgets.Textarea(\n",
    "        value='Detta är ett bra exempel på en text som kan annoteras via Sparv.',\n",
    "        placeholder='Type some',\n",
    "        description='Text:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width=\"400px\")\n",
    "    ),\n",
    "    button=widgets.Button(\n",
    "        description='Annotate it!',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "        tooltip='Use Språkbanken Sparv to annotate text!',\n",
    "        icon=''\n",
    "    ),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=3, step=1, description='' )\n",
    ")\n",
    "\n",
    "cc_output = widgets.Output() #layout={'border': '1px solid black'})\n",
    "\n",
    "def call_spraakbanken(text):\n",
    "    p = cc['progress']\n",
    "    try:\n",
    "        cc_output.clear_output()\n",
    "        data = {\n",
    "            'text': text,\n",
    "        } \n",
    "        url = \"https://ws.spraakbanken.gu.se/ws/sparv/v2/?settings={}\".format(json.dumps(settings))\n",
    "\n",
    "        p.value = 1\n",
    "        response = requests.post(url=url, headers=headers, data=data) \n",
    "        p.value = 2\n",
    "        with cc_output:\n",
    "            print(response.text)\n",
    "        p.value = 3\n",
    "    except Exception as ex:\n",
    "        raise\n",
    "    finally:\n",
    "        p.value = 0\n",
    "        \n",
    "from IPython.display import display\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    text = cc['text_widget'].value\n",
    "    call_spraakbanken(text)\n",
    "\n",
    "cc['button'].on_click(on_button_clicked)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([cc['text_widget'], cc['button'], cc['progress'] ]),\n",
    "    cc_output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpora.corpus_source_reader import SparvCorpusSourceReader\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#content = read_text_file('./data/1933_15.xml')\n",
    "#alto_parser = SparvCorpusSourceReader(xslt_filename='./corpora/alto_xml_extract.xslt', postags='|NN|', lemmatize=False)\n",
    "#text = alto_parser.transform(content)\n",
    "\n",
    "reader = SparvCorpusSourceReader('./data/1933_15.xml', postags='|NN|')\n",
    "\n",
    "[ document for document in reader ]\n",
    "\n",
    "#def tokenize_and_sanitize(document, de_hyphen=True, min_length=3, only_isalpha=True, remove_puncts=True, to_lower=True, remove_stop=True):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Läs in SOU ALTO-XML\n",
    "\n",
    "Välj dokument och\n",
    "1. Visa frekvens per ORDCLASS\n",
    "2. Visa vanligast ord per ORDKLASS (word or lemma)\n",
    "3. Visa BOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\ipywidgets\\widgets\\interaction.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwidget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_interact_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwarg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mshow_inline_matplotlib_plots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_display\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-22b2ccb6f05e>\u001b[0m in \u001b[0;36mdisplay_most_frequent_pos_tags\u001b[1;34m(document_id, category, top)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mlocation_freqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m#location_freqs.tabulate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mplot_freqdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation_freqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-22b2ccb6f05e>\u001b[0m in \u001b[0;36mplot_freqdist\u001b[1;34m(wf, n, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_freqdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_csv('./data/SOU_1990_total_ner_extracted.csv', sep='\\t', names=['filename', 'year', 'location', 'categories', 'entity'])\n",
    "\n",
    "entities['document_id'] = entities.filename.apply(lambda x: int(x.split('_')[1]))\n",
    "entities['categories'] = entities.categories.str.replace('/', ' ')\n",
    "entities['category'] = entities.categories.str.split(' ').str.get(0)\n",
    "entities['sub_category'] = entities.categories.str.split(' ').str.get(1)\n",
    "\n",
    "entities.drop(['location', 'categories'], inplace=True, axis=1)\n",
    "\n",
    "document_names = pd.read_csv('./data/SOU_1990_index.csv',\n",
    "                             sep='\\t',\n",
    "                             names=['year', 'sequence_id', 'report_name']).set_index('sequence_id')\n",
    "\n",
    "def plot_freqdist(wf, n=25, **kwargs):\n",
    "    data = list(zip(*wf.most_common(n)))\n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(13, 13/1.618))\n",
    "    plt.plot(x, y, '--ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.show()\n",
    "\n",
    "doc_names = { v: k for k, v in document_names.report_name.to_dict().items()}\n",
    "doc_names['All documents'] = 0\n",
    "@widgets.interact(category=entities.category.unique())\n",
    "def display_most_frequent_pos_tags(document_id=doc_names, category='LOC', top=10):\n",
    "    global entities\n",
    "    locations = entities\n",
    "    if document_id > 0:\n",
    "        locations = locations.loc[locations.document_id==document_id]\n",
    "    locations = locations.loc[locations.category==category]['entity']\n",
    "    location_freqs = nltk.FreqDist(locations)\n",
    "    #location_freqs.tabulate()\n",
    "    plot_freqdist(location_freqs, n=top)\n",
    "\n",
    "# display_most_frequent_pos_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diskursanalys - vad handlar texten om?\n",
    "Vilka ord, eller typer av ord kan \"sammanfatta\" vad texten handlar om - utan att redovisa specifika detaljer?  Nu tittar vi på all text i hela SOU-dokumentet ovan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction\n",
    "\n",
    "Keyword extraction är ett enkelt sätt att skapa en översikt över texter genom att identifiera nyckelord i texten enligt den rankingalgoritm som metoden använder. TF-IDF, RAKE och TextRank är tre algoritmer för keyword extraction. RAKE viktar ord i ett enskilt dokument, utan hänsyn till föreomst i andra dokument, medan TF-IDF viktar (omvänt, inverterat) wordfrekvensen (TF, term frequency) i ett enskilt dokument mot hur ofta ordet förekommer i något dokument i hela korpuset (DF, document frequency).\n",
    "\n",
    "References\n",
    "- [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- [Automatic keyword extraction from individual documents by Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley](https://www.researchgate.net/profile/Stuart_Rose/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents/links/55071c570cf27e990e04c8bb.pdf)\n",
    "- [TextRank: Bringing Order into Texts by Rada Mihalcea and Paul Tarau](https://github.com/davidadamojr/TextRank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from corpora.zip_utility import ZipReader, store_documents_to_archive\n",
    "from corpora.corpus_source_reader import SparvCorpusSourceReader\n",
    "\n",
    "default_transforms = [\n",
    "    lambda tokens: [ x.lower() for x in tokens ],\n",
    "    lambda tokens: [ x for x in tokens if len(x) > 2 ],\n",
    "    lambda tokens: [ x for x in tokens if any([c for c in x if c.isalpha()])],\n",
    "    lambda tokens: [ x for x in tokens if x not in nltk.corpus.stopwords.words('swedish') ]\n",
    "]\n",
    "\n",
    "def convert_alto_xml_to_text(source_archive, target_archive, postags=\"\", lemmatize=False, transforms=None, xslt_filename=None):\n",
    "    source = ZipReader(source_archive, pattern='*.xml', filenames=None)\n",
    "    documents = SparvCorpusSourceReader(source=source,\n",
    "                                        transforms=(transforms or []),\n",
    "                                        postags=postags,\n",
    "                                        lemmatize=lemmatize,\n",
    "                                        chunk_size=None,\n",
    "                                        xslt_filename=xslt_filename)\n",
    "    store_documents_to_archive(target_archive, documents)\n",
    "    \n",
    "# convert_alto_xml_to_text('./data/1945_10.zip', './data/1945_10_raw_text.zip', transforms=None, xslt_filename='./corpora/alto_xml_to_text.xslt')\n",
    "convert_alto_xml_to_text('./data/SOU_1990.zip', './data/1990_nnpm_lc_gt2_nsw_lemma_text.zip', postags=\"|NN|PM|\",\n",
    "                         lemmatize=True, transforms=default_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\usr\\python36\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CompressedFileReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f861e0aa359c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_demo_text_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/1945_10.zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"''\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f861e0aa359c>\u001b[0m in \u001b[0;36mget_demo_text_corpus\u001b[1;34m(archive, postags, lemmatize, transforms)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_demo_text_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|NN|PM|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCompressedFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'*.xml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparvCorpusSourceReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpostags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxslt_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./corpora/alto_xml_to_text.xslt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CompressedFileReader' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import nltk\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from corpora.zip_utility import ZipReader\n",
    "from corpora.corpus_source_reader import SparvCorpusSourceReader\n",
    "\n",
    "transforms = [\n",
    "    lambda tokens: [ x.lower() for x in tokens ],\n",
    "    lambda tokens: [ x for x in tokens if len(x) > 2 ],\n",
    "    lambda tokens: [ x for x in tokens if x not in nltk.corpus.stopwords.words('swedish') ]\n",
    "]\n",
    "    \n",
    "def get_demo_text_corpus(archive, postags=\"|NN|PM|\", lemmatize=False, transforms=None):\n",
    "    source = CompressedFileReader(archive, pattern='*.xml', filenames=None)\n",
    "    documents = SparvCorpusSourceReader(source=source, transforms=(transforms or []), postags=postags, lemmatize=lemmatize, chunk_size=None, xslt_filename='./corpora/alto_xml_to_text.xslt')\n",
    "    return documents\n",
    "\n",
    "documents = get_demo_text_corpus('./data/1945_10.zip', postags=\"''\", lemmatize=False, transforms=None)\n",
    "\n",
    "w = next((x for x in documents))\n",
    "\n",
    "\n",
    "if False:\n",
    "    from rake_nltk import Rake\n",
    "    r = Rake(language='swedish') # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "    r.extract_keywords_from_text(' '.join(w[1][:500]))\n",
    "    r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('./data/1945_10_text.zip', 'w', zipfile.ZIP_DEFLATED) as xip:\n",
    "    for (filename, document) in documents:\n",
    "        xip.writestr(filename, ' '.join(document))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "dataset = api.load(\"text8\")\n",
    "next((x for x in dataset))\n",
    "#dct = Dictionary(dataset)  # fit dictionary\n",
    "#corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
    "#model = TfidfModel(corpus)  # fit model\n",
    "#vector = model[corpus[0]]  # apply model to the first corpus document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
