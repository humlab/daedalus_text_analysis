{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Text Analysis (aka Text Mining)?\n",
    "\n",
    "> *The use of computational methods and techniques to extract knowledge from text.* [Wikipedia](https://en.wikipedia.org/wiki/Text_mining)\n",
    "\n",
    "The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\n",
    "\n",
    "- Often discovery of previously unknown structure \n",
    "- Often very large amounts of text\n",
    "- Often unstructured text\n",
    "- Automatic or semi-automatic information retrieval (IR) process\n",
    "- Often some form of birds eye view of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it relevant?\n",
    "- Rapid increase of available huge quantities of unstructured text\n",
    "- Digitized newspapers & cultural heritage\n",
    "- Online open data, social media, blogs\n",
    "- Computational methods needed due to size of text\n",
    "- Business cases/oppertunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some applications / domain\n",
    "- Document classification / clustering\n",
    "- Document search (e.g. Google search)\n",
    "- Text similarity (plagiarism, “viral text”)\n",
    "- Find text sentiments (twitter, forum posts)\n",
    "- Author attribution / stylometrics\n",
    "- Find actors and entities in text\n",
    "- Word / genre trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **What’s easy for humans can be extremely hard for computers**\n",
    "- **Human-in-the-loop or supervised learning can be very expensive**\n",
    "- Ambiguity and fuzziness of terms and phrases\n",
    "- Poor data quality, errors in data, wrong data, missing data, ambigeous data\n",
    "- Context, metadata, domain-specific data\n",
    "- Data size (to much, to little)\n",
    "- Computational methods requires a structured internal representation\n",
    "- Internal models are a simplified views of the data\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample high-level workflow\n",
    "\n",
    "<img src=\"./images/text_analysis_workflow.svg\" alt=\"\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESCRIBE: Sample basic text data metrics (feature extraction)**\n",
    "\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average word length\n",
    "- Number of stopwords\n",
    "- Number of special characters\n",
    "- Number of numerics\n",
    "- Number of uppercase words\n",
    "\n",
    "**PREPARE: Sample basic text data preperations**\n",
    "- Lower casing\n",
    "- Punctuation removal\n",
    "- Stopwords removal\n",
    "- Frequent words removal\n",
    "- Rare words removal\n",
    "- Spelling correction\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "**DESCRIBE / MODEL: Sample more advanced text modelling**\n",
    "\n",
    "- N-grams\n",
    "- Term Frequency\n",
    "- Inverse Document Frequency\n",
    "- Term co-occurance\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- Pointwise mutual information\n",
    "- Bag of Words / Representation\n",
    "- Sentiment Analysis\n",
    "- Vector Space Models (VSM)\n",
    "- Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis Sample Flow\n",
    "\n",
    "- A generic text analysis workflow\n",
    "- Sample tasks for each step in the flow.\n",
    "- Tasks depend on specific project\n",
    "- Also depend on state and quality of the text at hand\n",
    "- ...the specific research question\n",
    "- ...the kind of text etc.\n",
    "(This is not the researcher's workflow.)\n",
    "\n",
    "> <img src=\"./images/text-analysis_sample_tasks.svg\" style=\"width: 75%;padding: 0; margin: 0;\">\n",
    "> <center><i>**Fig**. Sample text analysis tasks</i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Concepts\n",
    "- **Computers _only_ understand numbers - everything else is abstractions of numbers**\n",
    "\n",
    "### Bag-of-Word\n",
    "\n",
    "> - A simple **vector representation** of documents that discards words' order and lexical units\n",
    "> - Each unique word is assigned a number (e.g. 1 to N where N is number of unique words)\n",
    "> - A document is a N-sized vector giving the the number of times each word occurs in the document\n",
    "> - Index n can be the count for word represented by number n\n",
    "> - Most entries are 0 e.g. a sparse vector - memory efficient\n",
    "> - **Very** efficient and relaible numerical methods can be used on this representation\n",
    "\n",
    "### Distributional hypothesis\n",
    "\n",
    "> \"You shall know a word by the company it keeps\" (Firth, J. R. 1957:11)\n",
    "\n",
    "### NLP\n",
    "> Natural language processing\n",
    "\n",
    "\n",
    "|Term|Description|\n",
    "|----|-----|\n",
    "NLP| Natural language processing\n",
    "IR|Information Retrieval (or extraction) - the use of computational methods to extract information from data|\n",
    "Corpus|A collection of (related) documents or text|\n",
    "Document|A text document (an article, web page, tweet, book) |\n",
    "Token, term, word\n",
    "Delimiter|A character och sequence of characters that seperates parts of the text (i.e. words, sentences, paragraphs, chapters).\n",
    "Phrase|\n",
    "Entity|A \"thing\",  a distinct item, (can be refered ty as proper noun)|\n",
    "Term distribution (in corpus)\n",
    "Term frequency|Frequency of word occurances in a document\n",
    "n-gram\n",
    "BOW, CBOW | A simplified (but yet powerful) representation of a document where the text has no order, structure or grammer. All the words are thrown into a \"bag\" and hence the contextual information is lost. A common representation is a word distribution vector.\n",
    "Dictionary|\n",
    "Stop words|Words (tokens) that are often discarded from text analysis in order to improve (quality) performance. Can e.g. be frequent words.\n",
    "Sentiment analysis|A method to assign metrics to the sentiment (conveyed emotional meaning) to text (or words).\n",
    "Parsing|The process of analysing the syntax of a text. Creates an internal representation (data structure) of the component parts/units such as words, delimiters, sentences and paragraphs.\n",
    "Dependency parsing|\n",
    "Topic, Topic modelling\n",
    "Keyword extraction\n",
    "Keyword in context (KWIC)\n",
    "Co-occurrance|when two words occur in the same context (corpus, paragraph, sentence, window). Dependen on kind of context, windows size etc.\n",
    "Collocation|When a is adjacent to another word i.e. the two words are next to each other (a subset of co-occuring words)\n",
    "Word embeddings\n",
    "\n",
    "Visualization:\n",
    "\n",
    "Wordcloud - Word frequency (weight)\n",
    "Networks - Collocation, Cooccurance\n",
    "\n",
    "Table of document term frequencies\n",
    "Table of corpus term frequencies\n",
    "\n",
    "Correlations\n",
    "\n",
    "Document view \n",
    "Document | Words | Unique words (Types) | Lemmas | Unique words (Types) / Words | Words / Sentences | Mean (Types/Tokens) per 1000 tokens chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
