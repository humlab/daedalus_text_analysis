{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Text Analysis (aka Text Mining)?\n",
    "> *The use of computational methods and techniques to extract knowledge from text.*\n",
    "\n",
    "- Often discovery of previously unknown structure \n",
    "- Often very large amounts of text\n",
    "- Often unstructured text\n",
    "- Automatic or semi-automatic IR process\n",
    "- Often some form of birds eye view of the text\n",
    "\n",
    "### Why is it relevant?\n",
    "- Rapid increase of available huge quantities of unstructured text\n",
    "- Digitized newspapers & cultural heritage\n",
    "- Online open data, social media, blogs\n",
    "- Computational methods needed due to size of text\n",
    "- Business cases (google etc)\n",
    "\n",
    "### Some applications / domain\n",
    "- Document classification / clustering\n",
    "- Document search (e.g. Google search)\n",
    "- Text similarity (plagiarism, “viral text”)\n",
    "- Find text sentiments (twitter, forum posts)\n",
    "- Find actors and entities in text\n",
    "- Word trends\n",
    "\n",
    "### Challenges\n",
    "**What’s easy for humans can be extremely hard for computers**\n",
    "- Ambiguity and fuzziness of terms and phrases\n",
    "- Poor data quality\n",
    "- Context, metadata, domain-specific data\n",
    "- Data size (to much, to little)\n",
    "- Missing data\n",
    "\n",
    "**Human-in-the-loop (aka supervised learning) can be very expensive**\n",
    "\n",
    "\n",
    "### Representation\n",
    "\n",
    "The text must be represented in a form that enables the use of computational methods\n",
    "e.g. bag-of-words (BOW)\n",
    "(insert illustration)\n",
    "\n",
    "A simple representation that assumes that word sequence is irrelevant\n",
    "\n",
    "All terms are assigned a unique number { t1, t2, …, tn }\n",
    "A document is then just a set of word counts { c1, c2, …, cn }\n",
    "The word count of word t2 is c2\n",
    "Is a simple vector representation of documents\n",
    "And vector computations are what a computer can do better than humans\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "**DESCRIBE: Sample basic text data metrics (feature extraction)**\n",
    "\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average word length\n",
    "- Number of stopwords\n",
    "- Number of special characters\n",
    "- Number of numerics\n",
    "- Number of uppercase words\n",
    "\n",
    "**PREPARE: Sample basic text data preperations**\n",
    "- Lower casing\n",
    "- Punctuation removal\n",
    "- Stopwords removal\n",
    "- Frequent words removal\n",
    "- Rare words removal\n",
    "- Spelling correction\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "**DESCRIBE / MODEL: Sample more advanced text modelling**\n",
    "\n",
    "- N-grams\n",
    "- Term Frequency\n",
    "- Inverse Document Frequency\n",
    "- Term co-occurance\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- Pointwise mutual information\n",
    "- Bag of Words\n",
    "- Sentiment Analysis\n",
    "- Vector space Models (VSM)\n",
    "- Word Embedding\n",
    "\n",
    "### Text Analysis Sample Flow\n",
    "\n",
    "The figure below gives a view of a generic text analysis workflow, together with a few sample tasks for each step in the flow. Note that the tasks for a specific project depends on for instance the state and quality of  the text at hand, the specific research question, the kind of text etc. (Note that this is not the researcher's workflow.)\n",
    "\n",
    "<img src=\"./images/text-analysis_sample_tasks.svg\" style=\"width: 75%;padding: 0; margin: 0;\">\n",
    "\n",
    "<center><i>**Fig**. Sample text analysis tasks</i></center>\n",
    "\n",
    "This notebook focuses mostly on parts of the \"Evaluate & Interpret\" step, but also visualisation that can be used in the \"Narrate & Dissiminate\" step.\n",
    "Assessing the quality of a topic model is a qualitative process that requires the \"human-in-the-loop\". The system can assist the researcher in a number of ways, with features such as:\n",
    "\n",
    "* Easy way of browsing through topic-word distributions\n",
    "* Easy way of browsing through document-topic distributions\n",
    "* Intuitive ways of finding conceptual interpretations of topics\n",
    "\n",
    "* Use of metrics to highlight suspect data\n",
    "\n",
    " * Display similarity of topics to known distributions (uniform distribution, mean corpus distribution etc)\n",
    " * Display similar or overlapping topics,  topic clusters (for some metric)\n",
    " * Display how ubiquitousness of topics\n",
    " * Display document clusters\n",
    " * Display topic-topic co-occurrence (same document)\n",
    " * Use reference documents that should have some expected topic?\n",
    "\n",
    "These notebook contains sample implementations of some of these features. See also:\n",
    "> - Reading tea leaves: how humans interpret topic models, Chang et al. (2009) <br></br>\n",
    "> - Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality, Lau et al. <br></br>\n",
    "> - http://dirichlet.net/pdf/wallach09evaluation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample high-level workflow\n",
    "\n",
    "Öppna för manuella steg.\n",
    "Feedback-loop - förfining. Kolla lokal med Södettörn. \n",
    "\n",
    "<img src=\"./images/Södertörn_workshop_workflow.svg\" alt=\"\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "0. Intro Humlab och mig\n",
    "\n",
    "1. Introduction\n",
    "   Vad är textanalys?\n",
    "   Vad kan man göra med textanalys?\n",
    "   \n",
    "   Varför är det populärt idag?\n",
    "   \"Big-data\", \"digitalisering\", social media analys, \"internet\"\n",
    "   Vad är alternativen? När finns inga alternativ? \n",
    "   \n",
    "   Exempel: (Ben, Johan)\n",
    "\n",
    "2. Key concepts\n",
    "\n",
    "3. Områden och metoder\n",
    "   Sökning, klassificering, sentiment, IR, ..., nätverksanalys, statistik, geolocationing, ...\n",
    "   \n",
    "4. Basal Corpus Statistics\n",
    "\n",
    "Nyckeltal Corpus\n",
    "\n",
    "Document, ordfrekvenser Frekvenser, frekvenser över tid, \n",
    "collaction, samförkomst\n",
    "   \n",
    "Document view Document | Words | Unique words (Types) | Lemmas | Unique words (Types) / Words | Words / Sentences | Mean (Types/Tokens) per 1000 tokens chunks\n",
    "\n",
    "Textkomplexitet?\n",
    "\n",
    "Text analytics involves a set of techniques and approaches towards bringing textual content to a point where it is represented as data and then mined for insights/trends/patterns.\n",
    "\n",
    "5. \"Data science\"\n",
    "\n",
    "\n",
    "\n",
    "### Distributional hypothesis\n",
    "\n",
    ">\"You shall know a word by the company it keeps\" (Firth, J. R. 1957:11)\n",
    "\n",
    "\n",
    "## Glossary\n",
    "\n",
    "\n",
    "|Term|Description|\n",
    "|----|-----|\n",
    "Corpus||\n",
    "Document\n",
    "Token, term, word\n",
    "Delimiter\n",
    "Sentence\n",
    "Paragraph\n",
    "Phrase\n",
    "Term distribution (in corpus)\n",
    "Term frequency\n",
    "n-gram\n",
    "BOW, CBOW | Bag-of-words (BOW)\n",
    "Dictionary\n",
    "Stop words\n",
    "Topic, Topic modelling\n",
    "Keyword extraction\n",
    "Keyword in context (KWIC)\n",
    "Co-occurrance|when two words occur in the same context (corpus, paragraph, sentence, window). Dependen on kind of context, windows size etc.\n",
    "Collocation|When a is adjacent to another word i.e. the two words are next to each other (a subset of co-occuring words)\n",
    "Word embeddings\n",
    "\n",
    "Visualization:\n",
    "\n",
    "Wordcloud - Word frequency (weight)\n",
    "Networks - Collocation, Cooccurance\n",
    "\n",
    "Table of document term frequencies\n",
    "Table of corpus term frequencies\n",
    "\n",
    "Correlations\n",
    "\n",
    "Document view \n",
    "Document | Words | Unique words (Types) | Lemmas | Unique words (Types) / Words | Words / Sentences | Mean (Types/Tokens) per 1000 tokens chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
