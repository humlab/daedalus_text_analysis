{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n",
    "1. Open Url https://open-science.humlab.umu.se\n",
    "2. Enter credentials. Usernames **phd_user_01, phd_user_02, ..., phd_user_11** and password **phd_course_2018**\n",
    "3. Open folder **phd_course** (just click on it).\n",
    "4. Open notebook **intro_to_topic_modelling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Topic Modelling?\n",
    "\n",
    "- Topic modelling can be seen as a method for finding a \"groups of words\" (i.e themes/topics) from a collection of documents that in some way capture the information in the collection.\n",
    "- It can also be thought of as a form of text mining – a way to obtain recurring patterns of words in textual material.\n",
    "\n",
    "## What is an LDA Topic Model?\n",
    "- LDA is a so called \"generative probabalistic\" model.\n",
    "- LDA, as all TM, assumes that a document consists of **underlying \"topics\"**\n",
    "- This means that certain **\"groups of words\" (i.e. topics) more frequently** occur in a specific document\n",
    "- Each topic is hence simply **a word frequency distribution**\n",
    "- The premise of LDA is the assumption is that the **documents have been generated via a statistical model/process**.\n",
    "- A simplified view of how a document is generated is\n",
    "  - Selects the document's (mix of) topics => a topic distribution\n",
    "  - Generate the document's words by repeating:\n",
    "    - Draw a topic from the topic distribution\n",
    "    - Draw a word from that topic\n",
    "\n",
    "Given this imaginary generative process, the corpus at hand is the correct answer!<br>\n",
    "Commonly used computational processes can be used to fit the corpus to the statistical model, which gives the topic distributions.<br>\n",
    "See *Blei, 2003: Latent dirichlet allocation* [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) for a description of LDA.\n",
    "\n",
    "<span style=\"float:left\">\n",
    "    <img src=\"./images/blei_lda.jpg\" style=\"width: 600px;padding: 0; margin: 0;\">\n",
    "    <br>\n",
    "    <img src=\"./images/blei_2012b.png\" style=\"width: 600px;padding: 0; margin: 0;\">\n",
    "</span>\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Jupyter Notebook?\n",
    "<span style=\"float:left\"><img src=\"./images/narrative_new.svg\" style=\"width: 300px;padding: 0; margin: 0;\"></span><br>\n",
    "> - [Jupyter](http://jupyter.org/) is an open-source software for **interactive and reproducible computing**.<br>\n",
    "> - The **open science movement** is a driving force for Jupyter's popularity.<br>\n",
    "> - Which in part is a response to the **reproducibility crisis in science** and the **statistical crisis in science**<br>\n",
    "> - Jupyter Notebooks contain **excutable code, equations, visualizations and narrative text**.<br>\n",
    "> - It is a **web application** with a simple and easy to use web interface.\n",
    "> - Supports a large number of programming languages (50+ e.g. Python, R, JavaScript)\n",
    "> - Sponsered by large companies such as Google and Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Instructions on How to Use Notebooks\n",
    "- **Menu Help -> User Interface Tour** gives an overview of the user interface.\n",
    "- **Code cells** contains the script code and have **In [x]** in the left margin.\n",
    "  - **In []** indicates that the code cell hasn't been executed yet.\n",
    "  - **In [n]** indicates that the code has been executed(n is an integer).\n",
    "  - **In [\\*]** indicates that the code is executing, or waiting to be executed (i.e. other cells are executing).\n",
    "- **The current code** is highlighted with a blue border - you make it current by clicking on it.\n",
    "- **SHIFT+ENTER** or **Play button** executes the current cell. Code cells aren't executed automatically.\n",
    "- **Out[n]** indicates the output (or result) of a cell's execution and is directly below the executed cell.\n",
    "- **SHIFT+ENTER** automatically selects the next code cell.\n",
    "- **SHIFT+ENTER** can hence be used repeatedly to executes the code cells in sequence.\n",
    "- **Menu Cell -> Run All** executes the entire notebook in a single step (can take some time to finish, notice how \"In [\\*]\" indicators change to \"In [n]\" ).\n",
    "- **Double-Click** on a cell to edit its content.\n",
    "- **ESC key** Leaves edit mode (or just click on any other cell).\n",
    "- **Kernel -> Restart** restarts server side kernel (use if notebook seems stuck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks\n",
    "- The risk of using tools and methods **without fully understanding** them\n",
    "- The risk of using tools and methods **for non-intended purposes or in new contexts**\n",
    "- How to verify **performance** (correctness of result)\n",
    "- Risk of **data dredging**, p-hacking, \"the statistical crisis\".\n",
    "- The risk that **engineer makes micro-decisions** the researcher don't know about, or don't fully understand.\n",
    "- The risk of **reading to much into visualizations** (networks, layouts, clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **What’s easy for humans can be extremely hard for computers**\n",
    "- **Human-in-the-loop or supervised learning can be very expensive**\n",
    "- Ambiguity and fuzziness of terms and phrases\n",
    "- Poor data quality, errors in data, wrong data, missing data, ambigious data\n",
    "- Understand domain contexts, metadata, domain-specific data\n",
    "- Data size (to much, to little)\n",
    "- How to understand internal representations of data used by computational methods\n",
    "- Internal representations are simplified views of the actual data (e.g. \"bag-of-word\" model)\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample high-level workflow\n",
    "\n",
    "<img src=\"./images/text_analysis_workflow.svg\" alt=\"\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>COLLECT</span> Extract Text From PDFs <span style='color: blue; float: right'>SKIP</span>  \n",
    "The source data consists of 25 english academic articles downloded as PDF from Zotera. The first step is to extract the text from the PDF files. This can also be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed, PDFPage\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "\n",
    "def extract_pdf_text(filename):\n",
    "    text_lines = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        \n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        resource_manager = PDFResourceManager()\n",
    "\n",
    "        result_buffer = StringIO()\n",
    "\n",
    "        device = TextConverter(resource_manager, result_buffer, codec='utf-8', laparams=LAParams())\n",
    "\n",
    "        interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        lines = result_buffer.getvalue().splitlines()\n",
    "        for line in lines:\n",
    "            text_lines.append(line)\n",
    "\n",
    "    return text_lines\n",
    "\n",
    "\n",
    "def extract_pdf_texts(source_folder, target_zip_filename):\n",
    "    with zipfile.ZipFile(target_zip_filename, 'w', zipfile.ZIP_DEFLATED) as target_zip:\n",
    "\n",
    "        for filename in glob.glob(os.path.join(source_folder,'*.pdf')):\n",
    "\n",
    "            print('Processing: ' + filename)\n",
    "\n",
    "            text_lines = extract_pdf_text(filename)\n",
    "\n",
    "            target_filename = os.path.splitext(os.path.split(filename)[1])[0] + '.txt'\n",
    "            target_filename = target_filename.lower().replace(' ', '_').replace(',','')\n",
    "\n",
    "            target_zip.writestr(target_filename, '\\n'.join(text_lines))\n",
    "            \n",
    "#source_folder = './data/pdf'\n",
    "#target_zip_filename = 'data/paper_extracted_texts.zip'\n",
    "#extract_pdf_texts(source_folder, target_zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>INITIALIZE </span> Setup and initialize used packages <span style='color: red; float: right'>MANDATORY RUN!</span>  \n",
    "The following CODE CELL must be run once to set up the run time environment. Please select the cell and hit **SHIFT-ENTER** or the **RUN** button in the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : POS tag set: PUNCT SYM X ADJ VERB CONJ NUM DET ADV ADP  NOUN PROPN PART PRON SPACE INTJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# folded code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, warnings, types\n",
    "import numpy as np, pandas as pd\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re, string, zipfile\n",
    "import nltk, spacy, textacy, textacy.extract, textacy.preprocess\n",
    "\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "from gensim import corpora, models, matutils\n",
    "from IPython.display import display, HTML #, clear_output, IFrame\n",
    "from pivottablejs import pivot_ui\n",
    "from spacy import displacy\n",
    "\n",
    "logger = utility.getLogger(format=\"%(levelname)s;%(message)s\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "pd.set_option('precision', 10)\n",
    "\n",
    "def get_filenames(zip_filename, extension='.txt'):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return [ x for x in zf.namelist() if x.endswith(extension) ]\n",
    "    \n",
    "def get_text(zip_filename, filename):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return zf.read(filename).decode(encoding='utf-8')\n",
    "\n",
    "DEFAULT_TERM_PARAMS = dict(\n",
    "    args=dict(ngrams=1, named_entities=True, normalize='lemma', as_strings=True),\n",
    "    kwargs=dict(filter_stops=True, filter_punct=True, filter_nums=True, min_freq=1, drop_determiners=True, include_pos=('NOUN', 'PROPN', ))\n",
    ")\n",
    "    \n",
    "def filter_terms(doc, term_args, chunk_size=None, min_length=2):\n",
    "    kwargs = utility.extend({}, DEFAULT_TERM_PARAMS['kwargs'], term_args['kwargs'])\n",
    "    args = utility.extend({}, DEFAULT_TERM_PARAMS['args'], term_args['args'])\n",
    "    terms = (x for x in doc.to_terms_list(\n",
    "        args['ngrams'],\n",
    "        args['named_entities'],\n",
    "        args['normalize'],\n",
    "        args['as_strings'],\n",
    "        **kwargs\n",
    "    ) if len(x) >= min_length)\n",
    "    return terms\n",
    "        \n",
    "def slim_title(x):\n",
    "    try:\n",
    "        m = re.match('.*\\((.*)\\)$', x).groups()\n",
    "        if m is not None and len(m) > 0:\n",
    "            return m[0]\n",
    "        return ' '.join(x.split(' ')[:3]) + '...'\n",
    "    except:\n",
    "        return x\n",
    "            \n",
    "LANGUAGE = 'en'\n",
    "SOURCE_FOLDER = './data'\n",
    "\n",
    "EXTRACTED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'paper_extracted_texts.zip')\n",
    "EDITED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'paper_edited_texts.zip')\n",
    "PREPROCESSED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'paper_preprocessed_text.zip')\n",
    "\n",
    "SOURCE_FILES = {\n",
    "    'source_text_raw': { 'filename': EXTRACTED_TEXT_FILENAME, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "    'source_text_edited': { 'filename': EDITED_TEXT_FILENAME, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "    'source_text_preprocessed': { 'filename': PREPROCESSED_TEXT_FILENAME, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "}\n",
    "\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "DF_TAGSET = pd.read_csv('./data/tagset.csv', sep='\\t').fillna('')\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "AGGREGATES = { 'mean': np.mean, 'sum': np.sum, 'max': np.max, 'std': np.std }\n",
    "\n",
    "logger.info('POS tag set: ' + ' '.join(list(DF_TAGSET.POS.unique())))\n",
    "\n",
    "%matplotlib inline\n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "class TopicModelNotComputed(Exception):\n",
    "    @staticmethod\n",
    "    def check():\n",
    "        if 'TM_GUI_MODEL' in globals():\n",
    "            gui =  globals()['TM_GUI_MODEL']\n",
    "            if None not in (gui, gui.model):\n",
    "                return True\n",
    "        msg = 'A topic model must be computed using step \"MODEL Compute an LDA Topic Model\"'\n",
    "        raise TopicModelNotComputed(msg)\n",
    "\n",
    "def get_current_model():\n",
    "    TopicModelNotComputed.check()\n",
    "    return globals()['TM_GUI_MODEL'].model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE: </span> Load and Prepare the Text Corpus <span style='color: red; float: right'>MANDATORY RUN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loading model: english...\n",
      "INFO : Using pipeline: tagger parser ner\n",
      "INFO : Working: Loading corpus ./data/corpus_en__disable().pkl...\n",
      "INFO : Working: Merging named entities...\n",
      "INFO : Done!\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "from textacy.spacier.utils import merge_spans\n",
    "\n",
    "def preprocess_text(source_filename, target_filename=None):\n",
    "    filenames = get_filenames(source_filename)\n",
    "    basename, extension = os.path.splitext(source_filename)\n",
    "    target_filename = target_filename or basename + '_preprocessed' + extension\n",
    "    texts = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    with zipfile.ZipFile(target_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for filename, text in texts:\n",
    "            logger.info('Processing ' + filename)\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.normalize_whitespace(text)   \n",
    "            text = textacy.preprocess.fix_bad_unicode(text)   \n",
    "            text = textacy.preprocess.replace_currency_symbols(text)\n",
    "            text = textacy.preprocess.unpack_contractions(text)\n",
    "            text = textacy.preprocess.replace_urls(text)\n",
    "            text = textacy.preprocess.replace_emails(text)\n",
    "            text = textacy.preprocess.replace_phone_numbers(text)\n",
    "            text = textacy.preprocess.remove_accents(text)\n",
    "            zf.writestr(filename, text)\n",
    "            \n",
    "def create_textacy_corpus(source_filename, language, preprocess_args):\n",
    "    make_title = lambda filename: filename.replace('_', ' ').replace('.txt', '').title()\n",
    "    filenames = get_filenames(source_filename)\n",
    "    corpus = textacy.Corpus(language)\n",
    "    text_stream = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    for filename, text in text_stream:\n",
    "        logger.info('Processing ' + filename)\n",
    "        text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "        text = textacy.preprocess.preprocess_text(text, **preprocess_args)\n",
    "        corpus.add_text(text, dict(filename=filename, title=make_title(filename)))\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    return corpus\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [ e for e in doc.ents if not e.text.isspace() ]\n",
    "    return doc\n",
    "\n",
    "def generate_textacy_corpus(source_filename, language, corpus_args, preprocess_args, merge_named_entities=True, force=False):\n",
    "    \n",
    "    corpus_tag = '_'.join([ k for k in preprocess_args if preprocess_args[k] ]) + \\\n",
    "        '_disable(' + ','.join(corpus_args.get('disable', [])) +')'\n",
    "    \n",
    "    textacy_corpus_filename = os.path.join(SOURCE_FOLDER, 'corpus_{}_{}.pkl'.format(language, corpus_tag))\n",
    "    \n",
    "    Language.factories['remove_whitespace_entities'] = lambda nlp, **cfg: remove_whitespace_entities\n",
    "    \n",
    "    logger.info('Loading model: english...')\n",
    "    nlp = textacy.load_spacy('en_core_web_sm', **corpus_args)\n",
    "    pipeline = lambda: [ x[0] for x in nlp.pipeline ]\n",
    "        \n",
    "    logger.info('Using pipeline: ' + ' '.join(pipeline()))\n",
    "\n",
    "    if force or not os.path.isfile(textacy_corpus_filename):\n",
    "        logger.info('Working: Computing new corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = create_textacy_corpus(source_filename, nlp, preprocess_args)\n",
    "        corpus.save(textacy_corpus_filename)\n",
    "    else:\n",
    "        logger.info('Working: Loading corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = textacy.Corpus.load(textacy_corpus_filename)\n",
    "        \n",
    "    if merge_named_entities:\n",
    "        logger.info('Working: Merging named entities...')\n",
    "        for doc in corpus:\n",
    "            named_entities = textacy.extract.named_entities(doc)\n",
    "            merge_spans(named_entities, doc.spacy_doc)\n",
    "    else:\n",
    "        logger.info('Note: named entities not merged')\n",
    "        \n",
    "    logger.info('Done!')\n",
    "    return textacy_corpus_filename, corpus\n",
    "\n",
    "def assign_document_titles(corpus):\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    \n",
    "def get_corpus_documents(corpus):\n",
    "    df = pd.DataFrame([\n",
    "        (document_id, doc.metadata['title'], doc.metadata['filename'])\n",
    "                for document_id, doc in enumerate(corpus) ], columns=['document_id', 'title', 'filename']\n",
    "    ).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "if not os.path.isfile(PREPROCESSED_TEXT_FILENAME):\n",
    "    logger.info(\"Preprocessing text archive...\")\n",
    "    preprocess_text(EDITED_TEXT_FILENAME, PREPROCESSED_TEXT_FILENAME)\n",
    "    \n",
    "TEXTACY_CORPUS_FILENAME, CORPUS = generate_textacy_corpus(PREPROCESSED_TEXT_FILENAME, LANGUAGE, corpus_args=dict(), preprocess_args=dict(), merge_named_entities=True, force=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b71aa78f35845a98571eb44391c5a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_cleanup_text_gui(corpus, callback):\n",
    "    \n",
    "    documents = get_corpus_documents(corpus)\n",
    "    document_options = {v: k for k, v in documents['title'].to_dict().items()}\n",
    "    \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.FloatSlider(value=0, min=0, max=1.0, step=0.01, description='Min frequency', layout=widgets.Layout(width='400px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=[1,2,3], value=1, layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=False, description='Filter nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=False, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "\n",
    "        callback,\n",
    "\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        document_id=gui.document_id,\n",
    "        \n",
    "        ngrams=gui.ngrams,\n",
    "        named_entities=gui.named_entities,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        filter_nums=gui.filter_nums,\n",
    "        include_pos=gui.include_pos,\n",
    "        min_freq=gui.min_freq,\n",
    "        drop_determiners=gui.drop_determiners\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.document_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(10, 10 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_cleaned_up_text(corpus, gui, display_type, document_id, **kwargs): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    #Additional candidates;\n",
    "    #is_alpha\tbool\tDoes the token consist of alphabetic characters? Equivalent to token.text.isalpha().\n",
    "    #is_ascii\tbool\tDoes the token consist of ASCII characters? Equivalent to [any(ord(c) >= 128 for c in token.text)].\n",
    "    #like_url\tbool\tDoes the token resemble a URL?\n",
    "    #like_email\tbool\tDoes the token resemble an email address?\n",
    "\n",
    "    doc = corpus[document_id]\n",
    "    \n",
    "    terms = [ x for x in doc.to_terms_list(as_strings=True, **kwargs) ]\n",
    "    \n",
    "    if display_type.startswith('source_text'):\n",
    "        \n",
    "        source_filename = SOURCE_FILES[display_type]['filename']\n",
    "        description =  SOURCE_FILES[display_type]['description']\n",
    "        text = get_text(source_filename, doc.metadata['filename'])\n",
    "        with gui.output_text:\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print('[ ' + description.upper() + ' ]')\n",
    "            print(text)\n",
    "        return\n",
    "\n",
    "    if len(terms) == 0:\n",
    "        with gui.output_text:\n",
    "            print(\"No text. Please change selection.\")\n",
    "        return\n",
    "    \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join(list(terms)))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                print('Word count (number of terms): {}'.format(wf.N()))\n",
    "                print('Unique word count (vocabulary): {}'.format(wf.B()))\n",
    "                print(' ')\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                display(df)\n",
    "\n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "xgui, xuix = display_cleanup_text_gui(CORPUS, display_cleaned_up_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Compute an LDA Topic Model<span style='color: red; float: right'>MANDATORY RUN</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8368bfcfbff3457db9de6f2d12da8468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(IntSlide…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "class LdaDataCompiler():\n",
    "    \n",
    "    @staticmethod\n",
    "    def compile_dictionary(model):\n",
    "        logger.info('Compiling dictionary...')\n",
    "        token_ids, tokens = list(zip(*model.id2word.items()))\n",
    "        dfs = model.id2word.dfs.values() if model.id2word.dfs is not None else [0] * len(tokens)\n",
    "        dictionary = pd.DataFrame({\n",
    "            'token_id': token_ids,\n",
    "            'token': tokens,\n",
    "            'dfs': list(dfs)\n",
    "        }).set_index('token_id')[['token', 'dfs']]\n",
    "        return dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_weights(tm, dictionary, num_words=200):\n",
    "        logger.info('Compiling topic-tokens weights...')\n",
    "\n",
    "        df_topic_weights = pd.DataFrame(\n",
    "            [ (topic_id, token, weight)\n",
    "                for topic_id, tokens in (tm.show_topics(tm.num_topics, num_words=num_words, formatted=False))\n",
    "                    for token, weight in tokens if weight > 0.0 ],\n",
    "            columns=['topic_id', 'token', 'weight']\n",
    "        )\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_topic_weights.set_index('token'),\n",
    "            dictionary.reset_index().set_index('token'),\n",
    "            how='inner',\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        return df.reset_index()[['topic_id', 'token_id', 'token', 'weight']]\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_overview(topic_token_weights, alpha, n_words=200):\n",
    "        \"\"\"\n",
    "        Group by topic_id and concatenate n_words words within group sorted by weight descending.\n",
    "        There must be a better way of doing this...\n",
    "        \"\"\"\n",
    "        logger.info('Compiling topic-tokens overview...')\n",
    "\n",
    "        df = topic_token_weights.groupby('topic_id')\\\n",
    "            .apply(lambda x: sorted(list(zip(x[\"token\"], x[\"weight\"])), key=lambda z: z[1], reverse=True))\\\n",
    "            .apply(lambda x: ' '.join([z[0] for z in x][:n_words])).reset_index()\n",
    "        df['alpha'] = df.topic_id.apply(lambda topic_id: alpha[topic_id])\n",
    "        df.columns = ['topic_id', 'tokens', 'alpha']\n",
    "\n",
    "        return df.set_index('topic_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_document_topics(model, corpus, documents, minimum_probability=0.001):\n",
    "\n",
    "        def document_topics_iter(model, corpus, minimum_probability):\n",
    "\n",
    "            data_iter = model.get_document_topics(corpus, minimum_probability=minimum_probability)\\\n",
    "                if hasattr(model, 'get_document_topics')\\\n",
    "                else model.load_document_topics()\n",
    "\n",
    "            for i, topics in enumerate(data_iter):\n",
    "                for x in topics:\n",
    "                    yield (i, x[0], x[1])\n",
    "        '''\n",
    "        Get document topic weights for all documents in corpus\n",
    "        Note!  minimum_probability=None filters less probable topics, set to 0 to retrieve all topcs\n",
    "\n",
    "        If gensim model then use 'get_document_topics', else 'load_document_topics' for mallet model\n",
    "        '''\n",
    "        logger.info('Compiling document topics...')\n",
    "        logger.info('  Creating data iterator...')\n",
    "        data = document_topics_iter(model, corpus, minimum_probability)\n",
    "        logger.info('  Creating frame from iterator...')\n",
    "        df_doc_topics = pd.DataFrame(data, columns=[ 'document_id', 'topic_id', 'weight' ]).set_index('document_id')\n",
    "        logger.info('  Merging data...')\n",
    "        df = pd.merge(documents, df_doc_topics, how='inner', left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_compiled_data(model, corpus, id2term, documents):\n",
    "\n",
    "        dictionary = LdaDataCompiler.compile_dictionary(model)\n",
    "        topic_token_weights = LdaDataCompiler.compile_topic_token_weights(model, dictionary, num_words=200)\n",
    "        topic_token_overview = LdaDataCompiler.compile_topic_token_overview(topic_token_weights, model.alpha)\n",
    "        document_topic_weights = LdaDataCompiler.compile_document_topics(model, corpus, documents, minimum_probability=0.001)\n",
    "\n",
    "        return types.SimpleNamespace(\n",
    "            dictionary=dictionary,\n",
    "            documents=documents,\n",
    "            topic_token_weights=topic_token_weights,\n",
    "            topic_token_overview=topic_token_overview,\n",
    "            document_topic_weights=document_topic_weights\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_topic_titles(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id==topic_id)]\n",
    "        df = df_temp\\\n",
    "                .sort_values('weight', ascending=False)\\\n",
    "                .groupby('topic_id')\\\n",
    "                .apply(lambda x: ' '.join(x.token[:n_words].str.title()))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_title(topic_token_weights, topic_id, n_words=100):\n",
    "        return LdaDataCompiler.get_topic_titles(topic_token_weights, topic_id, n_words=n_words).iloc[0]\n",
    "\n",
    "    #get_topics_tokens_as_text = get_topic_titles\n",
    "    #get_topic_tokens_as_text = get_topic_title\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_tokens(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id == topic_id)]\n",
    "        df = df_temp.sort_values('weight', ascending=False)[:n_words]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lda_topics(model, n_tokens=20):\n",
    "        return pd.DataFrame({\n",
    "            'Topic#{:02d}'.format(topic_id+1) : [ word[0] for word in model.show_topic(topic_id, topn=n_tokens) ]\n",
    "                for topic_id in range(model.num_topics)\n",
    "        })\n",
    "\n",
    "# OBS OBS! https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "DEFAULT_VECTORIZE_PARAMS = dict(tf_type='linear', apply_idf=False, idf_type='smooth', norm='l2', min_df=1, max_df=0.95)\n",
    "\n",
    "def compute_topic_model(corpus, tick=utility.noop, method='sklearn_lda', vec_args=None, term_args=None, tm_args=None, **args):\n",
    "    \n",
    "    tick()\n",
    "    vec_args = utility.extend({}, DEFAULT_VECTORIZE_PARAMS, vec_args)\n",
    "    \n",
    "    terms_iter = lambda: (filter_terms(doc, term_args) for doc in corpus)\n",
    "    tick()\n",
    "    \n",
    "    vectorizer = textacy.Vectorizer(**vec_args)\n",
    "    doc_term_matrix = vectorizer.fit_transform(terms_iter())\n",
    "\n",
    "    if method.startswith('sklearn'):\n",
    "        tm_model = textacy.TopicModel(method.split('_')[1], **tm_args)\n",
    "        tm_model.fit(doc_term_matrix)\n",
    "        tick()\n",
    "        doc_topic_matrix = tm_model.transform(doc_term_matrix)\n",
    "        tick()\n",
    "        tm_id2word = vectorizer.id_to_term\n",
    "        tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        compiled_data = None # FIXME\n",
    "    else:\n",
    "        doc_topic_matrix = None # ?\n",
    "        tm_id2word = corpora.Dictionary(terms_iter())\n",
    "        tm_corpus = [ tm_id2word.doc2bow(text) for text in terms_iter() ]\n",
    "        #tm_id2word = vectorizer.id_to_term\n",
    "        #tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        tm_model = models.LdaModel(\n",
    "            tm_corpus, \n",
    "            num_topics  =  tm_args.get('n_topics', 0),\n",
    "            id2word     =  tm_id2word,\n",
    "            iterations  =  tm_args.get('max_iter', 0),\n",
    "            passes      =  20,\n",
    "            alpha       = 'asymmetric'\n",
    "        )\n",
    "        documents = get_corpus_documents(corpus)\n",
    "        compiled_data = LdaDataCompiler.compute_compiled_data(tm_model, tm_corpus, tm_id2word, documents)\n",
    "    \n",
    "    tm_data = types.SimpleNamespace(\n",
    "        tm_model=tm_model,\n",
    "        tm_id2term=tm_id2word,\n",
    "        tm_corpus=tm_corpus,\n",
    "        doc_term_matrix=doc_term_matrix,\n",
    "        doc_topic_matrix=doc_topic_matrix,\n",
    "        vectorizer=vectorizer,\n",
    "        compiled_data=compiled_data\n",
    "    )\n",
    "    \n",
    "    tick(0)\n",
    "    \n",
    "    return tm_data\n",
    "\n",
    "def get_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    topic_ids = range(0,doc_topic_matrix.shape[1])\n",
    "    for document_id in range(0,doc_topic_matrix.shape[1]):\n",
    "        topic_weights = doc_topic_matrix[document_id, :]\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_weights[topic_id] >= threshold:\n",
    "                yield (document_id, topic_id, topic_weights[topic_id])\n",
    "\n",
    "def get_df_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    it = get_doc_topic_weights(doc_topic_matrix, threshold)\n",
    "    df = pd.DataFrame(list(it), columns=['document_id', 'topic_id', 'weight']).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "def display_topic_model_gui(corpus, compute_callback):\n",
    "    \n",
    "    pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]\n",
    "    # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    engine_options = { 'gensim': 'gensim' } #, 'sklearn_lda': 'sklearn_lda'}\n",
    "    normalize_options = { 'None': False, 'Use lemma': 'lemma', 'Lowercase': 'lower'}\n",
    "    ngrams_options = { '1': [1], '1, 2': [1, 2], '1,2,3': [1, 2, 3] }\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        n_topics=widgets.IntSlider(description='#topics', min=5, max=50, value=20, step=1),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1),\n",
    "        max_iter=widgets.IntSlider(description='Max iterations', min=100, max=1000, value=20, step=10),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=widgets.Layout(width='200px')),\n",
    "        filter_stops=widgets.ToggleButton(value=True, description='Remove stopword',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=True, description='Remove nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        apply_idf=widgets.ToggleButton(value=False, description='Apply IDF',  tooltip='Apply TF-IDF', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=['NOUN', 'PROPN'], rows=7, layout=widgets.Layout(width='200px')),\n",
    "        method=widgets.Dropdown(description='Engine', options=engine_options, value='gensim', layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute'),\n",
    "        boxes=None,\n",
    "        output = widgets.Output(layout={'height': '500px'}),\n",
    "        model=None\n",
    "    )\n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.n_topics,\n",
    "                gui.min_freq,\n",
    "                gui.max_iter\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners,\n",
    "                gui.apply_idf\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.method\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.VBox([gui.output]), # ,layout=widgets.Layout(top='20px', height='500px',width='100%'))\n",
    "    ])\n",
    "    fx = lambda *args: compute_callback(corpus, gui, *args)\n",
    "    gui.compute.on_click(fx)\n",
    "    return gui\n",
    "    \n",
    "\n",
    "def compute_callback(corpus, gui, *args):\n",
    "    \n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "    \n",
    "    tick(1)\n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        vec_args = dict(apply_idf=gui.apply_idf.value)\n",
    "        term_args = dict(\n",
    "            args=dict(\n",
    "                ngrams=gui.ngrams.value,\n",
    "                named_entities=gui.named_entities.value,\n",
    "                normalize=gui.normalize.value,\n",
    "                as_strings=True\n",
    "            ),\n",
    "            kwargs=dict(\n",
    "                filter_nums=gui.filter_nums.value,\n",
    "                drop_determiners=gui.drop_determiners.value,\n",
    "                min_freq=gui.min_freq.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                filter_stops=gui.filter_stops.value,\n",
    "                filter_punct=True\n",
    "            )\n",
    "        )\n",
    "        tm_args = dict(\n",
    "            n_topics=gui.n_topics.value,\n",
    "            max_iter=gui.max_iter.value,\n",
    "            learning_method='online', \n",
    "            n_jobs=1\n",
    "        )\n",
    "        method = gui.method.value\n",
    "        gui.model = compute_topic_model(\n",
    "            corpus=corpus,\n",
    "            tick=tick,\n",
    "            method=method,\n",
    "            vec_args=vec_args,\n",
    "            term_args=term_args,\n",
    "            tm_args=tm_args\n",
    "        )\n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        #display(gui.model.compiled_data.topic_token_overview)\n",
    "        display(LdaDataCompiler.get_lda_topics(gui.model.tm_model, n_tokens=20))\n",
    "        \n",
    "TM_GUI_MODEL = display_topic_model_gui(CORPUS, compute_callback)\n",
    "display(TM_GUI_MODEL.boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c1e140aae24c18b9cb778728df6ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Paper', index=23, layout=Layout(width='80%'), options={'The Redistributio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_document_entities_gui(corpus):\n",
    "    \n",
    "    def display_document_entities(document_id, corpus):\n",
    "        displacy.render(corpus[document_id].spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "\n",
    "    document_widget = widgets.Dropdown(description='Paper', options={v: k for k, v in df_documents['title'].to_dict().items()}, value=0, layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(display_document_entities,document_id=document_widget, corpus=widgets.fixed(corpus))\n",
    "\n",
    "    display(widgets.VBox([document_widget, widgets.VBox([itw.children[-1]],layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    display_document_entities_gui(CORPUS)\n",
    "except Except as ex:\n",
    "    logger.error(ec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Wordcloud<span style='color: red; float: right'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df53e5e072c1435fa9568000bbe0e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='tx02'></span>\", placeholder=''), HBox(children=(Button(description='<<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import common.widgets_utility as widgets_utility\n",
    "\n",
    "def display_wordcloud_gui(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    model = tm_data.tm_model\n",
    "    output_options = output_options or []\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(\n",
    "    tm_data,\n",
    "    topic_id=0,\n",
    "    n_words=100,\n",
    "    output_format='Wordcloud',\n",
    "    widget_container=None\n",
    "):\n",
    "    container = tm_data.compiled_data\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = container.topic_token_weights.loc[(container.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=n_words)\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id, n_words=n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(LdaDataCompiler.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    display_wordcloud_gui(display_wordcloud, tm_data, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n",
    "except TopicModelNotComputed as ex:\n",
    "    logger.info(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Chart<span style='color: red; float: right'>TRY IT</span>\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a0f2c0b9e84f9c922df13cfde36e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='wc01'></span>\", placeholder=''), HBox(children=(Button(description='<<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display topic's word distribution\n",
    "if False:\n",
    "    from common.model_utility import ModelUtility\n",
    "    from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "    from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "    import math\n",
    "\n",
    "    from itertools import product\n",
    "    \n",
    "    import bokeh.models as bm\n",
    "    import bokeh.palettes\n",
    "    from bokeh.io import output_file, push_notebook\n",
    "    from bokeh.core.properties import value, expr\n",
    "    from bokeh.transform import transform, jitter\n",
    "    from bokeh.layouts import row, column, widgetbox\n",
    "    from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "    from bokeh.models import ColumnDataSource, CustomJS\n",
    "    \n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens)\n",
    "\n",
    "    p = bokeh.plotting.figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = bokeh.models.ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bokeh.models.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def display_topic_tokens(tm_data, topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        p = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        bokeh.plotting.show(p)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "    widget_container.reset()\n",
    "    \n",
    "def display_topic_distribution_widgets(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "TM_DATA = TM_GUI_MODEL.model\n",
    "\n",
    "display_topic_distribution_widgets(display_topic_tokens, TM_DATA, 'wc01', ['Chart', 'Table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Trend Over Time or Documents<span style='color: red; float: right'>RUN</span>\n",
    "- Displays topic's share over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a4adc2f3644435800bfd1f01ced83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='topic_share_plot'></span>\", placeholder=''), HBox(children=(Button(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "#import numpy as np\n",
    "#import math\n",
    "#import bokeh.plotting\n",
    "#from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "#from bokeh.models.glyphs import VBar\n",
    "#from bokeh.io import curdoc, show\n",
    "\n",
    "import math\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=700, title='', tools=tools, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, widgets_container, output_format='Chart', tm_data=None, threshold=0.01):\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=200)\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = 'weight'\n",
    "    category_column = 'author'\n",
    "    df = container.document_topic_weights[(container.document_topic_weights.topic_id==topic_id)]\n",
    "    df = df[(df.weight > threshold)].reset_index()\n",
    "    df[category_column] = df.title.apply(slim_title)\n",
    "\n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = category_column.title()\n",
    "        y_label = value_column.title()\n",
    "        p = plot_topic_trend(df, category_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "def create_topic_trend_widgets(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "create_topic_trend_widgets(tm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic to Document Network<span style='color: red; float: right'>TRY IT</span>\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d71f5de2aaf4b7083c695daa5b2dbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='nx_id1'></span>\", placeholder=''), HBox(children=(Dropdown(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "def plot_document_topic_network(network, layout, scale=1.0, titles=None):\n",
    "\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = bokeh.plotting.figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=TOOLS)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(renderers=[r_topics], tooltips=None, callback=widgets_utility.wf.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(x='x', y='y', text='name', level='overlay', x_offset=0, y_offset=0, text_font_size='8pt')\n",
    "    \n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_network(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'nx_id1'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    text_widget = widgets_utility.wf.create_text_widget(text_id)  # style=\"display: inline; height='400px'\"),\n",
    "    scale_widget = widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False)\n",
    "    threshold_widget = widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False)\n",
    "    output_format_widget = widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network')\n",
    "    layout_widget = widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold')\n",
    "    progress_widget = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"40%\"))\n",
    "    \n",
    "    def tick(x=None):\n",
    "        progress_widget.value = progress_widget.value + 1 if x is None else x\n",
    "        \n",
    "    def display_topic_network(layout_algorithm, tm_data, threshold=0.10, scale=1.0, output_format='network'):\n",
    "            \n",
    "        tick(1)\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights)\n",
    "\n",
    "        df = container.document_topic_weights[container.document_topic_weights.weight > threshold].reset_index()\n",
    "        \n",
    "        df['slim_title'] = df.title.apply(slim_title)\n",
    "        network = NetworkUtility.create_bipartite_network(df, 'slim_title', 'topic_id')\n",
    "        \n",
    "        tick()\n",
    "\n",
    "        if output_format == 'network':\n",
    "            \n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            \n",
    "            tick()\n",
    "            \n",
    "            p = plot_document_topic_network(network, layout, scale=scale, titles=titles)\n",
    "            bokeh.plotting.show(p)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            display(df)\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        tick(0)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_network,\n",
    "        layout_algorithm=layout_widget,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=threshold_widget,\n",
    "        scale=scale_widget,\n",
    "        output_format=output_format_widget\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        text_widget,\n",
    "        widgets.HBox([layout_widget, threshold_widget]), \n",
    "        widgets.HBox([output_format_widget, scale_widget, progress_widget]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "main_topic_network(tm_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Trends - Heatmap\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanford’s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c54540267764692bf9dbb889532fe50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(ToggleButton(value=True, description='Flip XY', layout=Layout(width='80px'), too…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "import bokeh.transform\n",
    "\n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = ['#ffffff', '#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']\n",
    "    mapper = bokeh.models.LinearColorMapper(palette=colors, low=0.0, high=1.0) # low=df.weight.min(), high=max_weight)\n",
    "    color_transform = bokeh.transform.transform('weight', mapper)\n",
    "    color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bokeh.models.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bokeh.models.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, glyph, titles, text_id):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "    \n",
    "    ''' Setup axis categories '''\n",
    "    x_range = list(map(str, df[xs].unique()))\n",
    "    y_range = list(map(str, df[ys].unique()))\n",
    "    \n",
    "    ''' Setup coloring and color bar '''\n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource(df)\n",
    "\n",
    "    plot_height = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = bokeh.plotting.figure(title=\"Topic heatmap\", tools=TOOLS, toolbar_location=\"right\", x_range=x_range,\n",
    "           y_range=y_range, x_axis_location=\"above\", plot_width=1000, plot_height=plot_height)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    if glyph == 'Circle':\n",
    "        cr = p.circle(color=color_transform, **args)\n",
    "    else:\n",
    "        cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"8pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(tooltips=None, callback=widgets_utility.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "    \n",
    "def display_doc_topic_heatmap(tm_data, key='max', flip_axis=False, glyph='Circle'):\n",
    "    try:\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights, n_words=100)\n",
    "        df = container.document_topic_weights.copy().reset_index()\n",
    "        df['document_id'] = df.document_id.astype(str)\n",
    "        df['topic_id'] = df.topic_id.astype(str)\n",
    "        df['author'] = df.title.apply(slim_title)\n",
    "        p = plot_topic_relevance_by_year(df, xs='author', ys='topic_id', flip_axis=flip_axis, glyph=glyph, titles=titles, text_id='topic_relevance')\n",
    "        bokeh.plotting.show(p)\n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "            \n",
    "def doc_topic_heatmap_gui(tm_data):\n",
    "    \n",
    "    def text_widget(element_id=None, default_value='', style='', line_height='20px'):\n",
    "        value = \"<span class='{}' style='line-height: {};{}'>{}</span>\".format(element_id, line_height, style, default_value) if element_id is not None else ''\n",
    "        return widgets.HTML(value=value, placeholder='', description='', layout=widgets.Layout(height='150px'))\n",
    "\n",
    "    text_id = 'topic_relevance'\n",
    "    #text_widget = widgets_utility.wf.create_text_widget(text_id)\n",
    "    text_widget = text_widget(text_id)\n",
    "    glyph = widgets.Dropdown(options=['Circle', 'Square'], value='Square', description='Glyph', layout=widgets.Layout(width=\"180px\"))\n",
    "    flip_axis = widgets.ToggleButton(value=True, description='Flip XY', tooltip='Flip X and Y axis', icon='', layout=widgets.Layout(width=\"80px\"))\n",
    "\n",
    "    iw = widgets.interactive(display_doc_topic_heatmap, tm_data=widgets.fixed(tm_data), glyph=glyph, flip_axis=flip_axis)\n",
    "\n",
    "    display(widgets.VBox([widgets.HBox([flip_axis, glyph ]), text_widget, iw.children[-1]]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "doc_topic_heatmap_gui(get_current_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Key Terms \n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24781c66b04046859f63c6858d959364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Paper', index=3, layout=Layout(width='40%'), options={'Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy.keyterms\n",
    "\n",
    "def display_document_key_terms_gui(corpus):\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "    methods = { 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = {v: k for k, v in df_documents['title'].to_dict().items()}\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=0, normalize='lemma', n_keyterms=10):\n",
    "        keyterms = methods[method](corpus[document_id], normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ' '.join([ x for x, y in keyterms ])\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            display(terms)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(CORPUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
