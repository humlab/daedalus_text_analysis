{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling\n",
    "\n",
    "See *Blei, 2003: Latent dirichlet allocation* [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) for a description of LDA.\n",
    "\n",
    "### LDA Topic modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is The Jupyter Project and Jupyter Notebook\n",
    "\n",
    "> Project [Jupyter](http://jupyter.org/) develops open-source software for **interactive and reproducible computing**.<br>\n",
    "> The **open science movement** is a driving force for Jupyter's popularity.<br>\n",
    "> In part a response to the **reproducibility crisis in science** and the **statistical crisis in science** (aka data dredging, p-hacking) in science.<br>\n",
    "> With Jupyter Notebooks contain **excutable code, equations, visualizations and narrative text**.<br>\n",
    "> It is a **web application** (can run locally) with a simple and easy to use web interface.\n",
    "> <img src=\"./images/narrative_new.svg\" style=\"width: 300px;padding: 0; margin: 0;\"><br>\n",
    "> Jupyter supports a large number of programming languages (50+ e.g. Python, R, JavaScript)\n",
    "\n",
    "The project is sponsered by large companies such as Google and Microsoft, and funders such as Alfred P. Sloan foundation. See link [jupyter.org/about](http://jupyter.org/about) for all sponsors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Instructions on How to Use Notebooks\n",
    "- **Menu Help -> User Interface Tour** gives an overview of the user interface.\n",
    "- **Code cells** contains the script code and have **In [x]** in the left margin.\n",
    "  - **In []** indicates that the code cell hasn't been executed yet.\n",
    "  - **In [n]** indicates that the code has been executed(n is an integer).\n",
    "  - **In [\\*]** indicates that the code is executing, or waiting to be executed (i.e. other cells are executing).\n",
    "- **The current code** is highlighted with a blue border - you make it current by clicking on it.\n",
    "- **SHIFT+ENTER** or **Play button** executes the current cell. Code cells aren't executed automatically.\n",
    "- **Out[n]** indicates the output (or result) of a cell's execution and is directly below the executed cell.\n",
    "- **SHIFT+ENTER** automatically selects the next code cell.\n",
    "- **SHIFT+ENTER** can hence be used repeatedly to executes the code cells in sequence.\n",
    "- **Menu Cell -> Run All** executes the entire notebook in a single step (can take some time to finish, notice how \"In [\\*]\" indicators change to \"In [n]\" ).\n",
    "- **Double-Click** on a cell to edit its content.\n",
    "- **ESC key** Leaves edit mode (or just click on any other cell).\n",
    "- **Kernel -> Restart** restarts server side kernel (use if notebook seems stuck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks\n",
    "- The risk of using tools and methods **without fully understanding** them\n",
    "- The risk of using tools and methods **for non-intended purposes or in new contexts**\n",
    "- How to verify **performance** (correctness of result)\n",
    "- Risk of **data dredging**, p-hacking, \"the statistical crisis\".\n",
    "- The risk that **engineer makes micro-decisions** the researcher don'r know about\n",
    "- The risk of **reading to much into visualizations** (networks, layouts, clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **What’s easy for humans can be extremely hard for computers**\n",
    "- **Human-in-the-loop or supervised learning can be very expensive**\n",
    "- Ambiguity and fuzziness of terms and phrases\n",
    "- Poor data quality, errors in data, wrong data, missing data, ambigeous data\n",
    "- Context, metadata, domain-specific data\n",
    "- Data size (to much, to little)\n",
    "- Computational methods requires a structured internal representation\n",
    "- Internal models are a simplified views of the data\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample high-level workflow\n",
    "\n",
    "<img src=\"./images/text_analysis_workflow.svg\" alt=\"\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text corpus processing\n",
    "### Extract Text From PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed, PDFPage\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "\n",
    "def extract_pdf_text(filename):\n",
    "    text_lines = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        \n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        resource_manager = PDFResourceManager()\n",
    "\n",
    "        result_buffer = StringIO()\n",
    "\n",
    "        device = TextConverter(resource_manager, result_buffer, codec='utf-8', laparams=LAParams())\n",
    "\n",
    "        interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        lines = result_buffer.getvalue().splitlines()\n",
    "        for line in lines:\n",
    "            text_lines.append(line)\n",
    "\n",
    "    return text_lines\n",
    "\n",
    "\n",
    "def extract_pdf_texts(source_folder, target_zip_filename):\n",
    "    with zipfile.ZipFile(target_zip_filename, 'w', zipfile.ZIP_DEFLATED) as target_zip:\n",
    "\n",
    "        for filename in glob.glob(os.path.join(source_folder,'*.pdf')):\n",
    "\n",
    "            print('Processing: ' + filename)\n",
    "\n",
    "            text_lines = extract_pdf_text(filename)\n",
    "\n",
    "            target_filename = os.path.splitext(os.path.split(filename)[1])[0] + '.txt'\n",
    "            target_filename = target_filename.lower().replace(' ', '_').replace(',','')\n",
    "\n",
    "            target_zip.writestr(target_filename, '\\n'.join(text_lines))\n",
    "            \n",
    "#source_folder = './data'\n",
    "#target_zip_filename = 'data/texts.zip'\n",
    "#extract_pdf_texts(source_folder, target_zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF Conversion to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Text Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : POS tag set: PUNCT SYM X ADJ VERB CONJ NUM DET ADV ADP  NOUN PROPN PART PRON SPACE INTJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# folded code\n",
    "# -*- coding: utf-8 -*-\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import string\n",
    "import zipfile\n",
    "import spacy\n",
    "import textacy\n",
    "import textacy.extract\n",
    "import textacy.preprocess\n",
    "import common.utility as utility\n",
    "import warnings\n",
    "import types\n",
    "from gensim import corpora, models, matutils\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from spacy import displacy\n",
    "\n",
    "logger = utility.getLogger(format=\"%(levelname)s;%(message)s\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_filenames(zip_filename, extension='.txt'):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return [ x for x in zf.namelist() if x.endswith(extension) ]\n",
    "    \n",
    "def get_text(zip_filename, filename):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return zf.read(filename).decode(encoding='utf-8')\n",
    "\n",
    "DEFAULT_TERM_PARAMS = dict(\n",
    "    args=dict(ngrams=1, named_entities=True, normalize='lemma', as_strings=True),\n",
    "    kwargs=dict(filter_stops=True, filter_punct=True, filter_nums=True, min_freq=1, drop_determiners=True, include_pos=('NOUN', 'PROPN', ))\n",
    ")\n",
    "    \n",
    "def filter_terms(doc, term_args, chunk_size=None):\n",
    "    kwargs = utility.extend({}, DEFAULT_TERM_PARAMS['kwargs'], term_args['kwargs'])\n",
    "    args = utility.extend({}, DEFAULT_TERM_PARAMS['args'], term_args['args'])\n",
    "    terms = doc.to_terms_list(\n",
    "        args['ngrams'],\n",
    "        args['named_entities'],\n",
    "        args['normalize'],\n",
    "        args['as_strings'],\n",
    "        **kwargs\n",
    "    )\n",
    "    return terms\n",
    "\n",
    "LANGUAGE = 'en'\n",
    "SOURCE_FOLDER = './data'\n",
    "SOURCE_FILENAME = os.path.join(SOURCE_FOLDER, 'p1_paper_texts_edited.zip')\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "DF_TAGSET = pd.read_csv('./data/tagset.csv', sep='\\t').fillna('')\n",
    "\n",
    "logger.info('POS tag set: ' + ' '.join(list(DF_TAGSET.POS.unique())))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Step: Automatic Text Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(source_filename, **args):\n",
    "    filenames = get_filenames(source_filename)\n",
    "    basename, extension = os.path.splitext(source_filename)\n",
    "    target_filename = basename + '_preprocessed' + extension # '_'.join(list(args.keys())) + extension\n",
    "    texts = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    with zipfile.ZipFile(target_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for filename, text in texts:\n",
    "            logger.info('Processing ' + filename)\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.normalize_whitespace(text)   \n",
    "            text = textacy.preprocess.fix_bad_unicode(text)   \n",
    "            text = textacy.preprocess.replace_currency_symbols(text)\n",
    "            text = textacy.preprocess.unpack_contractions(text)\n",
    "            text = textacy.preprocess.replace_urls(text)\n",
    "            text = textacy.preprocess.replace_emails(text)\n",
    "            text = textacy.preprocess.replace_phone_numbers(text)\n",
    "            text = textacy.preprocess.remove_accents(text)\n",
    "            #text = preprocess.preprocess_text(text, **args)\n",
    "            zf.writestr(filename, text)\n",
    "            \n",
    "#preprocess_text(SOURCE_FILENAME)\n",
    "#preprocess_text(SOURCE_FILENAME, lowercase=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Step: Create Text Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loading model: english...\n",
      "INFO : Using pipeline: tagger parser ner\n",
      "INFO : Working: Loading corpus ./data/corpus_en__disable().pkl...\n",
      "INFO : Working: Merging named entities...\n",
      "INFO : Done!\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "from textacy.spacier.utils import merge_spans\n",
    "\n",
    "def create_textacy_corpus(source_filename, language, preprocess_args):\n",
    "    make_title = lambda filename: filename.replace('_', ' ').replace('.txt', '').title()\n",
    "    filenames = get_filenames(source_filename)\n",
    "    corpus = textacy.Corpus(language)\n",
    "    text_stream = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    for filename, text in text_stream:\n",
    "        logger.info('Processing ' + filename)\n",
    "        text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "        text = textacy.preprocess.preprocess_text(text, **preprocess_args)\n",
    "        corpus.add_text(text, dict(filename=filename, title=make_title(filename)))\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    return corpus\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [ e for e in doc.ents if not e.text.isspace() ]\n",
    "    return doc\n",
    "\n",
    "def generate_textacy_corpus(source_filename, language, corpus_args, preprocess_args, merge_named_entities=True, force=False):\n",
    "    \n",
    "    corpus_tag = '_'.join([ k for k in preprocess_args if preprocess_args[k] ]) + \\\n",
    "        '_disable(' + ','.join(corpus_args.get('disable', [])) +')'\n",
    "    \n",
    "    textacy_corpus_filename = os.path.join(SOURCE_FOLDER, 'corpus_{}_{}.pkl'.format(language, corpus_tag))\n",
    "    \n",
    "    Language.factories['remove_whitespace_entities'] = lambda nlp, **cfg: remove_whitespace_entities\n",
    "    \n",
    "    logger.info('Loading model: english...')\n",
    "    nlp = textacy.load_spacy('en_core_web_sm', **corpus_args)\n",
    "    pipeline = lambda: [ x[0] for x in nlp.pipeline ]\n",
    "        \n",
    "    logger.info('Using pipeline: ' + ' '.join(pipeline()))\n",
    "\n",
    "    if force or not os.path.isfile(textacy_corpus_filename):\n",
    "        logger.info('Working: Computing new corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = create_textacy_corpus(source_filename, nlp, preprocess_args)\n",
    "        corpus.save(textacy_corpus_filename)\n",
    "    else:\n",
    "        logger.info('Working: Loading corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = textacy.Corpus.load(textacy_corpus_filename)\n",
    "        \n",
    "    if merge_named_entities:\n",
    "        logger.info('Working: Merging named entities...')\n",
    "        for doc in corpus:\n",
    "            named_entities = textacy.extract.named_entities(doc)\n",
    "            merge_spans(named_entities, doc.spacy_doc)\n",
    "    else:\n",
    "        logger.info('Note: named entities not merged')\n",
    "        \n",
    "    logger.info('Done!')\n",
    "    return textacy_corpus_filename, corpus\n",
    "\n",
    "def assign_document_titles(corpus):\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    \n",
    "def get_corpus_documents(corpus):\n",
    "    df_documents = pd.DataFrame([ (document_id, doc.metadata['title'], doc.metadata['filename']) for document_id, doc in enumerate(corpus) ], columns=['document_id', 'title', 'filename']).set_index('document_id')\n",
    "    return df_documents\n",
    "\n",
    "textacy_corpus_filename, corpus = generate_textacy_corpus(SOURCE_FILENAME, LANGUAGE, corpus_args=dict(), preprocess_args=dict(), merge_named_entities=True, force=False)\n",
    "df_documents = get_corpus_documents(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a39cfd59bc04097a49db86ee0da9e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_cleanup_text_gui(corpus, callback):\n",
    "    \n",
    "    document_options = {v: k for k, v in df_documents['title'].to_dict().items()}\n",
    "    \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    display_options = { 'Source text': 'source_text', 'Sanitized text': 'sanitized_text', 'Statistics': 'statistics'}\n",
    "    gui = types.SimpleNamespace(\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.FloatSlider(value=0, min=0, max=1.0, step=0.01, description='Min frequency', layout=widgets.Layout(width='400px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=[1,2,3], value=1, layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=False, description='Filter nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=False, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "\n",
    "        callback,\n",
    "\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        document_id=gui.document_id,\n",
    "        \n",
    "        ngrams=cleanup_gui.ngrams,\n",
    "        named_entities=gui.named_entities,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        filter_nums=gui.filter_nums,\n",
    "        include_pos=gui.include_pos,\n",
    "        min_freq=gui.min_freq,\n",
    "        drop_determiners=gui.drop_determiners\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.document_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(10, 10 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_cleaned_up_text(corpus, gui, display_type, document_id, **kwargs): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    #Additional candidates;\n",
    "    #is_alpha\tbool\tDoes the token consist of alphabetic characters? Equivalent to token.text.isalpha().\n",
    "    #is_ascii\tbool\tDoes the token consist of ASCII characters? Equivalent to [any(ord(c) >= 128 for c in token.text)].\n",
    "    #like_url\tbool\tDoes the token resemble a URL?\n",
    "    #like_email\tbool\tDoes the token resemble an email address?\n",
    "                                                                    \n",
    "    terms = [ x for x in corpus[document_id].to_terms_list(as_strings=True, **kwargs) ]\n",
    "    \n",
    "    if display_type == 'source_text':\n",
    "        # Utskrift av de första och sista 250 tecknen:\n",
    "        with gui.output_text.clear_output():\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print(' '.join(list(terms)))\n",
    "        return\n",
    "        \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join(list(terms)))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                print('Antal ord (termer): {}'.format(wf.N()))\n",
    "                print('Antal unika termer (vokabulär): {}'.format(wf.B()))\n",
    "                print(' ')\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                display(df)\n",
    "\n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "xgui, xuix =display_cleanup_text_gui(corpus, display_cleaned_up_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute LDA Topic Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677511858eef4efc857a268ed9c856de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(IntSlide…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "# OBS OBS! https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "DEFAULT_VECTORIZE_PARAMS = dict(tf_type='linear', apply_idf=False, idf_type='smooth', norm='l2', min_df=1, max_df=0.95)\n",
    "\n",
    "def compute_topic_model(corpus, tick=utility.noop, method='sklearn_lda', vec_args=None, term_args=None, tm_args=None, **args):\n",
    "    \n",
    "    tick()\n",
    "    vec_args = utility.extend({}, DEFAULT_VECTORIZE_PARAMS, vec_args)\n",
    "    \n",
    "    terms_iter = lambda: (filter_terms(doc, term_args) for doc in corpus)\n",
    "\n",
    "    vectorizer = textacy.Vectorizer(**vec_args)\n",
    "    doc_term_matrix = vectorizer.fit_transform(terms_iter())\n",
    "    tick()\n",
    "\n",
    "    if method == 'sklearn_lda':\n",
    "        model = textacy.TopicModel('lda', **tm_args)\n",
    "        model.fit(doc_term_matrix)\n",
    "        tick()\n",
    "        doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "        tick()\n",
    "        id2word = vectorizer.id_to_term\n",
    "    else:\n",
    "        dictionary = corpora.Dictionary(terms_iter())\n",
    "        lda_corpus = [dictionary.doc2bow(text) for text in terms_iter()]\n",
    "        id2word = dictionary\n",
    "        #id2word = vectorizer.id_to_term\n",
    "        #lda_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        model = models.LdaModel(\n",
    "            lda_corpus, # [doc for doc in terms_iter()],\n",
    "            num_topics  =  tm_args.get('n_topics', 0),\n",
    "            id2word     =  id2word,\n",
    "            iterations  =  tm_args.get('max_iter', 0),\n",
    "            passes      =  20,\n",
    "            alpha       = 'asymmetric'\n",
    "        )\n",
    "    \n",
    "    tm_model = types.SimpleNamespace(\n",
    "        model=model,\n",
    "        doc_term_matrix=doc_term_matrix,\n",
    "        doc_topic_matrix=doc_topic_matrix if method == 'sklearn_lda' else None,\n",
    "        id_to_term=id2word,\n",
    "        id2term=id2word,\n",
    "        vectorizer=vectorizer\n",
    "    )\n",
    "    \n",
    "    tick(0)\n",
    "    \n",
    "    return tm_model\n",
    "\n",
    "def get_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    topic_ids = range(0,doc_topic_matrix.shape[1])\n",
    "    for document_id in range(0,doc_topic_matrix.shape[1]):\n",
    "        topic_weights = doc_topic_matrix[document_id, :]\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_weights[topic_id] >= threshold:\n",
    "                yield (document_id, topic_id, topic_weights[topic_id])\n",
    "\n",
    "def get_df_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    it = get_doc_topic_weights(doc_topic_matrix, threshold)\n",
    "    df = pd.DataFrame(list(it), columns=['document_id', 'topic_id', 'weight']).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "def display_topic_model_gui(corpus, compute_callback):\n",
    "    \n",
    "    pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    engine_options = {'gensim': 'gensim', 'sklearn_lda': 'sklearn_lda'}\n",
    "    normalize_options = { 'None': False, 'Use lemma': 'lemma', 'Lowercase': 'lower'}\n",
    "    ngrams_options = { '1': [1], '1, 2': [1, 2], '1,2,3': [1, 2, 3] }\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        n_topics=widgets.IntSlider(description='#topics', min=5, max=50, value=20, step=1),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1),\n",
    "        max_iter=widgets.IntSlider(description='Max iterations', min=100, max=1000, value=20, step=10),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=widgets.Layout(width='200px')),\n",
    "        filter_stops=widgets.ToggleButton(value=True, description='Remove stopword',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=True, description='Remove nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        apply_idf=widgets.ToggleButton(value=False, description='Apply IDF',  tooltip='Apply TF-IDF', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=['NOUN', 'PROPN'], rows=7, layout=widgets.Layout(width='200px')),\n",
    "        method=widgets.Dropdown(description='Engine', options=engine_options, value='gensim', layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute'),\n",
    "        boxes=None,\n",
    "        output = widgets.Output(layout={'height': '500px', 'border': '1px solid black'}),\n",
    "        model=None\n",
    "    )\n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.n_topics,\n",
    "                gui.min_freq,\n",
    "                gui.max_iter\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners,\n",
    "                gui.apply_idf\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.method\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.VBox([gui.output]), # ,layout=widgets.Layout(top='20px', height='500px',width='100%'))\n",
    "    ])\n",
    "    fx = lambda *args: compute_callback(gui, *args)\n",
    "    gui.compute.on_click(fx)\n",
    "    return gui\n",
    "    \n",
    "\n",
    "def compute_callback(gui, *args):\n",
    "    \n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "        \n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        vec_args = dict(apply_idf=gui.apply_idf.value)\n",
    "        term_args = dict(\n",
    "            args=dict(\n",
    "                ngrams=gui.ngrams.value,\n",
    "                named_entities=gui.named_entities.value,\n",
    "                normalize=gui.normalize.value,\n",
    "                as_strings=True\n",
    "            ),\n",
    "            kwargs=dict(\n",
    "                filter_nums=gui.filter_nums.value,\n",
    "                drop_determiners=gui.drop_determiners.value,\n",
    "                min_freq=gui.min_freq.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                filter_stops=gui.filter_stops.value,\n",
    "                filter_punct=True\n",
    "            )\n",
    "        )\n",
    "        tm_args = dict(\n",
    "            n_topics=gui.n_topics.value,\n",
    "            max_iter=gui.max_iter.value,\n",
    "            learning_method='online', \n",
    "            n_jobs=1\n",
    "        )\n",
    "        method = gui.method.value\n",
    "        gui.model = compute_topic_model(\n",
    "            corpus=corpus,\n",
    "            tick=tick,\n",
    "            method=method,\n",
    "            vec_args=vec_args,\n",
    "            term_args=term_args,\n",
    "            tm_args=tm_args\n",
    "        )\n",
    "        \n",
    "tm_gui = display_topic_model_gui(corpus, compute_callback)\n",
    "display(tm_gui.boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Compiling dictionary...\n",
      "INFO : Compiling topic-tokens weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>213</td>\n",
       "      <td>ability</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>213</td>\n",
       "      <td>ability</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>536</td>\n",
       "      <td>academic</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>536</td>\n",
       "      <td>academic</td>\n",
       "      <td>0.008979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>536</td>\n",
       "      <td>academic</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.007093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>492</td>\n",
       "      <td>acceptance</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.004193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.010441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.012705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.022869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>access</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>383</td>\n",
       "      <td>accomplishment</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>5</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>6</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>7</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>9</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>11</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>12</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>13</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>14</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>15</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>16</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>17</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>18</td>\n",
       "      <td>342</td>\n",
       "      <td>ﬁgure</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>3</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>4</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>5</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>6</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>7</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>9</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>10</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.003128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>11</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>12</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>13</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>14</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>15</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>16</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.001792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>17</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>19</td>\n",
       "      <td>366</td>\n",
       "      <td>ﬁnding</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>2</td>\n",
       "      <td>491</td>\n",
       "      <td>ﬁrm</td>\n",
       "      <td>0.002566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>3</td>\n",
       "      <td>491</td>\n",
       "      <td>ﬁrm</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_id  token_id           token    weight\n",
       "0            4       213         ability  0.004586\n",
       "1           19       213         ability  0.000048\n",
       "2           14       536        academic  0.000059\n",
       "3           17       536        academic  0.008979\n",
       "4           18       536        academic  0.000053\n",
       "5            3       492      acceptance  0.007093\n",
       "6            6       492      acceptance  0.001792\n",
       "7            7       492      acceptance  0.000172\n",
       "8            9       492      acceptance  0.001792\n",
       "9           11       492      acceptance  0.001792\n",
       "10          13       492      acceptance  0.001792\n",
       "11          15       492      acceptance  0.001792\n",
       "12          16       492      acceptance  0.001792\n",
       "13           0        14          access  0.000100\n",
       "14           2        14          access  0.004193\n",
       "15           4        14          access  0.010441\n",
       "16           8        14          access  0.000041\n",
       "17          10        14          access  0.000013\n",
       "18          14        14          access  0.012705\n",
       "19          17        14          access  0.022869\n",
       "20          19        14          access  0.000048\n",
       "21           1       383  accomplishment  0.001792\n",
       "22           3       383  accomplishment  0.000050\n",
       "23           5       383  accomplishment  0.001989\n",
       "24           6       383  accomplishment  0.001792\n",
       "25           7       383  accomplishment  0.000172\n",
       "26           8       383  accomplishment  0.000040\n",
       "27           9       383  accomplishment  0.001792\n",
       "28          11       383  accomplishment  0.001792\n",
       "29          12       383  accomplishment  0.000095\n",
       "...        ...       ...             ...       ...\n",
       "3970         5       342           ﬁgure  0.001989\n",
       "3971         6       342           ﬁgure  0.001792\n",
       "3972         7       342           ﬁgure  0.000172\n",
       "3973         9       342           ﬁgure  0.001792\n",
       "3974        11       342           ﬁgure  0.001792\n",
       "3975        12       342           ﬁgure  0.000095\n",
       "3976        13       342           ﬁgure  0.001792\n",
       "3977        14       342           ﬁgure  0.000057\n",
       "3978        15       342           ﬁgure  0.001792\n",
       "3979        16       342           ﬁgure  0.001792\n",
       "3980        17       342           ﬁgure  0.000050\n",
       "3981        18       342           ﬁgure  0.000055\n",
       "3982         1       366          ﬁnding  0.001792\n",
       "3983         3       366          ﬁnding  0.000050\n",
       "3984         4       366          ﬁnding  0.000033\n",
       "3985         5       366          ﬁnding  0.000014\n",
       "3986         6       366          ﬁnding  0.001792\n",
       "3987         7       366          ﬁnding  0.000172\n",
       "3988         9       366          ﬁnding  0.001792\n",
       "3989        10       366          ﬁnding  0.003128\n",
       "3990        11       366          ﬁnding  0.001792\n",
       "3991        12       366          ﬁnding  0.000095\n",
       "3992        13       366          ﬁnding  0.001792\n",
       "3993        14       366          ﬁnding  0.000057\n",
       "3994        15       366          ﬁnding  0.001792\n",
       "3995        16       366          ﬁnding  0.001792\n",
       "3996        17       366          ﬁnding  0.000050\n",
       "3997        19       366          ﬁnding  0.000048\n",
       "3998         2       491             ﬁrm  0.002566\n",
       "3999         3       491             ﬁrm  0.000050\n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def compile_dictionary(lda):\n",
    "    logger.info('Compiling dictionary...')\n",
    "    token_ids, tokens = list(zip(*lda.id2word.items()))\n",
    "    dfs = lda.id2word.dfs.values() if lda.id2word.dfs is not None else [0] * len(tokens)\n",
    "    dictionary = pd.DataFrame({\n",
    "        'token_id': token_ids,\n",
    "        'token': tokens,\n",
    "        'dfs': list(dfs)\n",
    "    }).set_index('token_id')[['token', 'dfs']]\n",
    "    return dictionary\n",
    "    \n",
    "def compile_topic_token_weights(lda, dictionary, num_words=200):\n",
    "    logger.info('Compiling topic-tokens weights...')\n",
    "\n",
    "    df_topic_weights = pd.DataFrame(\n",
    "        [ (topic_id, token, weight)\n",
    "            for topic_id, tokens in (lda.show_topics(lda.num_topics, num_words=num_words, formatted=False))\n",
    "                for token, weight in tokens if weight > 0.0 ],\n",
    "        columns=['topic_id', 'token', 'weight']\n",
    "    )\n",
    "\n",
    "    df = pd.merge(\n",
    "        df_topic_weights.set_index('token'),\n",
    "        dictionary.reset_index().set_index('token'),\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "    return df.reset_index()[['topic_id', 'token_id', 'token', 'weight']]\n",
    "\n",
    "lda = tm_gui.model.model\n",
    "id2word = tm_gui.model.id2term\n",
    "\n",
    "df_dictionary = compile_dictionary(lda)\n",
    "df_topic_token_weights = compile_topic_token_weights(lda, df_dictionary, num_words=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_document_entities_gui(corpus):\n",
    "    \n",
    "    def display_document_entities(document_id, corpus):\n",
    "        displacy.render(corpus[document_id].spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    document_widget = widgets.Dropdown(description='Paper', options={v: k for k, v in df_documents['title'].to_dict().items()}, value=0, layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(display_document_entities,document_id=document_widget, corpus=widgets.fixed(corpus))\n",
    "\n",
    "    display(widgets.VBox([document_widget, widgets.VBox([itw.children[-1]],layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_entities_gui(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Key Terms \n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009df02b1bce46eba7b5bc6080faa082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Paper', index=5, layout=Layout(width='40%'), options={'Com…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy.keyterms\n",
    "\n",
    "def display_document_key_terms_gui(corpus):\n",
    "    \n",
    "    methods = { 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = {v: k for k, v in df_documents['title'].to_dict().items()}\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=0, normalize='lemma', n_keyterms=10):\n",
    "        keyterms = methods[method](corpus[document_id], normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ' '.join([ x for x, y in keyterms ])\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            display(terms)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Document Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utility as utility\n",
    "import ipywidgets as widgets\n",
    "\n",
    "df = pd.DataFrame([ utility.extend(dict(title=x.metadata['title']), textacy.TextStats(x).basic_counts) for x in corpus ])\n",
    "df[['title', 'n_chars', 'n_words', 'n_unique_words', 'n_sents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "hyphen_regexp = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def tokenize_and_sanitize(document, min_length=3, only_isalpha=True, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "        \n",
    "    #if de_hyphen:\n",
    "    #    document = re.sub(hyphen_regexp, r\"\\1\\2\\n\", document)\n",
    "\n",
    "    tokens = (x for x in document if not (x.is_space))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "    tokens = nltk.word_tokenize(document)\n",
    "    \n",
    "    # Ta bort ord kortare än tre tecken\n",
    "    if min_length > 0:\n",
    "        tokens = [ x for x in tokens if len([w for w in x if w.isalpha()]) > min_length ]\n",
    "        \n",
    "    # Ta bort ord som inte innehåller någon siffra eller bokstav\n",
    "    if only_isalpha:\n",
    "        tokens = [ x for x in tokens if x.isalpha() ]\n",
    "\n",
    "    if remove_puncts:\n",
    "        tokens = [ x.translate(punct_table) for x in tokens ]\n",
    "\n",
    "    # Transformera till små bokstäver\n",
    "    if to_lower:\n",
    "        tokens = [ x.lower() for x in tokens ]\n",
    "        \n",
    "    # Ta bort de vanligaste stoporden\n",
    "    if remove_stop:\n",
    "        stopwords = nltk.corpus.stopwords.words('swedish')\n",
    "        tokens = [ x for x in tokens if x not in stopwords ]\n",
    "        \n",
    "    return [ x for x in tokens if len(x) > 0 ]\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    \n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(8, 8 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "container=dict(\n",
    "    display_type=widgets.Dropdown(\n",
    "        description='Show',\n",
    "        value='statistics',\n",
    "        options={\n",
    "            'Source text': 'source_text',\n",
    "            'Sanitized text': 'sanitized_text',\n",
    "            'Statistics': 'statistics'           \n",
    "    }),\n",
    "    min_length=widgets.IntSlider(value=0, min=0, max=5, step=1, description='Min alpha', tooltip='Min number of alphabetic characters'),\n",
    "    de_hyphen=widgets.ToggleButton(value=False, description='Dehyphen', disabled=False, tooltip='Fix hyphens', icon=''),\n",
    "    to_lower=widgets.ToggleButton(value=False, description='Lowercase', disabled=False, tooltip='Transform text to lowercase', icon=''),\n",
    "    remove_stop=widgets.ToggleButton(value=False, description='No stopwords', disabled=False, tooltip='Remove stopwords', icon=''),\n",
    "    only_isalpha=widgets.ToggleButton(value=False, description='Only alpha', disabled=False, tooltip='Keep only alphabetic words', icon=''),\n",
    "    remove_puncts=widgets.ToggleButton(value=False, description='Remove puncts.', disabled=False, tooltip='Remove punctioations characters', icon=''),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='' )\n",
    ")\n",
    "\n",
    "output1 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "output2 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "default_output = None\n",
    "\n",
    "tokens = []\n",
    "\n",
    "def display_document(display_type, to_lower, remove_stop, only_isalpha, remove_puncts, min_length, de_hyphen, word_count=500):\n",
    "\n",
    "    global tokens\n",
    "    \n",
    "    p =  container['progress']\n",
    "    p.value = 0\n",
    "    try:\n",
    "        output1.clear_output()\n",
    "        output2.clear_output()\n",
    "        default_output.clear_output()\n",
    "        document = read_text_file('./data/urn-nbn-se-kb-digark-2106487.txt')\n",
    "        p.value = p.value + 1\n",
    "\n",
    "        if display_type == 'source_text':\n",
    "            # Utskrift av de första och sista 250 tecknen:\n",
    "            with output1:\n",
    "                print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            p.value = p.value + 1\n",
    "            return\n",
    "        \n",
    "        p.value = p.value + 1\n",
    "\n",
    "        tokens = tokenize_and_sanitize(\n",
    "            document,\n",
    "            de_hyphen=de_hyphen,\n",
    "            min_length=min_length,\n",
    "            only_isalpha=only_isalpha,\n",
    "            remove_puncts=remove_puncts,\n",
    "            to_lower=to_lower,\n",
    "            remove_stop=remove_stop\n",
    "        )\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics']:\n",
    "            \n",
    "            p.value = p.value + 1\n",
    "            \n",
    "            if display_type == 'sanitized_text':\n",
    "                with output1:\n",
    "                    display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                        ' '.join(tokens[:word_count]),\n",
    "                        ' '.join(tokens[-word_count:])\n",
    "                    ))\n",
    "                p.value = p.value + 1\n",
    "                return\n",
    "            \n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(tokens)\n",
    "                p.value = p.value + 1\n",
    "                \n",
    "                with output1:\n",
    "                    \n",
    "                    df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                    display(df)\n",
    "                \n",
    "                with output2:\n",
    "                    \n",
    "                    print('Antal ord (termer): {}'.format(wf.N()))\n",
    "                    print('Antal unika termer (vokabulär): {}'.format(wf.B()))\n",
    "                    print(' ')\n",
    "                    \n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "                    \n",
    "                    wf = nltk.FreqDist([len(x) for x in tokens])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "    \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        p.value = 0\n",
    "\n",
    "i_widgets = widgets.interactive(display_document, **container)\n",
    "default_output = i_widgets.children[-1]\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        container['display_type'],\n",
    "        container['to_lower'],\n",
    "        container['remove_stop'],\n",
    "        container['de_hyphen'],\n",
    "        container['only_isalpha'],\n",
    "        container['remove_puncts']\n",
    "    ]),\n",
    "    widgets.HBox([container['min_length'], container['progress']]),\n",
    "    widgets.HBox([output1, output2]),\n",
    "    default_output\n",
    "]))\n",
    "\n",
    "i_widgets.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.datasets\n",
    "\n",
    "cw = textacy.datasets.SupremeCourt()\n",
    "cw.download()\n",
    "records = cw.records()\n",
    "\n",
    "txt_strm, meta_strm = textacy.fileio.split_record_fields(records, 'text')\n",
    "corpus = textacy.Corpus(u'en', texts=txt_strm, metadatas=meta_strm)\n",
    "vectorizer = textacy.Vectorizer(\n",
    "    weighting='tfidf',\n",
    "    normalize=True,\n",
    "    smooth_idf=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "doc_term_matrix = vectorizer.fit_transform((\n",
    "    doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "    for doc in corpus\n",
    "))\n",
    "print(repr(doc_term_matrix))\n",
    "model = textacy.TopicModel('nmf', n_topics=10)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "print(doc_topic_matrix.shape)\n",
    "\n",
    "for t_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=10):\n",
    "    print('topic', t_idx, ':', ' '.join(top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# folded code\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "hyphen_regexp = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        document = f.read()\n",
    "    return document\n",
    "\n",
    "def tokenize_and_sanitize(document, de_hyphen=True, min_length=3, only_isalpha=True, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "        \n",
    "    # hantera avstavningar\n",
    "    if de_hyphen:\n",
    "        document = re.sub(hyphen_regexp, r\"\\1\\2\\n\", document)\n",
    "\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    \n",
    "    # Ta bort ord kortare än tre tecken\n",
    "    if min_length > 0:\n",
    "        tokens = [ x for x in tokens if len([w for w in x if w.isalpha()]) > min_length ]\n",
    "        \n",
    "    # Ta bort ord som inte innehåller någon siffra eller bokstav\n",
    "    if only_isalpha:\n",
    "        tokens = [ x for x in tokens if x.isalpha() ]\n",
    "\n",
    "    if remove_puncts:\n",
    "        tokens = [ x.translate(punct_table) for x in tokens ]\n",
    "\n",
    "    # Transformera till små bokstäver\n",
    "    if to_lower:\n",
    "        tokens = [ x.lower() for x in tokens ]\n",
    "        \n",
    "    # Ta bort de vanligaste stoporden\n",
    "    if remove_stop:\n",
    "        stopwords = nltk.corpus.stopwords.words('swedish')\n",
    "        tokens = [ x for x in tokens if x not in stopwords ]\n",
    "        \n",
    "    return [ x for x in tokens if len(x) > 0 ]\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    \n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(8, 8 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "container=dict(\n",
    "    display_type=widgets.Dropdown(\n",
    "        description='Show',\n",
    "        value='statistics',\n",
    "        options={\n",
    "            'Source text': 'source_text',\n",
    "            'Sanitized text': 'sanitized_text',\n",
    "            'Statistics': 'statistics'           \n",
    "    }),\n",
    "    min_length=widgets.IntSlider(value=0, min=0, max=5, step=1, description='Min alpha', tooltip='Min number of alphabetic characters'),\n",
    "    de_hyphen=widgets.ToggleButton(value=False, description='Dehyphen', disabled=False, tooltip='Fix hyphens', icon=''),\n",
    "    to_lower=widgets.ToggleButton(value=False, description='Lowercase', disabled=False, tooltip='Transform text to lowercase', icon=''),\n",
    "    remove_stop=widgets.ToggleButton(value=False, description='No stopwords', disabled=False, tooltip='Remove stopwords', icon=''),\n",
    "    only_isalpha=widgets.ToggleButton(value=False, description='Only alpha', disabled=False, tooltip='Keep only alphabetic words', icon=''),\n",
    "    remove_puncts=widgets.ToggleButton(value=False, description='Remove puncts.', disabled=False, tooltip='Remove punctioations characters', icon=''),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='' )\n",
    ")\n",
    "\n",
    "output1 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "output2 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "default_output = None\n",
    "\n",
    "tokens = []\n",
    "\n",
    "def display_document(display_type, to_lower, remove_stop, only_isalpha, remove_puncts, min_length, de_hyphen, word_count=500):\n",
    "\n",
    "    global tokens\n",
    "    \n",
    "    p =  container['progress']\n",
    "    p.value = 0\n",
    "    try:\n",
    "        output1.clear_output()\n",
    "        output2.clear_output()\n",
    "        default_output.clear_output()\n",
    "        document = read_text_file('./data/urn-nbn-se-kb-digark-2106487.txt')\n",
    "        p.value = p.value + 1\n",
    "\n",
    "        if display_type == 'source_text':\n",
    "            # Utskrift av de första och sista 250 tecknen:\n",
    "            with output1:\n",
    "                print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            p.value = p.value + 1\n",
    "            return\n",
    "        \n",
    "        p.value = p.value + 1\n",
    "\n",
    "        tokens = tokenize_and_sanitize(\n",
    "            document,\n",
    "            de_hyphen=de_hyphen,\n",
    "            min_length=min_length,\n",
    "            only_isalpha=only_isalpha,\n",
    "            remove_puncts=remove_puncts,\n",
    "            to_lower=to_lower,\n",
    "            remove_stop=remove_stop\n",
    "        )\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics']:\n",
    "            \n",
    "            p.value = p.value + 1\n",
    "            \n",
    "            if display_type == 'sanitized_text':\n",
    "                with output1:\n",
    "                    display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                        ' '.join(tokens[:word_count]),\n",
    "                        ' '.join(tokens[-word_count:])\n",
    "                    ))\n",
    "                p.value = p.value + 1\n",
    "                return\n",
    "            \n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(tokens)\n",
    "                p.value = p.value + 1\n",
    "                \n",
    "                with output1:\n",
    "                    \n",
    "                    df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                    display(df)\n",
    "                \n",
    "                with output2:\n",
    "                    \n",
    "                    print('Antal ord (termer): {}'.format(wf.N()))\n",
    "                    print('Antal unika termer (vokabulär): {}'.format(wf.B()))\n",
    "                    print(' ')\n",
    "                    \n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "                    \n",
    "                    wf = nltk.FreqDist([len(x) for x in tokens])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "    \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        p.value = 0\n",
    "\n",
    "i_widgets = widgets.interactive(display_document, **container)\n",
    "default_output = i_widgets.children[-1]\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        container['display_type'],\n",
    "        container['to_lower'],\n",
    "        container['remove_stop'],\n",
    "        container['de_hyphen'],\n",
    "        container['only_isalpha'],\n",
    "        container['remove_puncts']\n",
    "    ]),\n",
    "    widgets.HBox([container['min_length'], container['progress']]),\n",
    "    widgets.HBox([output1, output2]),\n",
    "    default_output\n",
    "]))\n",
    "\n",
    "i_widgets.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Språkbanken NER tagging av SOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/SOU_1990_total_ner_extracted.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4ad9c92b19c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m entities = pd.read_csv('./data/SOU_1990_total_ner_extracted.csv', sep='\\t',\n\u001b[0;32m----> 9\u001b[0;31m                        names=['filename', 'year', 'location', 'categories', 'entity'])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data/SOU_1990_total_ner_extracted.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_csv('./data/SOU_1990_total_ner_extracted.csv', sep='\\t',\n",
    "                       names=['filename', 'year', 'location', 'categories', 'entity'])\n",
    "\n",
    "entities['document_id'] = entities.filename.apply(lambda x: int(x.split('_')[1]))\n",
    "entities['categories'] = entities.categories.str.replace('/', ' ')\n",
    "entities['category'] = entities.categories.str.split(' ').str.get(0)\n",
    "entities['sub_category'] = entities.categories.str.split(' ').str.get(1)\n",
    "\n",
    "entities.drop(['location', 'categories'], inplace=True, axis=1)\n",
    "\n",
    "document_names = pd.read_csv('./data/SOU_1990_index.csv',\n",
    "                             sep='\\t',\n",
    "                             names=['year', 'sequence_id', 'report_name']).set_index('sequence_id')\n",
    "\n",
    "def plot_freqdist(wf, n=25, **kwargs):\n",
    "    data = list(zip(*wf.most_common(n)))\n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(13, 13/1.618))\n",
    "    plt.plot(x, y, '--ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.show()\n",
    "\n",
    "doc_names = { v: k for k, v in document_names.report_name.to_dict().items()}\n",
    "doc_names['** ALL DOCUMENTS **'] = 0\n",
    "@widgets.interact(category=entities.category.unique())\n",
    "def display_most_frequent_pos_tags(document_id=doc_names, category='LOC', top=10):\n",
    "    global entities\n",
    "    locations = entities\n",
    "    if document_id > 0:\n",
    "        locations = locations.loc[locations.document_id==document_id]\n",
    "    locations = locations.loc[locations.category==category]['entity']\n",
    "    location_freqs = nltk.FreqDist(locations)\n",
    "    #location_freqs.tabulate()\n",
    "    plot_freqdist(location_freqs, n=top)\n",
    "\n",
    "# display_most_frequent_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>MANDATORY STEP</span> Setup and Initialize the Notebook\n",
    "Use the **play** button, or press **Shift-Enter** to execute a code cell (select it first). The code imports Python libraries and frameworks, and initializes the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Folded Code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import common.utility\n",
    "from common.model_utility import ModelUtility\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "import common.widgets_utility as wf\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "#import common.vectorspace_utility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import logging\n",
    "import bokeh.models as bm\n",
    "import bokeh.palettes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pivottablejs import pivot_ui\n",
    "from IPython.display import display, HTML, clear_output, IFrame\n",
    "from itertools import product\n",
    "from bokeh.io import output_file, push_notebook\n",
    "from bokeh.core.properties import value, expr\n",
    "from bokeh.transform import transform, jitter\n",
    "from bokeh.layouts import row, column, widgetbox\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.models import ColumnDataSource, CustomJS\n",
    "\n",
    "logger = logging.getLogger('explore-topic-models')\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "AGGREGATES = { 'mean': np.mean, 'sum': np.sum, 'max': np.max, 'std': np.std }\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "pd.set_option('precision', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>MANDATORY STEP</span> Select LDA Topic Model\n",
    "- Select one of the previously computed and prepared topic models that you wan't to use in subsequent steps.\n",
    "- Models are computed in batch in accordance to \n",
    "<a href=\"./images/workflow-prepare.svg\">process flow</a> used in the *Digitala modeller* project.\n",
    "- Note that subsequent code cells are NOT updated (executed) automatically when a new model is selected.\n",
    "- Use the **play** button, or press **Shift-Enter** to execute the selected cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hidden code: Select current model state\n",
    "class ModelState:\n",
    "    \n",
    "    def __init__(self, data_folder):\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.basenames = ModelUtility.get_model_names(data_folder)\n",
    "        self.basename = self.basenames[0]\n",
    "        self.on_set_model_callback = None\n",
    "        \n",
    "    def set_model(self, basename=None):\n",
    "\n",
    "        basename = basename or self.basename\n",
    "        \n",
    "        self.basename = basename\n",
    "        self.topic_keys = ModelUtility.get_topic_keys(self.data_folder, basename)\n",
    "        state.max_alpha = self.topic_keys.alpha.max()\n",
    "        self.topic_overview = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'topic_tokens')\n",
    "        self.document_topic_weights = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'doc_topic_weights')\\\n",
    "            .drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "        self.topic_token_weights = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'topic_token_weights')\\\n",
    "            .drop('Unnamed: 0', axis=1, errors='ignore')\\\n",
    "            .dropna(subset=['token'])\n",
    "        self._years = list(range(\n",
    "            self.document_topic_weights.year.min(), self.document_topic_weights.year.max() + 1))\n",
    "        self.min_year = min(self._years)\n",
    "        self.max_year = max(self._years)\n",
    "        self.years = [None] + self._years\n",
    "        self.n_topics = self.topic_overview.topic_id.max() + 1\n",
    "        # https://stackoverflow.com/questions/44561609/how-does-mallet-set-its-default-hyperparameters-for-lda-i-e-alpha-and-beta\n",
    "        self.initial_alpha = 0.0  # 5.0 / self.n_topics if 'mallet' in state.basename else 1.0 / self.n_topics\n",
    "        self.initial_beta = 0.0  # 0.01 if 'mallet' in basename else 1.0 / self.n_topics\n",
    "        self._lda = None\n",
    "        self._topic_titles = None\n",
    "        self.corpus_documents = ModelUtility.get_corpus_documents(self.data_folder, self.basename).set_index('document_id')\n",
    "        print(\"Current model: \" + self.basename.upper())\n",
    "        \n",
    "        if self.on_set_model_callback is not None:\n",
    "            self.on_set_model_callback(self)\n",
    "            \n",
    "        # _fix_topictokens()\n",
    "        return self\n",
    "    \n",
    "    #def get_document_topic_weights(self, year=None, topic_id=None):\n",
    "    #    df = self.document_topic_weights\n",
    "    #    if year is None and topic_id is None:\n",
    "    #        return df\n",
    "    #    if topic_id is None:\n",
    "    #        return df[(df.year == year)]\n",
    "    #    if year is None:\n",
    "    #        return df[(df.topic_id == topic_id)]\n",
    "    #    return df[(df.year == year)&(df.topic_id == topic_id)]\n",
    "    \n",
    "    def get_unique_topic_ids(self):\n",
    "        return self.document_topic_weights['topic_id'].unique()\n",
    "    \n",
    "    #def get_topic_weight_by_year_or_document(self, key='mean', pivot_column=None):\n",
    "    #    \n",
    "    #    if pivot_column is None:\n",
    "    #        pivot_column = 'year' if year is None else 'document_id'    \n",
    "    #        \n",
    "    #    df = self.document_topic_weights(year) \\\n",
    "    #        .groupby([pivot_column,'topic_id']) \\\n",
    "    #        .agg(AGGREGATES[key])[['weight']].reset_index()\n",
    "    #    return df, pivot_column\n",
    "    \n",
    "    #return self.get_document_topic_weight_by_pivot_column(pivot_column, key, filter={'column': 'year', 'values': [year]})\n",
    "    \n",
    "    def get_document_topic_weight_by_filter(self, filters=None):\n",
    "        df = self.document_topic_weights.query('weight > 0')\n",
    "        for filter in (filters or []):\n",
    "            if 'query' in filter.keys():\n",
    "                df = df.query(filter['query'])\n",
    "            elif isinstance(filter['value'], str):\n",
    "                df = df[(df[filter['column']]==filter['value'])]\n",
    "            elif isinstance(filter['value'], list):\n",
    "                df = df[(df[filter['column']].isin(filter['value']))]\n",
    "        return df\n",
    "    \n",
    "    def get_document_topic_weight_by_pivot_column(self, pivot_column, key='mean', filters=None):\n",
    "        df = self.get_document_topic_weight_by_filter(filters)\n",
    "        df = df.groupby([pivot_column, 'topic_id'])\\\n",
    "               .agg(AGGREGATES[key])[['weight']].reset_index()\n",
    "        return df[df.weight > 0]\n",
    "    \n",
    "    def get_topic_tokens_dict(self, topic_id, n_top=200):\n",
    "        return self.get_topic_tokens(topic_id)\\\n",
    "            .sort_values(['weight'], ascending=False)\\\n",
    "            .head(n_top)[['token', 'weight']]\\\n",
    "            .set_index('token').to_dict()['weight']\n",
    "\n",
    "    def compute_topic_terms_vector_space(self, n_words=100):\n",
    "        '''\n",
    "        Create an align topic-term vector space of top n_words from each topic\n",
    "        '''\n",
    "        unaligned_vector_dicts = ( self.get_topic_tokens_dict(topic_id, n_words) for topic_id in range(0, self.n_topics) )\n",
    "        X, feature_names = ModelUtility.compute_and_align_vector_space(unaligned_vector_dicts)\n",
    "        return X, feature_names\n",
    "\n",
    "    def get_lda(self):\n",
    "        raise Exception(\"Use of LDA model disabled in this Notebook\")\n",
    "        '''\n",
    "        Get gensim model. Only used for pyLDAvis display\n",
    "        '''\n",
    "        if self._lda is None:\n",
    "            filename = os.path.join(self.data_folder, self.basename, 'gensim_model_{}.gensim.gz'.format(self.basename))\n",
    "            if os.path.isfile(filename):\n",
    "                self._lda = LdaModel.load(filename)\n",
    "                print('LDA model loaded...')\n",
    "            else:\n",
    "                print('LDA not found on disk...')\n",
    "        return self._lda \n",
    "    \n",
    "    def get_topic_titles(self, n_words=100, cache=True):\n",
    "        if cache and self._topic_titles is not None:\n",
    "            return self._topic_titles\n",
    "        _topic_titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=n_words)\n",
    "        self._topic_titles = _topic_titles if cache else None\n",
    "        return _topic_titles\n",
    "    \n",
    "    def get_topic_tokens(self, topic_id, max_n_words=500):\n",
    "        tokens = state.topic_token_weights\\\n",
    "            .loc[lambda x: x.topic_id == topic_id]\\\n",
    "            .sort_values('weight',ascending=False)[:max_n_words]\n",
    "        return tokens\n",
    "    \n",
    "    def get_topic_alphas(self):\n",
    "        tokens = state.topic_token_weights\\\n",
    "            .loc[lambda x: x.topic_id == topic_id]\\\n",
    "            .sort_values('weight',ascending=False)[:max_n_words]\n",
    "        alpas = ModelUtility.get_topic_alphas\n",
    "        return tokens\n",
    "    \n",
    "    def get_topic_year_aggregate_weights(self, fn, threshold):\n",
    "        df = self.document_topic_weights[(self.document_topic_weights.weight > 0.001)]\n",
    "        df = df.groupby(['year', 'topic_id']).agg(fn)['weight'].reset_index()\n",
    "        df = df[(df.weight>=threshold)]\n",
    "        return df\n",
    "    \n",
    "    def get_topic_proportions(self):\n",
    "        corpus_documents = self.get_corpus_documents()\n",
    "        document_topic_weights = self.document_topic_weights\n",
    "        topic_proportion = ModelUtility.compute_topic_proportions(document_topic_weights, corpus_documents)\n",
    "        return topic_proportion\n",
    "    \n",
    "    def get_corpus_documents(self):\n",
    "        #if self.corpus_documents is None:\n",
    "        #    self.corpus_documents = ModelUtility.get_corpus_documents(self.data_folder, self.basename)\n",
    "        return self.corpus_documents\n",
    "\n",
    "    def on_set_model(self, callback):\n",
    "        self.on_set_model_callback = callback\n",
    "        return self\n",
    "        \n",
    "def on_set_model_handler(state):\n",
    "\n",
    "    if 'report_name' in state.corpus_documents:\n",
    "        return\n",
    "    \n",
    "    state.source_documents = pd.read_csv('data/SOU_1990_index.csv', sep='\\t', header=None, names=['year', 'report_id', 'report_name'])\n",
    "    state.corpus_documents['report_id'] = state.corpus_documents.document.str.split('_').apply(lambda x: x[1]).astype(np.int64)\n",
    "    state.corpus_documents['report_name'] = pd.merge(state.corpus_documents, state.source_documents, how='inner', on=['year', 'report_id']).report_name\n",
    "    state.corpus_documents['report_name'] = state.corpus_documents.apply(lambda x: '{}-{} {}'.format(x['year'], x['report_id'], x['report_name'])[:50], axis=1)\n",
    "    state.document_topic_weights['report_name'] = pd.merge(state.document_topic_weights, state.corpus_documents, left_on='document_id', right_index=True).report_name\n",
    "\n",
    "def select_model_main(state):\n",
    "    \n",
    "    basename_widget = widgets.Dropdown(\n",
    "        options=state.basenames,\n",
    "        value=state.basename,\n",
    "        description='Topic model',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='75%')\n",
    "    )\n",
    "    \n",
    "    w = widgets.interactive(state.set_model, basename=basename_widget, state=widgets.fixed(state))\n",
    "    display(widgets.VBox((basename_widget,) + (w.children[-1],)))\n",
    "    w.update()\n",
    "\n",
    "state = ModelState('./data').on_set_model(on_set_model_handler)\n",
    "\n",
    "select_model_main(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Word Distribution - Wordcloud and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_topic_distribution_widgets(callback, state, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=state.n_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', state.n_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(topic_id=0, n_words=100, output_format='Wordcloud', widget_container=None):\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = state.topic_token_weights.loc[(state.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = state.get_topic_titles(n_words=n_words, cache=True).iloc[topic_id]\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = state.get_topic_tokens(topic_id, n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(state.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "display_topic_distribution_widgets(display_wordcloud, state, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Word Distribution - Chart\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display topic's word distribution\n",
    "\n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = ColumnDataSource(tokens)\n",
    "\n",
    "    p = figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bm.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def plot_topic_tokens_charts(tokens, flag=True):\n",
    "\n",
    "    if flag:\n",
    "        left = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        show(left)\n",
    "        return\n",
    "\n",
    "    left = plot_topic_word_distribution(tokens, plot_width=450, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "    right = plot_topic_word_distribution(tokens, plot_width=450, plot_height=500, title='', tools='pan')\n",
    "\n",
    "    source = ColumnDataSource({'x':[], 'y':[], 'width':[], 'height':[]})\n",
    "    left.x_range.callback = create_js_callback('x', 'width', source)\n",
    "    left.y_range.callback = create_js_callback('y', 'height', source)\n",
    "\n",
    "    rect = bm.Rect(x='x', y='y', width='width', height='height', fill_alpha=0.0, line_color='blue', line_alpha=0.4)\n",
    "    right.add_glyph(source, rect)\n",
    "\n",
    "    show(row(left, right))\n",
    "\n",
    "def display_topic_tokens(topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    tokens = state.get_topic_tokens(topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        plot_topic_tokens_charts(tokens)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "    widget_container.reset()\n",
    "        \n",
    "display_topic_distribution_widgets(display_topic_tokens, state, 'wc01', ['Chart', 'Table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic's Trend Over Time or Documents\n",
    "- Displays topic's share over documents or time.\n",
    "- Note that source documents (i.e. SOU reports) are splitted into 1000 word chunks (LDA document) by the topic modelling process\n",
    "- If \"SOU Report\" or \"Year\" is selected then the **max** or **mean** weight is selected from corresponding LDA documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "import numpy as np\n",
    "import math\n",
    "import bokeh.plotting\n",
    "from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "from bokeh.models.glyphs import VBar\n",
    "from bokeh.io import curdoc, show\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=700, title='', tools=TOOLS, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, pivot_config, value_column, widgets_container, output_format='Chart', state=None, threshold=0.01):\n",
    "    \n",
    "    pivot_column = pivot_config['pivot_column']\n",
    "    tokens = state.get_topic_titles(n_words=200, cache=True).iloc[topic_id]\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = value_column if pivot_column is not None else 'weight'\n",
    "    \n",
    "    df = state.document_topic_weights[(state.document_topic_weights.topic_id==topic_id)]\n",
    "    \n",
    "    if pivot_column is not None:\n",
    "        df = df.groupby([pivot_column]).agg([np.mean, np.max])['weight'].reset_index()\n",
    "        df.columns = [pivot_column, 'mean', 'max' ]\n",
    "        df = df[(df[value_column] > threshold)]\n",
    "        \n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = pivot_column.title()\n",
    "        y_label = value_column.title() + ('weight' if value_column != 'weight' else '')\n",
    "        p = plot_topic_trend(df, pivot_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        show(p)\n",
    "\n",
    "def create_topic_trend_widgets(state):\n",
    "    pivot_options = {\n",
    "        '': { 'pivot_column': None, 'filter': None },\n",
    "        'SOU Report': { 'pivot_column': 'report_name', 'filter': None },\n",
    "        'Year': { 'pivot_column': 'year', 'filter': None },\n",
    "        'LDA Document': { 'pivot_column': 'document_id', 'filter': None }\n",
    "    } \n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        #year=wf.create_select_widget('Year', options=state.years, value=state.years[-1]),\n",
    "        pivot_config=widgets.Dropdown(\n",
    "            options=pivot_options,\n",
    "            value=pivot_options['SOU Report'],\n",
    "            description='Group by'\n",
    "        ),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=state.n_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "        aggregate=widgets.Dropdown(options=['max', 'mean'], value='max', description='Aggregate')\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', state.n_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        pivot_config=wc.pivot_config,\n",
    "        value_column=wc.aggregate,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        state=widgets.fixed(state),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.pivot_config, wc.aggregate, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "    \n",
    "create_topic_trend_widgets(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic to Document Network\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "     \n",
    "def plot_topic_year_network(network, layout, scale=1.0, titles=None):\n",
    "\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=TOOLS)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bm.HoverTool(renderers=[r_topics], tooltips=None, callback=wf.WidgetUtility.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(\n",
    "        x='x', y='y', text='name', level='overlay',\n",
    "        x_offset=0, y_offset=0, text_font_size='8pt'\n",
    "    )\n",
    "    \n",
    "    p.add_layout(\n",
    "        bm.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bm.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_year_network(state):\n",
    "    \n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='nx_id1',\n",
    "        text=wf.create_text_widget('nx_id1', style=\"display: inline; height='400px'\"),\n",
    "        year=widgets.IntSlider(description='Year', min=state.min_year, max=state.max_year, step=1, value=state.min_year, continues_update=False),\n",
    "        pivot_column=widgets.Dropdown(\n",
    "            options={\n",
    "                'SOU report': 'report_name',\n",
    "                'Year': 'year'\n",
    "            },\n",
    "            value='report_name',\n",
    "            description='Pivot'\n",
    "        ),\n",
    "        scale=widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False),\n",
    "        output_format=widgets.Dropdown(\n",
    "            options={'Network': 'network', 'Table': 'table'},\n",
    "            value='network',\n",
    "            description='Output'\n",
    "        ),\n",
    "        layout=widgets.Dropdown(\n",
    "            options=list(layout_algorithms.keys()),\n",
    "            value='Fruchterman-Reingold',\n",
    "            description='Layout'\n",
    "        ),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"40%\"))\n",
    "    ) \n",
    "    \n",
    "    wc.previous = wc.create_prev_id_button('year', 10000)\n",
    "    wc.next = wc.create_next_id_button('year', 10000)    \n",
    "    \n",
    "    def display_topic_year_network(\n",
    "        layout_algorithm,\n",
    "        threshold=0.50,\n",
    "        scale=1.0,\n",
    "        pivot_column='report_name',\n",
    "        year=None,\n",
    "        output_format='network'\n",
    "    ):\n",
    "        wc.progress.value = 1\n",
    "        \n",
    "        titles = state.get_topic_titles()\n",
    "        filters = []\n",
    "        if year is not None:\n",
    "            filters = [ { 'column': 'year', 'value': year }]\n",
    "        filters = filters + [ { 'query': 'weight >= {}'.format(threshold) } ]\n",
    "        df = state.get_document_topic_weight_by_pivot_column(pivot_column, key='max', filters=filters)\n",
    "        df = df[df.weight > threshold]\n",
    "        \n",
    "        wc.progress.value = 2\n",
    "\n",
    "        network = NetworkUtility.create_bipartite_network(df, pivot_column, 'topic_id')\n",
    "        \n",
    "        wc.progress.value = 3\n",
    "\n",
    "        if output_format == 'network':\n",
    "            \n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            \n",
    "            wc.progress.value = 4\n",
    "            \n",
    "            p = plot_topic_year_network(network, layout, scale=scale, titles=titles)\n",
    "            show(p)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            print(df.shape)\n",
    "            display(df)\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        wc.progress.value = 0\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_year_network,\n",
    "        layout_algorithm=wc.layout,\n",
    "        threshold=wc.threshold,\n",
    "        scale=wc.scale,\n",
    "        pivot_column=wc.pivot_column,\n",
    "        year=wc.year,\n",
    "        output_format=wc.output_format\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.layout, wc.year, wc.previous, wc.next]),\n",
    "        widgets.HBox([wc.pivot_column, wc.scale]),\n",
    "        widgets.HBox([wc.output_format, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "    \n",
    "main_topic_year_network(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Trends - Heatmap\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanford’s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "\n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = [\"#efefef\", \"#75968f\", \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\", \"#ddb7b1\", \"#cc7878\",\n",
    "              \"#933b41\", \"#550b1d\"]\n",
    "    mapper = bm.LinearColorMapper(palette=colors, low=df.weight.min(), high=max_weight)\n",
    "    color_transform = transform('weight', mapper)\n",
    "    color_bar = bm.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bm.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bm.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, glyph, titles, text_id):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "    \n",
    "    ''' Setup axis categories '''\n",
    "    x_range = list(map(str, df[xs].unique()))\n",
    "    y_range = list(map(str, df[ys].unique()))\n",
    "    \n",
    "    ''' Setup coloring and color bar '''\n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    plot_height = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = figure(title=\"Topic heatmap\", tools=TOOLS, toolbar_location=\"right\", x_range=x_range,\n",
    "           y_range=y_range, x_axis_location=\"above\", plot_width=1000, plot_height=plot_height)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    if glyph == 'Circle':\n",
    "        cr = p.circle(color=color_transform, **args)\n",
    "    else:\n",
    "        cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"5pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bm.HoverTool(tooltips=None, callback=wf.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def topic_heatmap_main(state):\n",
    "    \n",
    "    def display_topic_relevance_by_year(state, key='max', pivot_column=None, year=None, flip_axis=False, glyph='Circle', wdgs=None):\n",
    "        \n",
    "        try:\n",
    "            wdgs.reset()\n",
    "            wdgs.forward()\n",
    "            \n",
    "            titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=100)\n",
    "            wdgs.forward()\n",
    "\n",
    "            year = (year or 0)\n",
    "            \n",
    "            pivot_column = 'year' if year > 0 else (pivot_column or 'report_name')\n",
    "            filters = [{'column': 'year', 'values': [year]}] if year > 0 else []\n",
    "            \n",
    "            df = state.get_document_topic_weight_by_pivot_column(pivot_column, key, filters=filters)\n",
    "            \n",
    "            wdgs.forward()\n",
    "            \n",
    "            df[pivot_column] = df[pivot_column].astype(str)\n",
    "            df['topic_id'] = df.topic_id.astype(str)\n",
    "            \n",
    "            wdgs.forward()\n",
    "            \n",
    "            p = plot_topic_relevance_by_year(df, xs=pivot_column, ys='topic_id', flip_axis=flip_axis, glyph=glyph, titles=titles, text_id='topic_relevance')\n",
    "            \n",
    "            show(p)\n",
    "            wdgs.reset()\n",
    "        except Exception as ex:\n",
    "            raise\n",
    "            logger.error(ex)\n",
    "        finally:\n",
    "            wdgs.reset()\n",
    "\n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        text_id='topic_relevance',\n",
    "        text=wf.create_text_widget('topic_relevance'),\n",
    "        year=widgets.Dropdown(options=state.years, value=None, description='Year', layout=widgets.Layout(width=\"140px\")),\n",
    "        pivot_column=widgets.Dropdown(\n",
    "            options={\n",
    "                'SOU report': 'report_name',\n",
    "                # 'LDA document': 'document_id',\n",
    "                'Year': 'year'\n",
    "            },\n",
    "            value='report_name',\n",
    "            description='Pivot',\n",
    "            layout=widgets.Layout(width=\"200px\")\n",
    "        ),\n",
    "        aggregate=widgets.Dropdown(options=['max', 'mean'], value='max', description='Aggregate', layout=widgets.Layout(width=\"180px\")),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"35%\")),\n",
    "        glyph=widgets.Dropdown(options=['Circle', 'Square'], value='Square', description='Glyph', layout=widgets.Layout(width=\"180px\")),\n",
    "        flip_axis=widgets.ToggleButton(value=True, description='Flip XY', tooltip='Flip X and Y axis', icon='', layout=widgets.Layout(width=\"80px\"))\n",
    "    )\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_relevance_by_year,\n",
    "        state=widgets.fixed(state),\n",
    "        key=wc.aggregate,\n",
    "        pivot_column=wc.pivot_column,\n",
    "        year=wc.year,\n",
    "        glyph=wc.glyph,\n",
    "        flip_axis=wc.flip_axis,\n",
    "        wdgs=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([wc.pivot_column, wc.year, wc.aggregate, wc.flip_axis, wc.glyph, wc.progress ]),\n",
    "        wc.text,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "            \n",
    "topic_heatmap_main(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
