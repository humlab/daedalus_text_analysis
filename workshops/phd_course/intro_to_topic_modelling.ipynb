{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling\n",
    "\n",
    "See *Blei, 2003: Latent dirichlet allocation* [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) for a description of LDA.\n",
    "\n",
    "### LDA Topic modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is The Jupyter Project and Jupyter Notebook\n",
    "\n",
    "> Project [Jupyter](http://jupyter.org/) develops open-source software for **interactive and reproducible computing**.<br>\n",
    "> The **open science movement** is a driving force for Jupyter's popularity.<br>\n",
    "> In part a response to the **reproducibility crisis in science** and the **statistical crisis in science** (aka data dredging, p-hacking) in science.<br>\n",
    "> With Jupyter Notebooks contain **excutable code, equations, visualizations and narrative text**.<br>\n",
    "> It is a **web application** (can run locally) with a simple and easy to use web interface.\n",
    "> <img src=\"./images/narrative_new.svg\" style=\"width: 300px;padding: 0; margin: 0;\"><br>\n",
    "> Jupyter supports a large number of programming languages (50+ e.g. Python, R, JavaScript)\n",
    "\n",
    "The project is sponsered by large companies such as Google and Microsoft, and funders such as Alfred P. Sloan foundation. See link [jupyter.org/about](http://jupyter.org/about) for all sponsors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Instructions on How to Use Notebooks\n",
    "- **Menu Help -> User Interface Tour** gives an overview of the user interface.\n",
    "- **Code cells** contains the script code and have **In [x]** in the left margin.\n",
    "  - **In []** indicates that the code cell hasn't been executed yet.\n",
    "  - **In [n]** indicates that the code has been executed(n is an integer).\n",
    "  - **In [\\*]** indicates that the code is executing, or waiting to be executed (i.e. other cells are executing).\n",
    "- **The current code** is highlighted with a blue border - you make it current by clicking on it.\n",
    "- **SHIFT+ENTER** or **Play button** executes the current cell. Code cells aren't executed automatically.\n",
    "- **Out[n]** indicates the output (or result) of a cell's execution and is directly below the executed cell.\n",
    "- **SHIFT+ENTER** automatically selects the next code cell.\n",
    "- **SHIFT+ENTER** can hence be used repeatedly to executes the code cells in sequence.\n",
    "- **Menu Cell -> Run All** executes the entire notebook in a single step (can take some time to finish, notice how \"In [\\*]\" indicators change to \"In [n]\" ).\n",
    "- **Double-Click** on a cell to edit its content.\n",
    "- **ESC key** Leaves edit mode (or just click on any other cell).\n",
    "- **Kernel -> Restart** restarts server side kernel (use if notebook seems stuck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks\n",
    "- The risk of using tools and methods **without fully understanding** them\n",
    "- The risk of using tools and methods **for non-intended purposes or in new contexts**\n",
    "- How to verify **performance** (correctness of result)\n",
    "- Risk of **data dredging**, p-hacking, \"the statistical crisis\".\n",
    "- The risk that **engineer makes micro-decisions** the researcher don'r know about\n",
    "- The risk of **reading to much into visualizations** (networks, layouts, clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **What’s easy for humans can be extremely hard for computers**\n",
    "- **Human-in-the-loop or supervised learning can be very expensive**\n",
    "- Ambiguity and fuzziness of terms and phrases\n",
    "- Poor data quality, errors in data, wrong data, missing data, ambigeous data\n",
    "- Context, metadata, domain-specific data\n",
    "- Data size (to much, to little)\n",
    "- Computational methods requires a structured internal representation\n",
    "- Internal models are a simplified views of the data\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample high-level workflow\n",
    "\n",
    "<img src=\"./images/text_analysis_workflow.svg\" alt=\"\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr {\n",
       "    text-align: left;\n",
       "    vertical-align: top;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr {\n",
    "    text-align: left;\n",
    "    vertical-align: top;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text corpus processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Clean up the SOU text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/urn-nbn-se-kb-digark-2106487.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\ipywidgets\\widgets\\interaction.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwidget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_interact_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwarg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mshow_inline_matplotlib_plots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_display\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5e472a1b897d>\u001b[0m in \u001b[0;36mdisplay_document\u001b[1;34m(display_type, to_lower, remove_stop, only_isalpha, remove_puncts, min_length, de_hyphen, word_count)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0moutput2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mdefault_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_text_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/urn-nbn-se-kb-digark-2106487.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5e472a1b897d>\u001b[0m in \u001b[0;36mread_text_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/urn-nbn-se-kb-digark-2106487.txt'"
     ]
    }
   ],
   "source": [
    "# folded code\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import string\n",
    "\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "hyphen_regexp = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        document = f.read()\n",
    "    return document\n",
    "\n",
    "def tokenize_and_sanitize(document, de_hyphen=True, min_length=3, only_isalpha=True, remove_puncts=True, to_lower=True, remove_stop=True):\n",
    "        \n",
    "    # hantera avstavningar\n",
    "    if de_hyphen:\n",
    "        document = re.sub(hyphen_regexp, r\"\\1\\2\\n\", document)\n",
    "\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    \n",
    "    # Ta bort ord kortare än tre tecken\n",
    "    if min_length > 0:\n",
    "        tokens = [ x for x in tokens if len([w for w in x if w.isalpha()]) > min_length ]\n",
    "        \n",
    "    # Ta bort ord som inte innehåller någon siffra eller bokstav\n",
    "    if only_isalpha:\n",
    "        tokens = [ x for x in tokens if x.isalpha() ]\n",
    "\n",
    "    if remove_puncts:\n",
    "        tokens = [ x.translate(punct_table) for x in tokens ]\n",
    "\n",
    "    # Transformera till små bokstäver\n",
    "    if to_lower:\n",
    "        tokens = [ x.lower() for x in tokens ]\n",
    "        \n",
    "    # Ta bort de vanligaste stoporden\n",
    "    if remove_stop:\n",
    "        stopwords = nltk.corpus.stopwords.words('swedish')\n",
    "        tokens = [ x for x in tokens if x not in stopwords ]\n",
    "        \n",
    "    return [ x for x in tokens if len(x) > 0 ]\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    \n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(8, 8 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "container=dict(\n",
    "    display_type=widgets.Dropdown(\n",
    "        description='Show',\n",
    "        value='statistics',\n",
    "        options={\n",
    "            'Source text': 'source_text',\n",
    "            'Sanitized text': 'sanitized_text',\n",
    "            'Statistics': 'statistics'           \n",
    "    }),\n",
    "    min_length=widgets.IntSlider(value=0, min=0, max=5, step=1, description='Min alpha', tooltip='Min number of alphabetic characters'),\n",
    "    de_hyphen=widgets.ToggleButton(value=False, description='Dehyphen', disabled=False, tooltip='Fix hyphens', icon=''),\n",
    "    to_lower=widgets.ToggleButton(value=False, description='Lowercase', disabled=False, tooltip='Transform text to lowercase', icon=''),\n",
    "    remove_stop=widgets.ToggleButton(value=False, description='No stopwords', disabled=False, tooltip='Remove stopwords', icon=''),\n",
    "    only_isalpha=widgets.ToggleButton(value=False, description='Only alpha', disabled=False, tooltip='Keep only alphabetic words', icon=''),\n",
    "    remove_puncts=widgets.ToggleButton(value=False, description='Remove puncts.', disabled=False, tooltip='Remove punctioations characters', icon=''),\n",
    "    progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='' )\n",
    ")\n",
    "\n",
    "output1 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "output2 = widgets.Output() #layout={'border': '1px solid black'})\n",
    "default_output = None\n",
    "\n",
    "tokens = []\n",
    "\n",
    "def display_document(display_type, to_lower, remove_stop, only_isalpha, remove_puncts, min_length, de_hyphen, word_count=500):\n",
    "\n",
    "    global tokens\n",
    "    \n",
    "    p =  container['progress']\n",
    "    p.value = 0\n",
    "    try:\n",
    "        output1.clear_output()\n",
    "        output2.clear_output()\n",
    "        default_output.clear_output()\n",
    "        document = read_text_file('./data/urn-nbn-se-kb-digark-2106487.txt')\n",
    "        p.value = p.value + 1\n",
    "\n",
    "        if display_type == 'source_text':\n",
    "            # Utskrift av de första och sista 250 tecknen:\n",
    "            with output1:\n",
    "                print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            p.value = p.value + 1\n",
    "            return\n",
    "        \n",
    "        p.value = p.value + 1\n",
    "\n",
    "        tokens = tokenize_and_sanitize(\n",
    "            document,\n",
    "            de_hyphen=de_hyphen,\n",
    "            min_length=min_length,\n",
    "            only_isalpha=only_isalpha,\n",
    "            remove_puncts=remove_puncts,\n",
    "            to_lower=to_lower,\n",
    "            remove_stop=remove_stop\n",
    "        )\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics']:\n",
    "            \n",
    "            p.value = p.value + 1\n",
    "            \n",
    "            if display_type == 'sanitized_text':\n",
    "                with output1:\n",
    "                    display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                        ' '.join(tokens[:word_count]),\n",
    "                        ' '.join(tokens[-word_count:])\n",
    "                    ))\n",
    "                p.value = p.value + 1\n",
    "                return\n",
    "            \n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(tokens)\n",
    "                p.value = p.value + 1\n",
    "                \n",
    "                with output1:\n",
    "                    \n",
    "                    df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                    display(df)\n",
    "                \n",
    "                with output2:\n",
    "                    \n",
    "                    print('Antal ord (termer): {}'.format(wf.N()))\n",
    "                    print('Antal unika termer (vokabulär): {}'.format(wf.B()))\n",
    "                    print(' ')\n",
    "                    \n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "                    \n",
    "                    wf = nltk.FreqDist([len(x) for x in tokens])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "    \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        p.value = 0\n",
    "\n",
    "i_widgets = widgets.interactive(display_document, **container)\n",
    "default_output = i_widgets.children[-1]\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        container['display_type'],\n",
    "        container['to_lower'],\n",
    "        container['remove_stop'],\n",
    "        container['de_hyphen'],\n",
    "        container['only_isalpha'],\n",
    "        container['remove_puncts']\n",
    "    ]),\n",
    "    widgets.HBox([container['min_length'], container['progress']]),\n",
    "    widgets.HBox([output1, output2]),\n",
    "    default_output\n",
    "]))\n",
    "\n",
    "i_widgets.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT: Språkbanken NER tagging av SOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/SOU_1990_total_ner_extracted.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4ad9c92b19c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m entities = pd.read_csv('./data/SOU_1990_total_ner_extracted.csv', sep='\\t',\n\u001b[1;32m----> 9\u001b[1;33m                        names=['filename', 'year', 'location', 'categories', 'entity'])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\usr\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./data/SOU_1990_total_ner_extracted.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_csv('./data/SOU_1990_total_ner_extracted.csv', sep='\\t',\n",
    "                       names=['filename', 'year', 'location', 'categories', 'entity'])\n",
    "\n",
    "entities['document_id'] = entities.filename.apply(lambda x: int(x.split('_')[1]))\n",
    "entities['categories'] = entities.categories.str.replace('/', ' ')\n",
    "entities['category'] = entities.categories.str.split(' ').str.get(0)\n",
    "entities['sub_category'] = entities.categories.str.split(' ').str.get(1)\n",
    "\n",
    "entities.drop(['location', 'categories'], inplace=True, axis=1)\n",
    "\n",
    "document_names = pd.read_csv('./data/SOU_1990_index.csv',\n",
    "                             sep='\\t',\n",
    "                             names=['year', 'sequence_id', 'report_name']).set_index('sequence_id')\n",
    "\n",
    "def plot_freqdist(wf, n=25, **kwargs):\n",
    "    data = list(zip(*wf.most_common(n)))\n",
    "    x = list(data[0])\n",
    "    y = list(data[1])\n",
    "    labels = x\n",
    "\n",
    "    plt.figure(figsize=(13, 13/1.618))\n",
    "    plt.plot(x, y, '--ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='45')\n",
    "    plt.show()\n",
    "\n",
    "doc_names = { v: k for k, v in document_names.report_name.to_dict().items()}\n",
    "doc_names['** ALL DOCUMENTS **'] = 0\n",
    "@widgets.interact(category=entities.category.unique())\n",
    "def display_most_frequent_pos_tags(document_id=doc_names, category='LOC', top=10):\n",
    "    global entities\n",
    "    locations = entities\n",
    "    if document_id > 0:\n",
    "        locations = locations.loc[locations.document_id==document_id]\n",
    "    locations = locations.loc[locations.category==category]['entity']\n",
    "    location_freqs = nltk.FreqDist(locations)\n",
    "    #location_freqs.tabulate()\n",
    "    plot_freqdist(location_freqs, n=top)\n",
    "\n",
    "# display_most_frequent_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>MANDATORY STEP</span> Setup and Initialize the Notebook\n",
    "Use the **play** button, or press **Shift-Enter** to execute a code cell (select it first). The code imports Python libraries and frameworks, and initializes the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Folded Code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import common.utility\n",
    "from common.model_utility import ModelUtility\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "import common.widgets_utility as wf\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "#import common.vectorspace_utility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import logging\n",
    "import bokeh.models as bm\n",
    "import bokeh.palettes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pivottablejs import pivot_ui\n",
    "from IPython.display import display, HTML, clear_output, IFrame\n",
    "from itertools import product\n",
    "from bokeh.io import output_file, push_notebook\n",
    "from bokeh.core.properties import value, expr\n",
    "from bokeh.transform import transform, jitter\n",
    "from bokeh.layouts import row, column, widgetbox\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.models import ColumnDataSource, CustomJS\n",
    "\n",
    "logger = logging.getLogger('explore-topic-models')\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "AGGREGATES = { 'mean': np.mean, 'sum': np.sum, 'max': np.max, 'std': np.std }\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "pd.set_option('precision', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>MANDATORY STEP</span> Select LDA Topic Model\n",
    "- Select one of the previously computed and prepared topic models that you wan't to use in subsequent steps.\n",
    "- Models are computed in batch in accordance to \n",
    "<a href=\"./images/workflow-prepare.svg\">process flow</a> used in the *Digitala modeller* project.\n",
    "- Note that subsequent code cells are NOT updated (executed) automatically when a new model is selected.\n",
    "- Use the **play** button, or press **Shift-Enter** to execute the selected cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hidden code: Select current model state\n",
    "class ModelState:\n",
    "    \n",
    "    def __init__(self, data_folder):\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.basenames = ModelUtility.get_model_names(data_folder)\n",
    "        self.basename = self.basenames[0]\n",
    "        self.on_set_model_callback = None\n",
    "        \n",
    "    def set_model(self, basename=None):\n",
    "\n",
    "        basename = basename or self.basename\n",
    "        \n",
    "        self.basename = basename\n",
    "        self.topic_keys = ModelUtility.get_topic_keys(self.data_folder, basename)\n",
    "        state.max_alpha = self.topic_keys.alpha.max()\n",
    "        self.topic_overview = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'topic_tokens')\n",
    "        self.document_topic_weights = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'doc_topic_weights')\\\n",
    "            .drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "        self.topic_token_weights = ModelUtility\\\n",
    "            .get_result_model_sheet(self.data_folder, basename, 'topic_token_weights')\\\n",
    "            .drop('Unnamed: 0', axis=1, errors='ignore')\\\n",
    "            .dropna(subset=['token'])\n",
    "        self._years = list(range(\n",
    "            self.document_topic_weights.year.min(), self.document_topic_weights.year.max() + 1))\n",
    "        self.min_year = min(self._years)\n",
    "        self.max_year = max(self._years)\n",
    "        self.years = [None] + self._years\n",
    "        self.n_topics = self.topic_overview.topic_id.max() + 1\n",
    "        # https://stackoverflow.com/questions/44561609/how-does-mallet-set-its-default-hyperparameters-for-lda-i-e-alpha-and-beta\n",
    "        self.initial_alpha = 0.0  # 5.0 / self.n_topics if 'mallet' in state.basename else 1.0 / self.n_topics\n",
    "        self.initial_beta = 0.0  # 0.01 if 'mallet' in basename else 1.0 / self.n_topics\n",
    "        self._lda = None\n",
    "        self._topic_titles = None\n",
    "        self.corpus_documents = ModelUtility.get_corpus_documents(self.data_folder, self.basename).set_index('document_id')\n",
    "        print(\"Current model: \" + self.basename.upper())\n",
    "        \n",
    "        if self.on_set_model_callback is not None:\n",
    "            self.on_set_model_callback(self)\n",
    "            \n",
    "        # _fix_topictokens()\n",
    "        return self\n",
    "    \n",
    "    #def get_document_topic_weights(self, year=None, topic_id=None):\n",
    "    #    df = self.document_topic_weights\n",
    "    #    if year is None and topic_id is None:\n",
    "    #        return df\n",
    "    #    if topic_id is None:\n",
    "    #        return df[(df.year == year)]\n",
    "    #    if year is None:\n",
    "    #        return df[(df.topic_id == topic_id)]\n",
    "    #    return df[(df.year == year)&(df.topic_id == topic_id)]\n",
    "    \n",
    "    def get_unique_topic_ids(self):\n",
    "        return self.document_topic_weights['topic_id'].unique()\n",
    "    \n",
    "    #def get_topic_weight_by_year_or_document(self, key='mean', pivot_column=None):\n",
    "    #    \n",
    "    #    if pivot_column is None:\n",
    "    #        pivot_column = 'year' if year is None else 'document_id'    \n",
    "    #        \n",
    "    #    df = self.document_topic_weights(year) \\\n",
    "    #        .groupby([pivot_column,'topic_id']) \\\n",
    "    #        .agg(AGGREGATES[key])[['weight']].reset_index()\n",
    "    #    return df, pivot_column\n",
    "    \n",
    "    #return self.get_document_topic_weight_by_pivot_column(pivot_column, key, filter={'column': 'year', 'values': [year]})\n",
    "    \n",
    "    def get_document_topic_weight_by_filter(self, filters=None):\n",
    "        df = self.document_topic_weights.query('weight > 0')\n",
    "        for filter in (filters or []):\n",
    "            if 'query' in filter.keys():\n",
    "                df = df.query(filter['query'])\n",
    "            elif isinstance(filter['value'], str):\n",
    "                df = df[(df[filter['column']]==filter['value'])]\n",
    "            elif isinstance(filter['value'], list):\n",
    "                df = df[(df[filter['column']].isin(filter['value']))]\n",
    "        return df\n",
    "    \n",
    "    def get_document_topic_weight_by_pivot_column(self, pivot_column, key='mean', filters=None):\n",
    "        df = self.get_document_topic_weight_by_filter(filters)\n",
    "        df = df.groupby([pivot_column, 'topic_id'])\\\n",
    "               .agg(AGGREGATES[key])[['weight']].reset_index()\n",
    "        return df[df.weight > 0]\n",
    "    \n",
    "    def get_topic_tokens_dict(self, topic_id, n_top=200):\n",
    "        return self.get_topic_tokens(topic_id)\\\n",
    "            .sort_values(['weight'], ascending=False)\\\n",
    "            .head(n_top)[['token', 'weight']]\\\n",
    "            .set_index('token').to_dict()['weight']\n",
    "\n",
    "    def compute_topic_terms_vector_space(self, n_words=100):\n",
    "        '''\n",
    "        Create an align topic-term vector space of top n_words from each topic\n",
    "        '''\n",
    "        unaligned_vector_dicts = ( self.get_topic_tokens_dict(topic_id, n_words) for topic_id in range(0, self.n_topics) )\n",
    "        X, feature_names = ModelUtility.compute_and_align_vector_space(unaligned_vector_dicts)\n",
    "        return X, feature_names\n",
    "\n",
    "    def get_lda(self):\n",
    "        raise Exception(\"Use of LDA model disabled in this Notebook\")\n",
    "        '''\n",
    "        Get gensim model. Only used for pyLDAvis display\n",
    "        '''\n",
    "        if self._lda is None:\n",
    "            filename = os.path.join(self.data_folder, self.basename, 'gensim_model_{}.gensim.gz'.format(self.basename))\n",
    "            if os.path.isfile(filename):\n",
    "                self._lda = LdaModel.load(filename)\n",
    "                print('LDA model loaded...')\n",
    "            else:\n",
    "                print('LDA not found on disk...')\n",
    "        return self._lda \n",
    "    \n",
    "    def get_topic_titles(self, n_words=100, cache=True):\n",
    "        if cache and self._topic_titles is not None:\n",
    "            return self._topic_titles\n",
    "        _topic_titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=n_words)\n",
    "        self._topic_titles = _topic_titles if cache else None\n",
    "        return _topic_titles\n",
    "    \n",
    "    def get_topic_tokens(self, topic_id, max_n_words=500):\n",
    "        tokens = state.topic_token_weights\\\n",
    "            .loc[lambda x: x.topic_id == topic_id]\\\n",
    "            .sort_values('weight',ascending=False)[:max_n_words]\n",
    "        return tokens\n",
    "    \n",
    "    def get_topic_alphas(self):\n",
    "        tokens = state.topic_token_weights\\\n",
    "            .loc[lambda x: x.topic_id == topic_id]\\\n",
    "            .sort_values('weight',ascending=False)[:max_n_words]\n",
    "        alpas = ModelUtility.get_topic_alphas\n",
    "        return tokens\n",
    "    \n",
    "    def get_topic_year_aggregate_weights(self, fn, threshold):\n",
    "        df = self.document_topic_weights[(self.document_topic_weights.weight > 0.001)]\n",
    "        df = df.groupby(['year', 'topic_id']).agg(fn)['weight'].reset_index()\n",
    "        df = df[(df.weight>=threshold)]\n",
    "        return df\n",
    "    \n",
    "    def get_topic_proportions(self):\n",
    "        corpus_documents = self.get_corpus_documents()\n",
    "        document_topic_weights = self.document_topic_weights\n",
    "        topic_proportion = ModelUtility.compute_topic_proportions(document_topic_weights, corpus_documents)\n",
    "        return topic_proportion\n",
    "    \n",
    "    def get_corpus_documents(self):\n",
    "        #if self.corpus_documents is None:\n",
    "        #    self.corpus_documents = ModelUtility.get_corpus_documents(self.data_folder, self.basename)\n",
    "        return self.corpus_documents\n",
    "\n",
    "    def on_set_model(self, callback):\n",
    "        self.on_set_model_callback = callback\n",
    "        return self\n",
    "        \n",
    "def on_set_model_handler(state):\n",
    "\n",
    "    if 'report_name' in state.corpus_documents:\n",
    "        return\n",
    "    \n",
    "    state.source_documents = pd.read_csv('data/SOU_1990_index.csv', sep='\\t', header=None, names=['year', 'report_id', 'report_name'])\n",
    "    state.corpus_documents['report_id'] = state.corpus_documents.document.str.split('_').apply(lambda x: x[1]).astype(np.int64)\n",
    "    state.corpus_documents['report_name'] = pd.merge(state.corpus_documents, state.source_documents, how='inner', on=['year', 'report_id']).report_name\n",
    "    state.corpus_documents['report_name'] = state.corpus_documents.apply(lambda x: '{}-{} {}'.format(x['year'], x['report_id'], x['report_name'])[:50], axis=1)\n",
    "    state.document_topic_weights['report_name'] = pd.merge(state.document_topic_weights, state.corpus_documents, left_on='document_id', right_index=True).report_name\n",
    "\n",
    "def select_model_main(state):\n",
    "    \n",
    "    basename_widget = widgets.Dropdown(\n",
    "        options=state.basenames,\n",
    "        value=state.basename,\n",
    "        description='Topic model',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='75%')\n",
    "    )\n",
    "    \n",
    "    w = widgets.interactive(state.set_model, basename=basename_widget, state=widgets.fixed(state))\n",
    "    display(widgets.VBox((basename_widget,) + (w.children[-1],)))\n",
    "    w.update()\n",
    "\n",
    "state = ModelState('./data').on_set_model(on_set_model_handler)\n",
    "\n",
    "select_model_main(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Word Distribution - Wordcloud and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88deccf4431a4cd7a4ecf3118b9c62e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='tx02' style='line-height: 20px;'></span>\", placeholder=''), HBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_topic_distribution_widgets(callback, state, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=state.n_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', state.n_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(topic_id=0, n_words=100, output_format='Wordcloud', widget_container=None):\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = state.topic_token_weights.loc[(state.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = state.get_topic_titles(n_words=n_words, cache=True).iloc[topic_id]\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = state.get_topic_tokens(topic_id, n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(state.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "display_topic_distribution_widgets(display_wordcloud, state, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Word Distribution - Chart\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44db0d02a7e740e08b111059b5b68859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='wc01' style='line-height: 20px;'></span>\", placeholder=''), HBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display topic's word distribution\n",
    "\n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = ColumnDataSource(tokens)\n",
    "\n",
    "    p = figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bm.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def plot_topic_tokens_charts(tokens, flag=True):\n",
    "\n",
    "    if flag:\n",
    "        left = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        show(left)\n",
    "        return\n",
    "\n",
    "    left = plot_topic_word_distribution(tokens, plot_width=450, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "    right = plot_topic_word_distribution(tokens, plot_width=450, plot_height=500, title='', tools='pan')\n",
    "\n",
    "    source = ColumnDataSource({'x':[], 'y':[], 'width':[], 'height':[]})\n",
    "    left.x_range.callback = create_js_callback('x', 'width', source)\n",
    "    left.y_range.callback = create_js_callback('y', 'height', source)\n",
    "\n",
    "    rect = bm.Rect(x='x', y='y', width='width', height='height', fill_alpha=0.0, line_color='blue', line_alpha=0.4)\n",
    "    right.add_glyph(source, rect)\n",
    "\n",
    "    show(row(left, right))\n",
    "\n",
    "def display_topic_tokens(topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    tokens = state.get_topic_tokens(topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        plot_topic_tokens_charts(tokens)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "    widget_container.reset()\n",
    "        \n",
    "display_topic_distribution_widgets(display_topic_tokens, state, 'wc01', ['Chart', 'Table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic's Trend Over Time or Documents\n",
    "- Displays topic's share over documents or time.\n",
    "- Note that source documents (i.e. SOU reports) are splitted into 1000 word chunks (LDA document) by the topic modelling process\n",
    "- If \"SOU Report\" or \"Year\" is selected then the **max** or **mean** weight is selected from corresponding LDA documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ad89ad806a43ec9187bcad27e55005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='topic_share_plot' style='line-height: 20px;'></span>\", placeholder='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "import numpy as np\n",
    "import math\n",
    "import bokeh.plotting\n",
    "from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "from bokeh.models.glyphs import VBar\n",
    "from bokeh.io import curdoc, show\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=700, title='', tools=TOOLS, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, pivot_config, value_column, widgets_container, output_format='Chart', state=None, threshold=0.01):\n",
    "    \n",
    "    pivot_column = pivot_config['pivot_column']\n",
    "    tokens = state.get_topic_titles(n_words=200, cache=True).iloc[topic_id]\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = value_column if pivot_column is not None else 'weight'\n",
    "    \n",
    "    df = state.document_topic_weights[(state.document_topic_weights.topic_id==topic_id)]\n",
    "    \n",
    "    if pivot_column is not None:\n",
    "        df = df.groupby([pivot_column]).agg([np.mean, np.max])['weight'].reset_index()\n",
    "        df.columns = [pivot_column, 'mean', 'max' ]\n",
    "        df = df[(df[value_column] > threshold)]\n",
    "        \n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = pivot_column.title()\n",
    "        y_label = value_column.title() + ('weight' if value_column != 'weight' else '')\n",
    "        p = plot_topic_trend(df, pivot_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        show(p)\n",
    "\n",
    "def create_topic_trend_widgets(state):\n",
    "    pivot_options = {\n",
    "        '': { 'pivot_column': None, 'filter': None },\n",
    "        'SOU Report': { 'pivot_column': 'report_name', 'filter': None },\n",
    "        'Year': { 'pivot_column': 'year', 'filter': None },\n",
    "        'LDA Document': { 'pivot_column': 'document_id', 'filter': None }\n",
    "    } \n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        #year=wf.create_select_widget('Year', options=state.years, value=state.years[-1]),\n",
    "        pivot_config=widgets.Dropdown(\n",
    "            options=pivot_options,\n",
    "            value=pivot_options['SOU Report'],\n",
    "            description='Group by'\n",
    "        ),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=state.n_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "        aggregate=widgets.Dropdown(options=['max', 'mean'], value='max', description='Aggregate')\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', state.n_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', state.n_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        pivot_config=wc.pivot_config,\n",
    "        value_column=wc.aggregate,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        state=widgets.fixed(state),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.pivot_config, wc.aggregate, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "    \n",
    "create_topic_trend_widgets(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic to Document Network\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709be140b5874fb784a6c5335ed38e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='nx_id1' style='line-height: 20px;display: inline; height='400px''></sp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "     \n",
    "def plot_topic_year_network(network, layout, scale=1.0, titles=None):\n",
    "\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=TOOLS)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bm.HoverTool(renderers=[r_topics], tooltips=None, callback=wf.WidgetUtility.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(\n",
    "        x='x', y='y', text='name', level='overlay',\n",
    "        x_offset=0, y_offset=0, text_font_size='8pt'\n",
    "    )\n",
    "    \n",
    "    p.add_layout(\n",
    "        bm.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bm.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_year_network(state):\n",
    "    \n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        n_topics=state.n_topics,\n",
    "        text_id='nx_id1',\n",
    "        text=wf.create_text_widget('nx_id1', style=\"display: inline; height='400px'\"),\n",
    "        year=widgets.IntSlider(description='Year', min=state.min_year, max=state.max_year, step=1, value=state.min_year, continues_update=False),\n",
    "        pivot_column=widgets.Dropdown(\n",
    "            options={\n",
    "                'SOU report': 'report_name',\n",
    "                'Year': 'year'\n",
    "            },\n",
    "            value='report_name',\n",
    "            description='Pivot'\n",
    "        ),\n",
    "        scale=widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False),\n",
    "        output_format=widgets.Dropdown(\n",
    "            options={'Network': 'network', 'Table': 'table'},\n",
    "            value='network',\n",
    "            description='Output'\n",
    "        ),\n",
    "        layout=widgets.Dropdown(\n",
    "            options=list(layout_algorithms.keys()),\n",
    "            value='Fruchterman-Reingold',\n",
    "            description='Layout'\n",
    "        ),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"40%\"))\n",
    "    ) \n",
    "    \n",
    "    wc.previous = wc.create_prev_id_button('year', 10000)\n",
    "    wc.next = wc.create_next_id_button('year', 10000)    \n",
    "    \n",
    "    def display_topic_year_network(\n",
    "        layout_algorithm,\n",
    "        threshold=0.50,\n",
    "        scale=1.0,\n",
    "        pivot_column='report_name',\n",
    "        year=None,\n",
    "        output_format='network'\n",
    "    ):\n",
    "        wc.progress.value = 1\n",
    "        \n",
    "        titles = state.get_topic_titles()\n",
    "        filters = []\n",
    "        if year is not None:\n",
    "            filters = [ { 'column': 'year', 'value': year }]\n",
    "        filters = filters + [ { 'query': 'weight >= {}'.format(threshold) } ]\n",
    "        df = state.get_document_topic_weight_by_pivot_column(pivot_column, key='max', filters=filters)\n",
    "        df = df[df.weight > threshold]\n",
    "        \n",
    "        wc.progress.value = 2\n",
    "\n",
    "        network = NetworkUtility.create_bipartite_network(df, pivot_column, 'topic_id')\n",
    "        \n",
    "        wc.progress.value = 3\n",
    "\n",
    "        if output_format == 'network':\n",
    "            \n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            \n",
    "            wc.progress.value = 4\n",
    "            \n",
    "            p = plot_topic_year_network(network, layout, scale=scale, titles=titles)\n",
    "            show(p)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            print(df.shape)\n",
    "            display(df)\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        wc.progress.value = 0\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_year_network,\n",
    "        layout_algorithm=wc.layout,\n",
    "        threshold=wc.threshold,\n",
    "        scale=wc.scale,\n",
    "        pivot_column=wc.pivot_column,\n",
    "        year=wc.year,\n",
    "        output_format=wc.output_format\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.layout, wc.year, wc.previous, wc.next]),\n",
    "        widgets.HBox([wc.pivot_column, wc.scale]),\n",
    "        widgets.HBox([wc.output_format, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "    \n",
    "main_topic_year_network(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Trends - Heatmap\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanford’s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dec696f234b435fa1d94447cb482638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Pivot', layout=Layout(width='200px'), options={'SOU report…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "\n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = [\"#efefef\", \"#75968f\", \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\", \"#ddb7b1\", \"#cc7878\",\n",
    "              \"#933b41\", \"#550b1d\"]\n",
    "    mapper = bm.LinearColorMapper(palette=colors, low=df.weight.min(), high=max_weight)\n",
    "    color_transform = transform('weight', mapper)\n",
    "    color_bar = bm.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bm.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bm.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, glyph, titles, text_id):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "    \n",
    "    ''' Setup axis categories '''\n",
    "    x_range = list(map(str, df[xs].unique()))\n",
    "    y_range = list(map(str, df[ys].unique()))\n",
    "    \n",
    "    ''' Setup coloring and color bar '''\n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    plot_height = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = figure(title=\"Topic heatmap\", tools=TOOLS, toolbar_location=\"right\", x_range=x_range,\n",
    "           y_range=y_range, x_axis_location=\"above\", plot_width=1000, plot_height=plot_height)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    if glyph == 'Circle':\n",
    "        cr = p.circle(color=color_transform, **args)\n",
    "    else:\n",
    "        cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"5pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bm.HoverTool(tooltips=None, callback=wf.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def topic_heatmap_main(state):\n",
    "    \n",
    "    def display_topic_relevance_by_year(state, key='max', pivot_column=None, year=None, flip_axis=False, glyph='Circle', wdgs=None):\n",
    "        \n",
    "        try:\n",
    "            wdgs.reset()\n",
    "            wdgs.forward()\n",
    "            \n",
    "            titles = ModelUtility.get_topic_titles(state.topic_token_weights, n_words=100)\n",
    "            wdgs.forward()\n",
    "\n",
    "            year = (year or 0)\n",
    "            \n",
    "            pivot_column = 'year' if year > 0 else (pivot_column or 'report_name')\n",
    "            filters = [{'column': 'year', 'values': [year]}] if year > 0 else []\n",
    "            \n",
    "            df = state.get_document_topic_weight_by_pivot_column(pivot_column, key, filters=filters)\n",
    "            \n",
    "            wdgs.forward()\n",
    "            \n",
    "            df[pivot_column] = df[pivot_column].astype(str)\n",
    "            df['topic_id'] = df.topic_id.astype(str)\n",
    "            \n",
    "            wdgs.forward()\n",
    "            \n",
    "            p = plot_topic_relevance_by_year(df, xs=pivot_column, ys='topic_id', flip_axis=flip_axis, glyph=glyph, titles=titles, text_id='topic_relevance')\n",
    "            \n",
    "            show(p)\n",
    "            wdgs.reset()\n",
    "        except Exception as ex:\n",
    "            raise\n",
    "            logger.error(ex)\n",
    "        finally:\n",
    "            wdgs.reset()\n",
    "\n",
    "    wc = wf.BaseWidgetUtility(\n",
    "        text_id='topic_relevance',\n",
    "        text=wf.create_text_widget('topic_relevance'),\n",
    "        year=widgets.Dropdown(options=state.years, value=None, description='Year', layout=widgets.Layout(width=\"140px\")),\n",
    "        pivot_column=widgets.Dropdown(\n",
    "            options={\n",
    "                'SOU report': 'report_name',\n",
    "                # 'LDA document': 'document_id',\n",
    "                'Year': 'year'\n",
    "            },\n",
    "            value='report_name',\n",
    "            description='Pivot',\n",
    "            layout=widgets.Layout(width=\"200px\")\n",
    "        ),\n",
    "        aggregate=widgets.Dropdown(options=['max', 'mean'], value='max', description='Aggregate', layout=widgets.Layout(width=\"180px\")),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"35%\")),\n",
    "        glyph=widgets.Dropdown(options=['Circle', 'Square'], value='Square', description='Glyph', layout=widgets.Layout(width=\"180px\")),\n",
    "        flip_axis=widgets.ToggleButton(value=True, description='Flip XY', tooltip='Flip X and Y axis', icon='', layout=widgets.Layout(width=\"80px\"))\n",
    "    )\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_relevance_by_year,\n",
    "        state=widgets.fixed(state),\n",
    "        key=wc.aggregate,\n",
    "        pivot_column=wc.pivot_column,\n",
    "        year=wc.year,\n",
    "        glyph=wc.glyph,\n",
    "        flip_axis=wc.flip_axis,\n",
    "        wdgs=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([wc.pivot_column, wc.year, wc.aggregate, wc.flip_axis, wc.glyph, wc.progress ]),\n",
    "        wc.text,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "            \n",
    "topic_heatmap_main(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
